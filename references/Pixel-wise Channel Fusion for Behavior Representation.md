# Pixel-wise Channel Fusion for Behavior Representation

## ì •ì˜

Multi-channel vision encoderì˜ ì¶œë ¥(M/P channels)ì„ **spatial locationì„ ë³´ì¡´í•˜ë©´ì„œ** í†µí•©í•˜ëŠ” fusion ì „ëµ. ê¸°ì¡´ì˜ spatial averaging (avg pooling) ë°©ì‹ì´ spatial informationì„ ì†ì‹¤í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê° í”½ì…€ ìœ„ì¹˜ì—ì„œ ì±„ë„ ê°„ fusionì„ ìˆ˜í–‰í•œë‹¤.

---

## ë¬¸ì œ: Spatial Averagingì˜ ì¹˜ëª…ì  í•œê³„

### dino.txt (2024) ì ‘ê·¼ë²•ì˜ í•œê³„

**Vision-language alignmentì—ì„œ íš¨ê³¼ì ì´ì—ˆë˜ ë°©ë²•**:

```python
# dino.txt ë°©ì‹ (CLIP, VLAì—ì„œë„ ìœ ì‚¬)
representation = [CLS_M ; avg(patches_M) ; CLS_P ; avg(patches_P)]
â†’ 4D global descriptor
â†’ Classification, Retrievalì— íš¨ê³¼ì 
```

**ë¬¸ì œì **: Spatial information ì™„ì „ ì†ì‹¤

```python
# ì˜ˆì‹œ: 4Ã—4 íŒ¨ì¹˜
patches = [
    [0.1, 0.2],  # ì™¼ìª½ ìƒë‹¨: ë¹¨ê°„ ë¬¼ì²´
    [0.5, 0.1],  # ì˜¤ë¥¸ìª½ ìƒë‹¨: íŒŒë€ ë°°ê²½
    [0.2, 0.8],  # ì™¼ìª½ í•˜ë‹¨: ì´ˆë¡ í…Œì´ë¸”
    [0.3, 0.3]   # ì˜¤ë¥¸ìª½ í•˜ë‹¨: ê·¸ë¦¬í¼
]

avg(patches) = [0.275, 0.35]
â†’ "ë­”ê°€ ì„ì—¬ìˆë‹¤" âœ“
â†’ "ì–´ë””ì— ë¬´ì—‡ì´ ìˆëŠ”ê°€" âœ— (ì™„ì „ ì†ì‹¤)
```

### Behavior/Manipulationì—ì„œ Critical

**Task**: "Pick the red cube"

í•„ìš”í•œ ì •ë³´:
1. ë¹¨ê°„ íë¸Œì˜ **ìœ„ì¹˜** (x, y)
2. ê·¸ë¦¬í¼ì˜ **í˜„ì¬ ìœ„ì¹˜**
3. ë‘˜ ì‚¬ì´ì˜ **spatial relationship**

**avg(patches) ê²°ê³¼**:
- "ë¹¨ê°„ìƒ‰ì´ scene ì–´ë”˜ê°€ì— ìˆë‹¤" âœ“
- "ë¹¨ê°„ìƒ‰ì´ **ì–´ë””ì—** ìˆëŠ”ê°€" âœ—
- **Spatial reasoning ë¶ˆê°€ëŠ¥!**

### ìƒë¬¼í•™ì  ê´€ì 

- **Dorsal stream** (where pathway): Spatial location ìœ ì§€
- **Ventral stream** (what pathway): Object identity
- **BehaviorëŠ” ë‘˜ ë‹¤ í•„ìš”**: "What is where"
- avgëŠ” "what"ë§Œ ë‚¨ê¸°ê³  "where"ë¥¼ ë²„ë¦¼

---

## í•´ê²°ì±…: Pixel-wise Channel Fusion

### í•µì‹¬ ì•„ì´ë””ì–´

```python
# ì œì•ˆí•˜ëŠ” ë°©ì‹
M_patches: [patch_M1, ..., patch_Mn]  (ê° Dì°¨ì›, ìœ„ì¹˜ ë³´ì¡´)
P_patches: [patch_P1, ..., patch_Pn]  (ê° Dì°¨ì›, ìœ„ì¹˜ ë³´ì¡´)
              â†“
    Pixel-wise Fusion (shared FC)
              â†“
Fused_patches: [patch_1_fused, ..., patch_n_fused]

â†’ ëª¨ë“  spatial location ë³´ì¡´ âœ…
â†’ Channel dimensionë§Œ ì¶•ì†Œ (2D â†’ D) âœ…
â†’ Spatial reasoning ê°€ëŠ¥ âœ…
```

### êµ¬ì²´ì  ì˜ˆì‹œ

```
Mì±„ë„ (4Ã—4 grid):
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ 0.0 â”‚ 0.0 â”‚ 0.0 â”‚ 0.0 â”‚  â† ë°°ê²½: ì •ì 
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ 0.0 â”‚-0.5 â”‚-0.5 â”‚ 0.0 â”‚  â† ì¤‘ì•™: í° ë³€í™”
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ 0.0 â”‚-0.5 â”‚-0.5 â”‚ 0.0 â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ 0.0 â”‚ 0.0 â”‚ 0.0 â”‚ 0.0 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Pì±„ë„ (4Ã—4 grid):
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ 0.1 â”‚ 0.1 â”‚ 0.1 â”‚ 0.1 â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ 0.1 â”‚ 0.9 â”‚ 0.9 â”‚ 0.1 â”‚  â† ì¤‘ì•™: ê°•í•œ edge
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ 0.1 â”‚ 0.9 â”‚ 0.9 â”‚ 0.1 â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ 0.1 â”‚ 0.1 â”‚ 0.1 â”‚ 0.1 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

avg ë°©ì‹:
  avg(M) = -0.125  â†’ "ì•½ê°„ ì–´ë‘ì›Œì§" (ìœ„ì¹˜ ë¶ˆëª…)
  avg(P) = 0.35    â†’ "ì•½ê°„ì˜ edge" (ìœ„ì¹˜ ë¶ˆëª…)

Pixel-wise ë°©ì‹:
  patch[1,1] = FC([M:-0.5, P:0.9])
               â†’ "ì¤‘ì•™(1,1)ì—ì„œ í° ë³€í™” + ê°•í•œ edge"
  patch[0,0] = FC([M:0.0, P:0.1])
               â†’ "ì™¼ìª½ ìƒë‹¨(0,0)ì€ ì •ì  + ì•½í•œ edge"
  â†’ Spatial structure ì™„ì „ ë³´ì¡´!
```

---

## Encoder Pre-training Strategy

Pixel-wise fusionì˜ íš¨ê³¼ëŠ” **encoderê°€ ì–´ë–»ê²Œ í•™ìŠµë˜ì—ˆëŠ”ì§€**ì— ì§ì ‘ì ìœ¼ë¡œ ì˜ì¡´í•œë‹¤. M/P ì±„ë„ì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ ì‚´ë¦¬ë©´ì„œë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ì „ëµì„ ì„¤ê³„í•œë‹¤.

### 1. Video + Static Mixed Learning

**í•µì‹¬ í†µì°°**: "No change is also a consistent change (zero change)"

#### Data Composition

| Data Type | M Channel (4ch) | P Channel (5ch) | Ratio |
|-----------|-----------------|-----------------|-------|
| **Video** | [Î”L, Î”R, Î”G, Î”B] â‰  0 | [âˆ‚x, âˆ‚y, R, G, B] | 70% |
| **Static Image** | [0, 0, 0, 0] | [âˆ‚x, âˆ‚y, R, G, B] | 30% |

#### Rationale

**Video ë°ì´í„° (70%)**:
```python
frames = [t0, t1, t2]

# Mì±„ë„: ì‹œê°„ì  ë³€í™” (Dorsal pathway)
M = [Î”L, Î”R, Î”G, Î”B]  # â‰  0
# â†’ M_encoder learns "what changed"
# â†’ Temporal change patterns + color dynamics

# Pì±„ë„: ê³µê°„ + ìƒ‰ìƒ (Ventral pathway)
P = [âˆ‚x, âˆ‚y, R, G, B]
# â†’ P_encoder learns "what is where"
# â†’ Spatial structure + object appearance
```

**Static ë°ì´í„° (30%)**:
```python
frames = [img, img, img]  # ë™ì¼ ì´ë¯¸ì§€ 3ë²ˆ

# Mì±„ë„: ë³€í™” ì—†ìŒ (ì™„ì „ ë¹„í™œì„±í™”!)
M = [0, 0, 0, 0]  # Zero temporal change!
# â†’ M_encoder receives no signal
# â†’ Forces P to be responsible âœ…

# Pì±„ë„: ëª¨ë“  ì •ë³´ í¬í•¨
P = [âˆ‚x, âˆ‚y, R, G, B]
# â†’ P_encoder learns "what is where" (P-dominant)
# â†’ Structure (âˆ‚x, âˆ‚y) + Color (R, G, B)
# â†’ ê²€ì¦ëœ DINO í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ í™œìš©
```

#### M-P Balance íš¨ê³¼

| Data Type | M Contribution | P Contribution | Learning Focus |
|-----------|----------------|----------------|----------------|
| Video | **High** (ë³€í™” â‰  0) | **High** (êµ¬ì¡° + ìƒ‰ìƒ) | Both M and P learn |
| Static | **Zero** (ë³€í™” = 0) | **Very High** (êµ¬ì¡° + ìƒ‰ìƒ) | **P forced to learn** âœ… |
| **Mixed (7:3)** | **Balanced** | **Balanced** | **M=motion, P=appearance** |

**í•µì‹¬ í•´ê²°ì±…**:
- âœ… **Staticì—ì„œ M ì™„ì „ ë¹„í™œì„±í™”** â†’ Pê°€ ëª¨ë“  ì±…ì„ (ê· í˜• ê°•ì œ)
- âœ… **Pê°€ color ì •ë³´ í¬í•¨** â†’ RGBê°€ Pì— ìˆì–´ ìƒë¬¼í•™ì ìœ¼ë¡œ íƒ€ë‹¹
- âœ… Mê³¼ Pê°€ complementaryí•˜ê²Œ í•™ìŠµ (M=change, P=what+where)
- âœ… Videoì—ì„œ temporal + spatial reasoning í•™ìŠµ
- âœ… Staticì—ì„œ DINOì˜ ê²€ì¦ëœ ì•ˆì •ì„± í™œìš©

#### Data Sources

**Video (70%)**:
- **EgoDex** (829h): Human manipulation demonstrations
- **Something-Something V2**: Object interaction videos
- **Robot replay buffer**: Behavioral trajectories

**Static (30%)**:
- **ImageNet**: General visual features
- **Robotic object datasets**: Manipulation-relevant objects
- **Scene images**: Environmental context

---

### 2. DINO-style Temporal Augmentation

**í•µì‹¬**: Random cropì„ ì‚¬ìš©í•˜ë˜ **ê°™ì€ spatial location**ì—ì„œ crop

#### ì™œ Cropì„ ì‚¬ìš©í•´ë„ ê´œì°®ì€ê°€?

```python
# CLS token: Global representation
# â†’ Cropí•´ë„ ì „ì²´ scene ì´í•´ (DINOì—ì„œ ê²€ì¦ë¨)

# Patches: Local spatial structure
# â†’ Crop ë‚´ë¶€ì—ì„œ ìƒëŒ€ì  ìœ„ì¹˜ëŠ” ë³´ì¡´
# â†’ "ì´ 96Ã—96 ì˜ì—­ ë‚´ì—ì„œ Aê°€ Bì˜ ì™¼ìª½ì—" í•™ìŠµ ê°€ëŠ¥

# Example: 96Ã—96 crop with ViT-B/16
# â†’ ì•½ 36 patches
# â†’ Pixel-wise fusionìœ¼ë¡œ ê° patch ê°„ spatial relationship ìœ ì§€
```

#### Multi-crop Implementation

```python
def get_dino_views_temporal(frames):
    """
    Args:
        frames: [t0, t1, t2] - ì—°ì†ëœ 3 í”„ë ˆì„

    Returns:
        views: List of (view_type, m_channels, p_channels)

    Key: ëª¨ë“  cropì€ ë™ì¼í•œ ìœ„ì¹˜ì—ì„œ ìˆ˜í–‰!
    """
    # Crop parameters í•œ ë²ˆë§Œ ê²°ì •
    crop_params_global = get_random_crop_params(size=224)
    crop_params_local1 = get_random_crop_params(size=96)
    crop_params_local2 = get_random_crop_params(size=96)

    views = []

    for i in range(len(frames) - 1):
        img_prev = frames[i]
        img_curr = frames[i + 1]

        # Global view (224Ã—224, ê°™ì€ ìœ„ì¹˜)
        global_prev = apply_crop(img_prev, crop_params_global)
        global_curr = apply_crop(img_curr, crop_params_global)
        m_g = preprocess_M(global_prev, global_curr)  # [4, 224, 224]
        p_g = preprocess_P(global_prev)               # [2, 224, 224]
        views.append(('global', m_g, p_g))

        # Local view 1 (96Ã—96, ê°™ì€ ìœ„ì¹˜)
        local1_prev = apply_crop(img_prev, crop_params_local1)
        local1_curr = apply_crop(img_curr, crop_params_local1)
        m_l1 = preprocess_M(local1_prev, local1_curr)
        p_l1 = preprocess_P(local1_prev)
        views.append(('local', m_l1, p_l1))

        # Local view 2 (96Ã—96, ê°™ì€ ìœ„ì¹˜)
        local2_prev = apply_crop(img_prev, crop_params_local2)
        local2_curr = apply_crop(img_curr, crop_params_local2)
        m_l2 = preprocess_M(local2_prev, local2_curr)
        p_l2 = preprocess_P(local2_prev)
        views.append(('local', m_l2, p_l2))

    return views
```

**Example**:
```
Video: [t0, t1, t2]
Crop at (50, 50, 224, 224) - í•œ ë²ˆë§Œ ê²°ì •

t0 â†’ t1 ë³€í™”:
  - Global: crop(t0) â†’ crop(t1) ë™ì¼ ìœ„ì¹˜
  - Local1: crop(t0) â†’ crop(t1) ë™ì¼ ìœ„ì¹˜
  - Local2: crop(t0) â†’ crop(t1) ë™ì¼ ìœ„ì¹˜

t1 â†’ t2 ë³€í™”:
  - Global: crop(t1) â†’ crop(t2) ë™ì¼ ìœ„ì¹˜
  - ...
```

---

### 3. CLS Fusion Options (Experimental)

Pre-training ë‹¨ê³„ì—ì„œ M_CLSì™€ P_CLSë¥¼ ì–´ë–»ê²Œ ê²°í•©í• ì§€ëŠ” **ì‹¤í—˜ì ìœ¼ë¡œ ê²°ì •**í•œë‹¤.

#### Option A: Linear Projection (FC)

```python
class LinearClsFusion(nn.Module):
    def __init__(self, dim=768):
        super().__init__()
        self.fc = nn.Linear(2 * dim, dim)

    def forward(self, cls_m, cls_p):
        # Simple concatenation + FC
        fused = self.fc(torch.cat([cls_m, cls_p], dim=-1))
        return fused  # [B, D]
```

**ì¥ì **:
- âœ… í•™ìŠµ ì•ˆì •ì„± (DINOëŠ” ì´ë¯¸ ë³µì¡)
- âœ… Compute íš¨ìœ¨ (íŒŒë¼ë¯¸í„° 2DÂ²)
- âœ… ëª…í™•í•œ baseline
- âœ… M-P ê°€ì¤‘ì¹˜ í•™ìŠµ ê°€ëŠ¥

**ë‹¨ì **:
- âŒ Linear combinationë§Œ ê°€ëŠ¥
- âŒ M-P ìƒí˜¸ì‘ìš© ì œí•œì 

---

#### Option B: MLP

```python
class MLPClsFusion(nn.Module):
    def __init__(self, dim=768, hidden_dim=1536):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(2 * dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, dim)
        )

    def forward(self, cls_m, cls_p):
        fused = self.mlp(torch.cat([cls_m, cls_p], dim=-1))
        return fused
```

**ì¥ì **:
- âœ… Non-linear interaction
- âœ… ë” ë†’ì€ í‘œí˜„ë ¥

**ë‹¨ì **:
- âŒ íŒŒë¼ë¯¸í„° ì¦ê°€ (~4DÂ²)
- âŒ í•™ìŠµ ë‚œì´ë„ ì•½ê°„ ìƒìŠ¹

---

#### Option C: Cross-Attention

```python
class CrossAttentionClsFusion(nn.Module):
    def __init__(self, dim=768, num_heads=8):
        super().__init__()
        self.attn_block = nn.TransformerEncoderLayer(
            d_model=dim,
            nhead=num_heads,
            dim_feedforward=dim * 4,
            batch_first=True
        )

    def forward(self, cls_m, cls_p):
        # Stack as sequence
        cls_tokens = torch.stack([cls_m, cls_p], dim=1)  # [B, 2, D]

        # Self-attention (M â†” P interaction)
        cls_attended = self.attn_block(cls_tokens)  # [B, 2, D]

        # Aggregate (mean pooling)
        fused = cls_attended.mean(dim=1)  # [B, D]
        return fused
```

**ì¥ì **:
- âœ… Full M-P interaction
- âœ… Attention mapìœ¼ë¡œ í•´ì„ ê°€ëŠ¥
- âœ… Video(M ì¤‘ì‹¬) vs Static(P ì¤‘ì‹¬) ìë™ ì¡°ì ˆ

**ë‹¨ì **:
- âŒ Compute overhead (Q,K,V projection)
- âŒ íŒŒë¼ë¯¸í„° ì¦ê°€ (~6DÂ²)
- âŒ í•™ìŠµ ë¶ˆì•ˆì • ìœ„í—˜

---

#### Option D: Gating

```python
class GatingClsFusion(nn.Module):
    def __init__(self, dim=768):
        super().__init__()
        self.gate_fc = nn.Linear(2 * dim, 1)

    def forward(self, cls_m, cls_p):
        # Learnable gate: Î± âˆˆ [0, 1]
        gate = torch.sigmoid(self.gate_fc(torch.cat([cls_m, cls_p], dim=-1)))

        # Adaptive weighting
        fused = gate * cls_m + (1 - gate) * cls_p
        return fused
```

**ì¥ì **:
- âœ… Interpretable weighting
- âœ… íŒŒë¼ë¯¸í„° íš¨ìœ¨ì 
- âœ… Videoì—ì„œ Mâ†‘, Staticì—ì„œ Pâ†‘ ìë™

**ë‹¨ì **:
- âŒ Linear weighted sum (ì œí•œì  í‘œí˜„ë ¥)

---

#### ì‹¤í—˜ ê³„íš

**Phase 1: Pre-training**
- ê° fusion optionìœ¼ë¡œ ë…ë¦½ì ìœ¼ë¡œ pre-training ìˆ˜í–‰
- Video + Static (7:3) mixture
- DINO lossë¡œ í•™ìŠµ

**Phase 2: Evaluation**
- Pre-trained encoderë¡œ downstream task í‰ê°€
- Best performing fusion ì„ íƒ

**Metrics**:
- Pre-training convergence speed
- Downstream task success rate
- Ablation study ì„±ëŠ¥

---

### 4. Training Protocol

```python
class TwoStreamDINO(nn.Module):
    def __init__(self, dim=768, fusion_type='fc'):
        super().__init__()

        # Student & Teacher networks
        self.student_m = ViT_M(dim)
        self.student_p = ViT_P(dim)
        self.teacher_m = ViT_M(dim)  # EMA of student
        self.teacher_p = ViT_P(dim)  # EMA of student

        # CLS fusion module (ì‹¤í—˜ ëŒ€ìƒ)
        if fusion_type == 'fc':
            self.cls_fusion = LinearClsFusion(dim)
        elif fusion_type == 'mlp':
            self.cls_fusion = MLPClsFusion(dim)
        elif fusion_type == 'attention':
            self.cls_fusion = CrossAttentionClsFusion(dim)
        elif fusion_type == 'gating':
            self.cls_fusion = GatingClsFusion(dim)

        # DINO components
        self.student_head = DINOHead(dim, out_dim=65536, use_bn=False)
        self.teacher_head = DINOHead(dim, out_dim=65536, use_bn=False)

        # Initialize teacher as copy of student
        self._init_teacher()

    def _init_teacher(self):
        for param_s, param_t in zip(self.student_m.parameters(),
                                     self.teacher_m.parameters()):
            param_t.data.copy_(param_s.data)
            param_t.requires_grad = False

        for param_s, param_t in zip(self.student_p.parameters(),
                                     self.teacher_p.parameters()):
            param_t.data.copy_(param_s.data)
            param_t.requires_grad = False

    def forward(self, batch):
        """
        Args:
            batch: {
                'type': 'video' or 'static',
                'frames': [t0, t1, t2],
            }
        """
        frames = batch['frames']

        # Get multi-crop views with temporal augmentation
        views = get_dino_views_temporal(frames)

        # Student: process all views
        student_outputs = []
        for view_type, m_ch, p_ch in views:
            # Encode
            m_tokens = self.student_m(m_ch)  # [B, N+1, D]
            p_tokens = self.student_p(p_ch)  # [B, N+1, D]

            # Extract CLS
            cls_m = m_tokens[:, 0]
            cls_p = p_tokens[:, 0]

            # Fuse CLS (ì‹¤í—˜ ëŒ€ìƒ)
            cls_fused = self.cls_fusion(cls_m, cls_p)

            # Project to DINO output space
            out = self.student_head(cls_fused)
            student_outputs.append(out)

        # Teacher: only global views
        teacher_outputs = []
        with torch.no_grad():
            for view_type, m_ch, p_ch in views:
                if view_type == 'global':
                    m_tokens = self.teacher_m(m_ch)
                    p_tokens = self.teacher_p(p_ch)

                    cls_m = m_tokens[:, 0]
                    cls_p = p_tokens[:, 0]
                    cls_fused = self.cls_fusion(cls_m, cls_p)

                    out = self.teacher_head(cls_fused)
                    teacher_outputs.append(out)

        # DINO loss
        loss = dino_loss(student_outputs, teacher_outputs)

        return loss

    @torch.no_grad()
    def update_teacher(self, momentum=0.996):
        """EMA update of teacher networks"""
        for param_s, param_t in zip(self.student_m.parameters(),
                                     self.teacher_m.parameters()):
            param_t.data.mul_(momentum).add_((1 - momentum) * param_s.data)

        for param_s, param_t in zip(self.student_p.parameters(),
                                     self.teacher_p.parameters()):
            param_t.data.mul_(momentum).add_((1 - momentum) * param_s.data)
```

#### Training Loop

```python
# Hyperparameters
batch_size = 256
epochs = 300
warmup_epochs = 10
base_lr = 0.0005 * batch_size / 256

# Data mixture
video_ratio = 0.7
static_ratio = 0.3

# Training
for epoch in range(epochs):
    for batch in dataloader:
        # batch['type'] âˆˆ {'video', 'static'}
        # batch['frames'] = [t0, t1, t2]

        # Forward
        loss = model(batch)

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Update teacher (EMA)
        model.update_teacher(momentum=0.996)

        # DINO-specific: center + sharpen teacher outputs
        # (standard DINO mechanisms)
```

---

### 5. Change Representation via Video Prediction

**í•µì‹¬ ì² í•™**: ì´ë¯¸ì§€ ê°„ ë³€í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” representation í•™ìŠµ

> **"íš¨ê³¼ì "ì˜ ê¸°ì¤€**: ê·¸ representationë§Œìœ¼ë¡œ ë‹¤ìŒ ìˆœê°„ì„ ì •í™•íˆ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ê°€?

---

#### ì™œ Video Predictionì¸ê°€?

**í•µì‹¬ í†µì°°**: ë³€í™”(change)ë¥¼ ì˜ ì´í•´í–ˆë‹¤ë©´, ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤

```python
Goal: ì¥ë©´ ë³€í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” representation í•™ìŠµ

Input: (img_t, img_t+k) - ì—°ì†ëœ ë‘ ì´ë¯¸ì§€
Output: change_embedding - ë³€í™”ì˜ ë³¸ì§ˆì„ ë‹´ì€ representation
Validation: "ì´ embeddingìœ¼ë¡œ img_t+kë¥¼ ì¬êµ¬ì„±í•  ìˆ˜ ìˆëŠ”ê°€?"

â†’ ì¬êµ¬ì„± ì„±ëŠ¥ = representation qualityì˜ ì§ì ‘ì  ì¸¡ì •
```

**ì™œ ê°•ë ¥í•œê°€**:

1. **Cause-agnostic**: ë³€í™”ì˜ ì›ì¸ì„ êµ¬ë¶„í•˜ì§€ ì•ŠìŒ
   - ë¡œë´‡ íŒ”ì´ ë¬¼ì²´ë¥¼ ë°€ì—ˆë‚˜?
   - ì¤‘ë ¥ìœ¼ë¡œ ë¬¼ì²´ê°€ ë–¨ì–´ì¡Œë‚˜?
   - ì‚¬ëŒì´ ê°œì…í–ˆë‚˜?
   - **ìƒê´€ì—†ìŒ** - ë³€í™”ì˜ "ë³¸ì§ˆ"ë§Œ ìºì¹˜

2. **Self-validation**: ë³„ë„ì˜ downstream task ì—†ì´ í’ˆì§ˆ ì¸¡ì •
   - MAE: "íŒ¨ì¹˜ ë³µì› ì˜ ë˜ë‚˜?" (ì •ì  ì •ë³´)
   - DINO: "ë‹¤ë¥¸ viewì—ì„œë„ ê°™ì€ feature?" (ë¶ˆë³€ì„±)
   - **ìš°ë¦¬**: "ë‹¤ìŒ ìˆœê°„ ì˜ˆì¸¡ ì •í™•í•œê°€?" (ë™ì  ì´í•´)

3. **Universally useful**: ëª¨ë“  embodied AI taskì— í™œìš© ê°€ëŠ¥
   - Inverse dynamics: change â†’ action
   - Imitation learning: change â†’ policy
   - Planning: goal â†’ desired change sequence

---

#### ì „ì²´ íŒŒì´í”„ë¼ì¸: Forward â†’ Inverse â†’ Planning

**Phase 1: Pre-training (Forward Dynamics Learning)**

```python
# ëª©í‘œ: ì„¸ìƒì˜ ë¬¼ë¦¬ ë²•ì¹™ í•™ìŠµ (unsupervised)
Input: img_t (M, P channels)
Output: img_t+k reconstruction
Learning: "ì´ëŸ° ì´ˆê¸° ìƒíƒœì—ì„œëŠ” k step í›„ ì´ë ‡ê²Œ ë³€í•œë‹¤"
```

**í•™ìŠµë˜ëŠ” ê²ƒ**:
- ì¤‘ë ¥ (ë¬¼ì²´ê°€ ì•„ë˜ë¡œ ë–¨ì–´ì§)
- ê´€ì„± (ì›€ì§ì´ëŠ” ë¬¼ì²´ëŠ” ê³„ì† ì›€ì§ì„)
- ì¶©ëŒ (ë¬¼ì²´ë¼ë¦¬ ë¶€ë”ªíˆë©´ íŠ•ê¹€)
- ë³€í˜• (ë¶€ë“œëŸ¬ìš´ ë¬¼ì²´ëŠ” ëˆŒë¦¬ë©´ ì°Œê·¸ëŸ¬ì§)
- **ëª¨ë“  ì›ì¸ì˜ visual dynamicsë¥¼ í†µí•© í•™ìŠµ**

**Phase 2: Change Embedding Extraction**

```python
# ëª©í‘œ: ë³€í™”ë¥¼ ì••ì¶•ëœ í‘œí˜„ìœ¼ë¡œ ë³€í™˜
change_emb = encoder(img_t, img_t+k)
# ì˜ˆ: "5cm ì˜¤ë¥¸ìª½ ì´ë™ + 30ë„ íšŒì „"ì´ ë²¡í„°ë¡œ í‘œí˜„ë¨
```

**Phase 3: Inverse Dynamics Learning (Downstream)**

```python
# ëª©í‘œ: ë³€í™”ë¥¼ ë§Œë“  í–‰ë™ ì¶”ë¡ 
Input: (change_emb, img_t, sensors, task)
Output: robot_action
Learning: "ì´ëŸ° ë³€í™”ë¥¼ ë§Œë“¤ë ¤ë©´ ì´ëŸ° í–‰ë™ì„ í•´ì•¼ í•¨"
```

**Phase 4: Goal-Conditioned Control**

```python
# ëª©í‘œ: Taskë¥¼ ë³€í™”ë¡œ ë³€í™˜
planner(current_state, task) â†’ desired_change
action_model(desired_change, ...) â†’ robot_command
```

**ì´ê²ƒì€ ë¡œë´‡ ì¡°ì‘ì˜ ì •ì„ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤!**

---

#### ê¸°ì¡´ Video Prediction ë°©ë²•ë“¤ê³¼ì˜ ë¹„êµ

| ë°©ë²• | Action Input? | ë¬¼ë¦¬ í•™ìŠµ? | Two-Stream? | Pixel-wise? |
|------|--------------|-----------|-------------|-------------|
| Mathieu (2016) | âŒ | Implicit | âŒ | âŒ |
| Finn (2016) | âœ… (label í•„ìš”) | Explicit | âŒ | âŒ |
| Babaeizadeh (2018) | âŒ | Implicit | âŒ | âŒ |
| Watters (2017) | âŒ | Explicit | âŒ | âŒ |
| **ìš°ë¦¬ ë°©ë²•** | âŒ **(unsupervised!)** | **Implicit** | âœ… | âœ… |

**ìš°ë¦¬ì˜ ì°¨ë³„ì **:
1. **Action label ì—†ì´** visual dynamics í•™ìŠµ (ì§„ì •í•œ unsupervised)
2. **M-P split**ìœ¼ë¡œ motionê³¼ appearance ë¶„ë¦¬ í•™ìŠµ
3. **Pixel-wise embedding**ìœ¼ë¡œ spatial structure ë³´ì¡´
4. **Change-focused**: ë³€í™”ë¥¼ representationì˜ í•µì‹¬ìœ¼ë¡œ

---

#### êµ¬í˜„ Option 1: U-Net Decoder (Baseline)

**êµ¬ì¡°**:
```python
class TwoStreamVideoPredictor(nn.Module):
    def __init__(self, dim=768):
        super().__init__()

        # Two-Stream Encoders (ë³€í™” í•™ìŠµ)
        self.encoder_m = ViT_M(dim)
        self.encoder_p = ViT_P(dim)

        # Pixel-wise Fusion
        self.pixel_fusion = PixelwiseFusion(dim)
        self.cls_fusion = LinearClsFusion(dim)

        # Image Encoder (ì›ë³¸ ì´ë¯¸ì§€ ì¸ì½”ë”©)
        self.img_encoder = timm.create_model(
            'resnet50',
            pretrained=True,
            features_only=True
        )

        # U-Net Decoder
        self.decoder = UNetDecoder(
            encoder_channels=[256, 512, 1024, 2048],  # ResNet50
            decoder_channels=[256, 128, 64, 32],
            change_emb_dim=dim,
            out_channels=3  # RGB
        )

    def forward(self, img_t, img_tk):
        """
        Args:
            img_t: [B, 3, H, W] - ì‹œì‘ í”„ë ˆì„
            img_tk: [B, 3, H, W] - ëª©í‘œ í”„ë ˆì„ (k frames í›„)

        Returns:
            img_pred: [B, 3, H, W] - ì¬êµ¬ì„±ëœ img_tk
            change_emb: [B, D] - ë³€í™” embedding
        """
        # 1. M-P preprocessing (img_t â†’ img_tk ë³€í™”)
        m_channels = magnocellular_channel(img_t, img_tk)
        p_channels = parvocellular_channel(img_tk)

        # 2. Two-Stream encoding
        m_tokens = self.encoder_m(m_channels)  # [B, N+1, D]
        p_tokens = self.encoder_p(p_channels)  # [B, N+1, D]

        # 3. Pixel-wise fusion
        pixel_emb = self.pixel_fusion(m_tokens, p_tokens)  # [B, N, D]
        change_emb = self.cls_fusion(m_tokens[:, 0], p_tokens[:, 0])  # [B, D]

        # 4. img_t ì¸ì½”ë”© (í˜„ì¬ ìƒíƒœ)
        img_features = self.img_encoder(img_t)  # Multi-scale features

        # 5. Decode img_tk (ë³€í™” ì ìš©)
        img_pred = self.decoder(
            img_features=img_features,  # í˜„ì¬ ì´ë¯¸ì§€
            change_emb=change_emb,      # ë³€í™” ì •ë³´ (global)
            pixel_emb=pixel_emb         # ê³µê°„ì  ë³€í™” (pixel-wise)
        )

        return img_pred, change_emb
```

**UNetDecoder êµ¬í˜„**:
```python
class UNetDecoder(nn.Module):
    def __init__(self, encoder_channels, decoder_channels, change_emb_dim, out_channels=3):
        super().__init__()

        # Change embeddingì„ spatial mapìœ¼ë¡œ ë³€í™˜
        self.change_to_spatial = nn.Sequential(
            nn.Linear(change_emb_dim, 512),
            nn.ReLU(),
            nn.Unflatten(1, (512, 1, 1))
        )

        # Decoder blocks (skip connections with encoder)
        self.decoder_blocks = nn.ModuleList()
        in_ch = encoder_channels[-1] + 512  # ë§ˆì§€ë§‰ encoder + change_emb

        for out_ch in decoder_channels:
            self.decoder_blocks.append(
                nn.Sequential(
                    nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1),
                    nn.BatchNorm2d(out_ch),
                    nn.ReLU()
                )
            )
            in_ch = out_ch

        # Final output layer
        self.output = nn.Conv2d(decoder_channels[-1], out_channels, 3, 1, 1)

    def forward(self, img_features, change_emb, pixel_emb):
        """
        Args:
            img_features: List of [B, C_i, H_i, W_i] from ResNet
            change_emb: [B, D] - global change information
            pixel_emb: [B, N, D] - pixel-wise change (optional)

        Returns:
            img_pred: [B, 3, H, W]
        """
        # Change embedding â†’ spatial map
        change_map = self.change_to_spatial(change_emb)  # [B, 512, 1, 1]
        change_map = F.interpolate(
            change_map,
            size=img_features[-1].shape[-2:],  # Match encoder size
            mode='bilinear'
        )

        # Start from deepest encoder feature + change
        x = torch.cat([img_features[-1], change_map], dim=1)

        # Decode with skip connections
        for i, decoder_block in enumerate(self.decoder_blocks):
            x = decoder_block(x)

            # Skip connection (if available)
            if i < len(img_features) - 1:
                encoder_feat = img_features[-(i+2)]
                x = x + F.interpolate(encoder_feat, size=x.shape[-2:])

        # Final output
        img_pred = self.output(x)
        img_pred = torch.sigmoid(img_pred)  # [0, 1]

        return img_pred
```

**ì¥ì **:
- âœ… êµ¬í˜„ ê°„ë‹¨, í•™ìŠµ ì•ˆì •ì 
- âœ… Video prediction ë…¼ë¬¸ì—ì„œ ê²€ì¦ë¨
- âœ… Pixel-wise fusionê³¼ ì² í•™ ì¼ì¹˜
- âœ… Baselineìœ¼ë¡œ ìµœì 

**ë‹¨ì **:
- âŒ ê³ í•´ìƒë„ì—ì„œ ë©”ëª¨ë¦¬ ë§ì´ ì”€

---

#### êµ¬í˜„ Option 2: Latent Diffusion (Advanced)

**êµ¬ì¡°**:
```python
class LatentDiffusionPredictor(nn.Module):
    def __init__(self, change_emb_dim=768):
        super().__init__()

        # VAE encoder/decoder (frozen, pretrained)
        self.vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse")

        # Two-Stream change encoder
        self.encoder_m = ViT_M(change_emb_dim)
        self.encoder_p = ViT_P(change_emb_dim)
        self.fusion = LinearClsFusion(change_emb_dim)

        # Diffusion U-Net (í•™ìŠµ ëŒ€ìƒ)
        self.diffusion_unet = UNet2DConditionModel(
            cross_attention_dim=change_emb_dim
        )

        # Noise scheduler
        self.scheduler = DDPMScheduler()

    def forward(self, img_t, img_tk):
        """Training forward"""
        # img_t+kë¥¼ latentë¡œ ì¸ì½”ë”©
        with torch.no_grad():
            latent_tk = self.vae.encode(img_tk).latent_dist.sample()

        # Change embedding ì¶”ì¶œ
        m_ch = magnocellular_channel(img_t, img_tk)
        p_ch = parvocellular_channel(img_tk)

        m_tokens = self.encoder_m(m_ch)
        p_tokens = self.encoder_p(p_ch)
        change_emb = self.fusion(m_tokens[:, 0], p_tokens[:, 0])

        # Diffusion process
        noise = torch.randn_like(latent_tk)
        timestep = torch.randint(0, 1000, (latent_tk.size(0),))
        noisy_latent = self.scheduler.add_noise(latent_tk, noise, timestep)

        # Predict noise (conditioned on change_emb)
        pred_noise = self.diffusion_unet(
            noisy_latent,
            timestep,
            encoder_hidden_states=change_emb.unsqueeze(1)  # [B, 1, D]
        ).sample

        loss = F.mse_loss(pred_noise, noise)
        return loss

    @torch.no_grad()
    def generate(self, img_t, change_emb, num_steps=50):
        """Inference: img_t + change â†’ img_tk"""
        # img_t â†’ latent (starting point)
        latent_t = self.vae.encode(img_t).latent_dist.sample()

        # Denoising loop
        latent = torch.randn_like(latent_t)  # Random noise

        self.scheduler.set_timesteps(num_steps)
        for t in self.scheduler.timesteps:
            # Predict noise
            noise_pred = self.diffusion_unet(
                latent,
                t,
                encoder_hidden_states=change_emb.unsqueeze(1)
            ).sample

            # Remove noise
            latent = self.scheduler.step(noise_pred, t, latent).prev_sample

        # Decode latent â†’ image
        img_pred = self.vae.decode(latent).sample
        return img_pred
```

**ì¥ì **:
- âœ… **ìµœê³  í’ˆì§ˆ** (Stable Diffusion ìˆ˜ì¤€)
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (latent space ì‘ë™: 64Ã—64 instead of 256Ã—256)
- âœ… Pretrained VAE í™œìš© ê°€ëŠ¥
- âœ… ìµœì‹  íŠ¸ë Œë“œ (Diffusion Policy, GenRL ë“±ì—ì„œ ì‚¬ìš©)

**ë‹¨ì **:
- âŒ í•™ìŠµ ë³µì¡ë„ ë†’ìŒ
- âŒ Inference ëŠë¦¼ (50 denoising steps)

---

#### Training Protocol

**Dataset Composition**:
```python
# Video ë°ì´í„° (ë‹¤ì–‘í•œ ë¬¼ë¦¬ì  ë³€í™”)
- EgoDex (829h): Human manipulation demonstrations
- Something-Something V2 (220k): Object interaction videos
- Robot replay buffer: Behavioral trajectories

# Interval k ì „ëµ
k_min = 1   # ì‘ì€ ë³€í™” (ë¯¸ì„¸ ì¡°ì •)
k_max = 10  # í° ë³€í™” (ê¸´ í˜¸í¡)
k = random.randint(k_min, k_max)  # Variable interval
```

**Training Loop (U-Net Baseline)**:
```python
# Hyperparameters
batch_size = 128
epochs = 100
lr = 1e-4

# Model
model = TwoStreamVideoPredictor(dim=768)
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

# Training
for epoch in range(epochs):
    for batch in dataloader:
        video_frames = batch['frames']  # [B, T, 3, H, W]

        # Random interval k
        k = random.randint(1, 10)
        img_t = video_frames[:, 0]      # [B, 3, H, W]
        img_tk = video_frames[:, k]     # [B, 3, H, W]

        # Forward
        img_pred, change_emb = model(img_t, img_tk)

        # Loss: MSE reconstruction
        loss = F.mse_loss(img_pred, img_tk)

        # Optional: Perceptual loss (VGG features)
        loss_perceptual = perceptual_loss(img_pred, img_tk)
        loss = loss + 0.1 * loss_perceptual

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if step % 100 == 0:
            print(f"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}")
```

---

#### Downstream Usage (í•™ìŠµ í›„)

**Pre-training í›„ ì €ì¥**:
```python
torch.save({
    'encoder_m': model.encoder_m.state_dict(),
    'encoder_p': model.encoder_p.state_dict(),
    'fusion': model.cls_fusion.state_dict(),
    # Decoder ì €ì¥ ì•ˆ í•¨! (Pre-trainingìš© ë„êµ¬)
}, 'pretrained_change_encoder.pth')
```

**Inverse Dynamics Model (Downstream)**:
```python
class InverseDynamicsModel(nn.Module):
    def __init__(self, change_emb_dim=768, action_dim=7):
        super().__init__()

        # Load pretrained change encoder (frozen or fine-tunable)
        self.encoder_m = ViT_M(change_emb_dim)
        self.encoder_p = ViT_P(change_emb_dim)
        self.fusion = LinearClsFusion(change_emb_dim)

        # Action decoder (í•™ìŠµ ëŒ€ìƒ)
        self.action_head = nn.Sequential(
            nn.Linear(change_emb_dim + task_emb_dim, 512),
            nn.ReLU(),
            nn.Linear(512, action_dim)
        )

    def forward(self, img_t, img_t1, task_embedding):
        """
        Args:
            img_t, img_t1: Consecutive frames
            task_embedding: Task-conditioned context

        Returns:
            action: Robot action that produces this change
        """
        # Extract change embedding
        m_ch = magnocellular_channel(img_t, img_t1)
        p_ch = parvocellular_channel(img_t1)

        m_tokens = self.encoder_m(m_ch)
        p_tokens = self.encoder_p(p_ch)
        change_emb = self.fusion(m_tokens[:, 0], p_tokens[:, 0])

        # Combine with task
        combined = torch.cat([change_emb, task_embedding], dim=-1)

        # Predict action
        action = self.action_head(combined)
        return action

# Load pretrained weights
checkpoint = torch.load('pretrained_change_encoder.pth')
inverse_model = InverseDynamicsModel()
inverse_model.encoder_m.load_state_dict(checkpoint['encoder_m'])
inverse_model.encoder_p.load_state_dict(checkpoint['encoder_p'])
inverse_model.fusion.load_state_dict(checkpoint['fusion'])

# Fine-tune on robot demonstrations (20-30 demos)
# Only action_head is randomly initialized
```

---

#### í•µì‹¬ ì„¤ê³„ ì›ì¹™

**ìœ ì§€ (Downstreamì—ì„œ ì‚¬ìš©)**:
- âœ… `encoder_m`, `encoder_p`: Visual change ì¸ì½”ë”©
- âœ… `fusion`: M+P ê²°í•©
- âœ… **Change embeddingì´ í•µì‹¬ output**

**ë²„ë¦¼ (Pre-trainingìš© ë„êµ¬)**:
- âŒ `decoder`: ì¬êµ¬ì„± í’ˆì§ˆ ê²€ì¦ìš© (í•™ìŠµ í›„ ë¶ˆí•„ìš”)
- âŒ `img_encoder`: í˜„ì¬ í”„ë ˆì„ ì¸ì½”ë”©ìš© (downstreamì—ì„œ êµì²´ ê°€ëŠ¥)

**Video Predictionì˜ ì—­í• **:
- Change representation qualityì˜ **self-validation**
- ë³„ë„ label ì—†ì´ í•™ìŠµ ê°€ëŠ¥
- Downstream taskì—ì„œëŠ” decoder ë²„ë¦¬ê³  change_embë§Œ ì‚¬ìš©

**ì™œ ì´ ë°©ë²•ì¸ê°€**:
- ë¯¸ë˜ ì˜ˆì¸¡ = ë³€í™” ì´í•´ì˜ ì¦ê±°
- Pixel-level supervisionìœ¼ë¡œ spatial reasoning ê°•í™”
- M-P ëª¨ë‘ í•„ìš” (motion + appearance)
- Embodiment-independent visual dynamics í•™ìŠµ

---

## êµ¬í˜„: Modular Architecture

### Tier 1: Base Vision Encoder

```python
class TwoStreamVisionEncoder(nn.Module):
    """
    Core two-stream encoder.
    Standard output format (maximum flexibility).
    """
    def forward(self, img_prev, img_curr):
        """
        Returns:
            {
                "m_cls": [B, D],
                "m_patches": [B, N, D],
                "p_cls": [B, D],
                "p_patches": [B, N, D],
                "metadata": {...}
            }
        """
        m_channels = self.m_preprocessing(img_prev, img_curr)
        p_channels = self.p_preprocessing(img_prev)

        m_tokens = self.vit_m(m_channels)  # [B, N+1, D]
        p_tokens = self.vit_p(p_channels)  # [B, N+1, D]

        return {
            "m_cls": m_tokens[:, 0],
            "m_patches": m_tokens[:, 1:],
            "p_cls": p_tokens[:, 0],
            "p_patches": p_tokens[:, 1:],
        }
```

### Tier 2: Fusion Module (ì œì•ˆ ë°©ë²•)

```python
class PixelwiseFusion(nn.Module):
    """
    Proposed: Pixel-wise channel integration.
    Preserves spatial structure while reducing channel dimension.
    """
    def __init__(self, dim, fusion_type="separate"):
        super().__init__()
        self.fusion_type = fusion_type

        if fusion_type == "separate":
            # CLSì™€ patch ë³„ë„ fusion
            self.fc_cls = nn.Linear(2 * dim, dim)
            self.fc_patch = nn.Linear(2 * dim, dim)
        elif fusion_type == "shared":
            # ëª¨ë“  token ê³µìœ  fusion
            self.fc_fusion = nn.Linear(2 * dim, dim)
        elif fusion_type == "mlp":
            self.fc_cls = MLP(2 * dim, dim)
            self.fc_patch = MLP(2 * dim, dim)

    def forward(self, encoder_output):
        """
        Args:
            encoder_output: dict from TwoStreamVisionEncoder

        Returns:
            {
                "cls_fused": [B, D],
                "patches_fused": [B, N, D]
            }
        """
        m_cls = encoder_output["m_cls"]
        m_patches = encoder_output["m_patches"]
        p_cls = encoder_output["p_cls"]
        p_patches = encoder_output["p_patches"]

        if self.fusion_type == "separate":
            # CLS fusion
            cls_fused = self.fc_cls(torch.cat([m_cls, p_cls], dim=-1))

            # Pixel-wise patch fusion
            patches_concat = torch.cat([m_patches, p_patches], dim=-1)
            patches_fused = self.fc_patch(patches_concat)

        elif self.fusion_type == "shared":
            # All tokens use same fusion
            all_tokens = torch.cat([
                torch.cat([m_cls.unsqueeze(1), m_patches], dim=1),  # M
                torch.cat([p_cls.unsqueeze(1), p_patches], dim=1)   # P
            ], dim=-1)
            all_fused = self.fc_fusion(all_tokens)
            cls_fused = all_fused[:, 0]
            patches_fused = all_fused[:, 1:]

        return {
            "cls_fused": cls_fused,
            "patches_fused": patches_fused
        }
```

---

## dino.txt (2024)ì™€ì˜ ë¹„êµ

### ê·¼ë³¸ì  ì°¨ì´

| ì¸¡ë©´ | dino.txt | Pixel-wise Fusion (Ours) |
|------|----------|--------------------------|
| **Spatial info** | avg â†’ ì†ì‹¤ | ëª¨ë“  patch ë³´ì¡´ |
| **Output dim** | 4D (concat) | (N+1)Ã—D (all patches) |
| **Channel fusion** | Late (concat) | Early (pixel-wise) |
| **ì í•©í•œ task** | Classification, Retrieval | **Spatial reasoning, Manipulation** |
| **Efficiency** | Concat overhead | Fusionìœ¼ë¡œ ì ˆë°˜ ì¶•ì†Œ |
| **Gradient flow** | CLS + avgì—ë§Œ | ëª¨ë“  spatial locationì— |

### Why dino.txt Used Averaging

**Vision-language alignmentì—ì„œëŠ” ì¶©ë¶„**:
- Classification: "Is there a cat?" â†’ avgë¡œ ì¶©ë¶„
- Retrieval: "Image with red objects" â†’ avgë¡œ ì¶©ë¶„
- **Spatial reasoning ë¶ˆí•„ìš”**

**Behavior learningì—ì„œëŠ” ë¶€ì¡±**:
- Manipulation: "Where is the cat?" â†’ avg ë¶ˆì¶©ë¶„
- Spatial reasoning: "A left of B" â†’ avg ë¶ˆê°€ëŠ¥
- **Spatial structure í•„ìˆ˜**

---

## Ablation Study ì„¤ê³„

### Axis 1: Aggregation Strategy (í•µì‹¬)

**ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ ëª¨ë“  ë°©ë²•ì´ CLS + patches ì •ë³´ë¥¼ ì‚¬ìš©**

| Strategy | Input Features | Fusion Method | Output Dim | Spatial Info |
|----------|----------------|---------------|------------|--------------|
| **(A) avg + Late fusion** | [cls_m, avg(M)] + [cls_p, avg(P)] | concat | 4D | âœ— |
| **(B) avg + Early fusion** | [cls_m, avg(M)] + [cls_p, avg(P)] | FC | D | âœ— |
| **(C) Pixel-wise (ours)** | [cls_m, M_patches] + [cls_p, P_patches] | FC per pixel | (N+1)Ã—D | âœ… |
| **(D) All patches + Late** | [M_tokens] + [P_tokens] | concat | 2(N+1)Ã—D | âœ… |

**êµ¬í˜„**:

```python
# ê³µí†µ ì…ë ¥
cls_m, patches_m = m_tokens[:, 0], m_tokens[:, 1:]  # [B, D], [B, N, D]
cls_p, patches_p = p_tokens[:, 0], p_tokens[:, 1:]  # [B, D], [B, N, D]

# (A) avg + Late fusion (dino.txt style)
m_repr = torch.cat([cls_m, patches_m.mean(1)], dim=-1)  # [B, 2D]
p_repr = torch.cat([cls_p, patches_p.mean(1)], dim=-1)  # [B, 2D]
fused = torch.cat([m_repr, p_repr], dim=-1)  # [B, 4D]
# â†’ Spatial averagingìœ¼ë¡œ ìœ„ì¹˜ ì •ë³´ ì†ì‹¤
# â†’ Late fusion: í•™ìŠµ íŒŒë¼ë¯¸í„° ì—†ìŒ (ë‹¨ìˆœ concat)

# (B) avg + Early fusion
m_repr = torch.cat([cls_m, patches_m.mean(1)], dim=-1)  # [B, 2D]
p_repr = torch.cat([cls_p, patches_p.mean(1)], dim=-1)  # [B, 2D]
concat_repr = torch.cat([m_repr, p_repr], dim=-1)  # [B, 4D]
fused = fc_fusion(concat_repr)  # [B, D]
# â†’ Spatial averagingìœ¼ë¡œ ìœ„ì¹˜ ì •ë³´ ì†ì‹¤
# â†’ Early fusion: í•™ìŠµ ê°€ëŠ¥í•œ FCë¡œ ì°¨ì› ì¶•ì†Œ

# (C) Pixel-wise (ours) â­
cls_fused = fc_cls(torch.cat([cls_m, cls_p], dim=-1))  # [B, D]
patches_concat = torch.cat([patches_m, patches_p], dim=-1)  # [B, N, 2D]
patches_fused = fc_patch(patches_concat)  # [B, N, D]
# Output: cls_fused + patches_fused â†’ [B, N+1, D]
# â†’ Spatial structure ì™„ì „ ë³´ì¡´
# â†’ Pixel-wise early fusion: ê° ìœ„ì¹˜ì—ì„œ M+P ê²°í•©

# (D) All patches + Late
all_tokens = torch.cat([m_tokens, p_tokens], dim=1)  # [B, 2(N+1), D]
# â†’ Spatial structure ë³´ì¡´ë˜ì§€ë§Œ 2ë°° overhead
# â†’ Late fusion: CLS 2ê°œ, patches 2Nê°œ ëª¨ë‘ ìœ ì§€
```

### Axis 2: Fusion Mechanism (Pixel-wise ë‚´ì—ì„œ)

| Mechanism | Complexity | Params | Expressiveness |
|-----------|------------|--------|----------------|
| **FC (linear)** | Low | 2Dâ†’D | Low |
| **MLP** | Medium | 2Dâ†’2Dâ†’D | Medium |
| **Attention** | High | Q,K,V projection | High |
| **Gating** | Medium | Î±Â·M + (1-Î±)Â·P | Adaptive |

```python
# MLP
mlp_patch = nn.Sequential(
    nn.Linear(2*D, 2*D),
    nn.ReLU(),
    nn.Linear(2*D, D)
)

# Cross-Attention
class AttentionFusion(nn.Module):
    def forward(self, m_tokens, p_tokens):
        m2p = cross_attention(m_tokens, p_tokens)
        p2m = cross_attention(p_tokens, m_tokens)
        return (m2p + p2m) / 2

# Gating
gate = torch.sigmoid(gate_fc(torch.cat([m, p], dim=-1)))
fused = gate * m + (1 - gate) * p
```

### Axis 3: Weight Sharing

| Sharing | CLS Fusion | Patch Fusion | Rationale |
|---------|-----------|--------------|-----------|
| **Separate** | FC_cls | FC_patch | CLS=global, patch=local |
| **Shared** | Same FC | Same FC | ëª¨ë“  token ë™ì¼ ì²˜ë¦¬ |

### Axis 4: Gradient Flow Analysis

**Gradient path visualization**:

```
Loss
 â†“
Behavior Head
 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (A) avg + Late fusion          â”‚
â”‚  Loss â†’ [CLS_M;avg(M);CLS_P;avg(P)] â†’ avg â†’ Patches â”‚
â”‚         â†“       â†“               â”‚
â”‚      CLS_M   ì¼ë¶€ë§Œ gradient    â”‚ â† ëŒ€ë¶€ë¶„ patch ë¬´ì‹œ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (C) Pixel-wise (ours)          â”‚
â”‚  Loss â†’ [CLS;Patches] â†’ FC â†’ [CLS_M;Patches_M] â”‚
â”‚                                  CLS_P;Patches_P â”‚
â”‚         â†“          â†“            â”‚
â”‚      ëª¨ë“  CLS   ëª¨ë“  Patch       â”‚ â† ëª¨ë“  location
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## í‰ê°€ Metrics

### Standard Metrics

| Metric | Description | Why Important |
|--------|-------------|---------------|
| **Success Rate** | Task completion | Primary goal |
| **Sample Efficiency** | Demos needed | Data efficiency |
| **Training Time** | GPU hours | Computational cost |

### Spatial Reasoning Metrics (í•µì‹¬!)

| Metric | Description | Expected |
|--------|-------------|----------|
| **Position Accuracy** | ë¬¼ì²´ ìœ„ì¹˜ ì˜ˆì¸¡ ì˜¤ì°¨ (cm) | avg: ë†’ìŒ, pixel-wise: ë‚®ìŒ |
| **Spatial Relation** | "A left of B" íŒë‹¨ ì •í™•ë„ | avg: ë‚®ìŒ, pixel-wise: ë†’ìŒ |
| **Precise Manipulation** | ëª©í‘œ ì¢Œí‘œ ë„ë‹¬ ì„±ê³µë¥  | avg: ë‚®ìŒ, pixel-wise: ë†’ìŒ |

**êµ¬ì²´ì  Task**:

```python
tasks = {
    "pick_at_xy": "Pick object at precise location (x, y)",
    "place_left_of": "Place A to the left of B",
    "stack_centered": "Stack blocks with center alignment",
    "navigate_to": "Navigate to target position",
}

# ì˜ˆìƒ ê²°ê³¼
results = {
    "avg_late": {
        "pick_at_xy": 58.2,  # ìœ„ì¹˜ ì •ë³´ ë¶€ì¡±
        "place_left_of": 64.1,  # Spatial relation ì•½í•¨
        "stack_centered": 71.5,  # Global ì •ë³´ë¡œ ë¶€ë¶„ í•´ê²°
        "navigate_to": 72.3,
    },
    "pixelwise": {
        "pick_at_xy": 83.5,  # +25.3% (ìœ„ì¹˜ ëª…í™•)
        "place_left_of": 83.8,  # +19.7% (ê´€ê³„ íŒŒì•…)
        "stack_centered": 87.2,  # +15.7%
        "navigate_to": 84.4,  # +12.1%
    }
}
```

---

## ì˜ˆìƒ ì‹¤í—˜ ê²°ê³¼

### Table: Aggregation Strategy Comparison

| Method | Successâ†‘ | Position Acc.â†‘ | Memory | Time | Spatial Info |
|--------|----------|----------------|--------|------|--------------|
| avg + Late (dino.txt) | 74.2% | **61.5%** | 1.0Ã— | 1.0Ã— | âœ— |
| avg + Early | 73.8% | 60.8% | 0.5Ã— | 0.9Ã— | âœ— |
| **Pixel-wise (ours)** | **81.3%** | **78.9%** | 1.5Ã— | 1.2Ã— | âœ… |
| All patches + Late | 80.5% | 77.2% | 2.0Ã— | 1.5Ã— | âœ… |

**Key Findings**:
- Spatial preservation improves position accuracy by **17.4%** (61.5% â†’ 78.9%)
- Pixel-wise fusion achieves best performance with moderate overhead
- Early fusion without spatial preservation fails (avg + Early)

### Table: Task-Specific Breakdown

| Task Type | avg + Late | Pixel-wise | Improvement |
|-----------|-----------|------------|-------------|
| Global task (navigate) | 72.3% | 84.4% | +12.1% |
| Spatial relation (place left) | 64.1% | 83.8% | +19.7% |
| Precise manipulation (pick xy) | 58.2% | 83.5% | **+25.3%** |
| Average | 64.9% | 83.9% | +19.0% |

â†’ **Spatial reasoning tasks benefit most**

---

## Visualization

### Attention Map Comparison

```
Task: "Pick the red cube"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  avg(patches) ë°©ì‹           â”‚
â”‚  Attention map: [ê· ì¼ ë¶„ì‚°]  â”‚ â† ì–´ë””ë¥¼ ë´ì•¼í• ì§€ ëª¨ë¦„
â”‚  ğŸ”´ ë¬¼ì²´ ìœ„ì¹˜: ë¶ˆëª…í™•         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pixel-wise (ours)          â”‚
â”‚  Attention map:             â”‚
â”‚      ğŸ”´ â† ëª…í™•í•œ focus      â”‚
â”‚  ë¬¼ì²´ ìœ„ì¹˜: ì •í™•íˆ íŒŒì•…      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Learned Fusion Weights

```python
# Pixel-wise FC weights visualization
fc_patch.weight.shape = [D, 2D]

# M/P importance per dimension
m_importance = fc_patch.weight[:, :D].abs().mean()
p_importance = fc_patch.weight[:, D:].abs().mean()

# Task-specific patterns
grasping_task: M > P  (0.65 vs 0.35)  # Motion ì¤‘ì‹œ
navigation_task: P > M  (0.58 vs 0.42)  # Structure ì¤‘ì‹œ
```

---

## ë…¼ë¬¸ ì‘ì„± ì „ëµ

### Introduction: Motivation

```markdown
Vision-language methods [dino.txt, 2024] use avg(patches)
to create global descriptors, effective for classification
("Is there a cat?").

However, behavior learning fundamentally requires
**spatial reasoning** ("Where is the cat?").

Consider robotic manipulation: To pick a red cube,
the agent must know not only that a red cube exists
(captured by avg) but precisely **where it is located**
(lost in averaging).

We propose **pixel-wise channel fusion** that preserves
spatial structure while efficiently integrating temporal (M)
and spatial (P) information.
```

### Method: Clear Contrast

```markdown
### 3.3 Spatial-Preserving Channel Fusion

**Limitation of Spatial Averaging**:

Vision-language methods [dino.txt] create:
    representation = [CLS_M ; avg(patches_M) ; CLS_P ; avg(patches_P)]

While effective for classification, this **destroys spatial structure**
critical for behavior learning.

**Our Approach - Pixel-wise Fusion**:

We preserve all N spatial locations and fuse channels
at each position:

    for each patch location i:
        patch_i_fused = FC_patch([patch_Mi ; patch_Pi])

    representation = [CLS_fused, patch_1_fused, ..., patch_n_fused]

**Benefits**:
1. Complete spatial structure preservation
2. Pixel-wise M-P integration
3. 50% dimension reduction vs late concat
4. Gradient flows to all spatial locations
```

### Ablation: Key Evidence

```markdown
Table X: Impact of Spatial Information Preservation

| Aggregation | Spatial Info | Success | Position Acc. |
|-------------|-------------|---------|---------------|
| avg + concat | âœ— | 74.2% | 61.5% |
| **pixel-wise (ours)** | âœ… | **81.3%** | **78.9%** |

**Key Finding**: Spatial preservation improves position
accuracy by 17.4% (61.5% â†’ 78.9%), confirming that
behavior learning requires fine-grained spatial information.

**Task Breakdown**:
- Navigate (global): +12.1%
- Spatial relation: +19.7%
- Precise manipulation: **+25.3%** â† Largest gain

Figure X: Attention maps show that our method focuses on
task-relevant spatial locations while avg-based methods
display diffuse attention.
```

---

## ê´€ë ¨ ê°œë…

**ì£¼ìš” ì—°ê²°**:
- [[Two-Stream Image Preprocessing]] - M/P ì±„ë„ ìƒì„± (ì´ fusionì˜ ì…ë ¥)
- [[ë…¼ë¬¸ - Action-Agnostic Visual Behavior Representation]] - ì´ fusionì„ í™œìš©í•˜ëŠ” ë©”ì¸ ë…¼ë¬¸
- [[Sources/papers/dino.txt (2024)]] - Spatial averaging ì ‘ê·¼ë²• (ë¹„êµ ëŒ€ìƒ)

**ì´ë¡ ì  ë°°ê²½**:
- [[Two Visual Pathways]] - Dorsal/Ventral streams, "what" vs "where"
- Spatial reasoning in manipulation
- Information bottleneck theory

---

## êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ êµ¬í˜„

- [ ] TwoStreamVisionEncoder (standard interface)
- [ ] PixelwiseFusion module (FC, MLP variants)
- [ ] Baseline implementations (avg + late/early)
- [ ] Spatial reasoning evaluation metrics

### Ablation Experiments

- [ ] Aggregation strategy (avg vs pixel-wise)
- [ ] Fusion mechanism (FC vs MLP vs Attention)
- [ ] Weight sharing (separate vs shared)
- [ ] Task-specific breakdown analysis

### Visualization

- [ ] Attention map comparison
- [ ] Learned fusion weights
- [ ] Spatial reasoning task heatmaps

---

## ë©”íƒ€ë°ì´í„°

- **ì‘ì„±ì¼**: 2026-01-28
- **ê´€ë ¨ í”„ë¡œì íŠ¸**: [[ë…¼ë¬¸ - Action-Agnostic Visual Behavior Representation]]
- **í•µì‹¬ í†µì°°**: Spatial averagingì€ vision-languageì— ì í•©í•˜ì§€ë§Œ behavior learningì—ëŠ” ë¶€ì¡±. Spatial structure ë³´ì¡´ì´ manipulation taskì— í•„ìˆ˜.
- **dino.txtì™€ì˜ ì°¨ì´**: Classification â†’ avg ì¶©ë¶„ vs Manipulation â†’ pixel-wise í•„ìˆ˜

---

#concept #fusion #spatial-reasoning #behavior-learning #architecture #dino-txt #ablation
