# ë…¼ë¬¸ - Action-Agnostic Visual Behavior Representation

## ë©”íƒ€ë°ì´í„°

- **ìƒíƒœ**: Planning
- **ì‹œì‘ì¼**: 2025-12-10
- **ëª©í‘œ í•™íšŒ**: RSS 2026 (primary) / ICRA 2027 / CoRL 2026 (backup)
- **ê´€ë ¨ íŠ¹í—ˆ**: [[íŠ¹í—ˆ - ì‹œê³„ì—´ ì‹œê° ê´€ì°° ê¸°ë°˜ ì¡°ê±´ë¶€ í…ìŠ¤íŠ¸ ìƒì„± ì‹œìŠ¤í…œ]]

### RSS 2026 íˆ¬ê³  ì •ë³´

- **í•™íšŒ ì¼ì •**: 2026ë…„ 7ì›” 13-17ì¼
- **í•™íšŒ ì¥ì†Œ**: Sydney, Australia
- **Abstract ë§ˆê°**: 2026ë…„ 1ì›” ì¤‘ìˆœ~ë§ ì˜ˆì • (ë¯¸ë°œí‘œ)
- **Full paper ë§ˆê°**: Abstract ë§ˆê° í›„ ì•½ 1ì£¼ì¼ (ë¯¸ë°œí‘œ)
- **PDF ìˆ˜ì • ê°€ëŠ¥**: Full paper ë§ˆê° í›„ ì•½ 1ì£¼ì¼ ì˜ˆì •
- **Rebuttal**: 2026ë…„ 3ì›” ë§ ì˜ˆì •
- **ìµœì¢… ê²°ì •**: 2026ë…„ 4ì›” ì¤‘ìˆœ ì˜ˆì •
- **í˜ì´ì§€ ì œí•œ**: ì—†ìŒ (ë‹¨, Limitations ì„¹ì…˜ í•„ìˆ˜)
- **í‰ê°€ ê¸°ì¤€**: Novelty, Technical quality, Significance, Potential impact, Clarity

**ì°¸ê³ **: ê³µì‹ ì¼ì •ì€ ì•„ì§ ë¯¸ë°œí‘œ ìƒíƒœ. ê³¼ê±° RSS íŒ¨í„´(2025, 2024)ì„ ë°”íƒ•ìœ¼ë¡œ ì˜ˆìƒí•œ ì¼ì •ì„. ê³µì‹ ë°œí‘œ ì‹œ ì—…ë°ì´íŠ¸ í•„ìš”.

---

## í•œ ë¬¸ì¥ ìš”ì•½

Task descriptionê³¼ ì—°ì† ì´ë¯¸ì§€ë¡œë¶€í„° action-agnostic behavior representationì„ í•™ìŠµí•˜ì—¬, embodiment-independent robot learningì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

---

## í•µì‹¬ ì•„ì´ë””ì–´

### ë¬¸ì œ

ë¡œë´‡ë§ˆë‹¤ ë‹¤ë¥¸ action space â†’ ë°ì´í„° ì¬ì‚¬ìš© ë¶ˆê°€

### í•´ê²°ì±…

í–‰ë™ì˜ ë³¸ì§ˆì€ ì‹œê°ì  ë³€í™” â†’ Visual outcomeìœ¼ë¡œ behavior í‘œí˜„

### í•µì‹¬ í†µì°° (LAPA ê²€ì¦)

**ì‚¬ëŒ ë¹„ë””ì˜¤ > ë¡œë´‡ ë°ì´í„°**
- LAPA (ICLR 2025): ì‚¬ëŒ ë¹„ë””ì˜¤ë¡œ í•™ìŠµ ì‹œ ë¡œë´‡ ë°ì´í„°ë³´ë‹¤ ìš°ìˆ˜
- Visual changeì— action ì •ë³´ê°€ ë‹´ê¹€ (embodiment ë¬´ê´€)
- â†’ ìš°ë¦¬ ê°€ì„¤: **Task-conditioned visual behavior representation**

### ë” ê¹Šì€ ì² í•™: Forward/Inverse ë¶„ë¦¬ (2026-01-06 ì¶”ê°€)

> **"Actionì€ ì¸í„°í˜ì´ìŠ¤, ë³¸ì§ˆì€ Visual Flow"**

**í•µì‹¬ í†µì°°:**

Task ì„±ê³µ = ì˜¬ë°”ë¥¸ visual flow ìƒì„±

ë‘ ê°€ì§€ ì§€ì‹:
1. **Forward Knowledge (ì–´ë ¤ìš´ ë¶€ë¶„)**
   - "ì–´ë–¤ visual ë³€í™”ê°€ ì„±ê³µìœ¼ë¡œ ì´ì–´ì§€ëŠ”ê°€?"
   - Embodiment-independent
   - ì‚¬ëŒ ë¹„ë””ì˜¤ë¡œ í•™ìŠµ ê°€ëŠ¥
   - ë§ì€ ê²½í—˜ í•„ìš” â†’ 220k ë¹„ë””ì˜¤

2. **Inverse Knowledge (ì‰¬ìš´ ë¶€ë¶„)**
   - "ê·¸ ë³€í™”ë¥¼ ë§Œë“¤ë ¤ë©´ ì–´ë–¤ actionì„ ë‚´ì•¼ í•˜ë‚˜?"
   - Embodiment-specific
   - Forwardë¥¼ ì•Œë©´ ì—­ì¶”ë¡  ê°€ëŠ¥
   - ì ì€ ë°ëª¨ë¡œ ì¶©ë¶„ â†’ 20-30 demos

**ì™œ ì‚¬ëŒ ë¹„ë””ì˜¤ê°€ íš¨ê³¼ì ì¸ê°€:**
- ê¸°ì¡´ ì„¤ëª…: "Visual changeê°€ actionì„ í‘œí˜„"
- ë” ê¹Šì€ ì´ìœ : **Forwardë¥¼ ë¨¼ì € í•™ìŠµí•˜ë©´, InverseëŠ” ì‰½ë‹¤**
- ì‚¬ëŒ ë¹„ë””ì˜¤ = Forward knowledge ì œê³µ (ì–´ë ¤ìš´ ë¶€ë¶„)
- ë¡œë´‡ ë°ëª¨ = Inverse knowledge ì œê³µ (ì‰¬ìš´ ë¶€ë¶„)
- â†’ LAPAê°€ 220k ë¹„ë””ì˜¤ë¡œ ì„±ê³µí•œ ì´ìœ 

**Task-conditioningì˜ ì—­í• :**
- Forward í•™ìŠµì„ íš¨ìœ¨í™”
- Taskê°€ ì¤‘ìš”í•œ visual featureë¥¼ ëª…ì‹œ
- "ë¹¨ê°„ ì»µ" â†’ ìƒ‰ìƒ feature ê°•ì¡°
- "ë“¤ì–´ì˜¬ë¦¬ê¸°" â†’ ìˆ˜ì§ motion ê°•ì¡°
- â†’ ë¶ˆí•„ìš”í•œ ì •ë³´ ë¬´ì‹œ â†’ **ì ì€ ë°ì´í„°ë¡œ Forward í•™ìŠµ ê°€ëŠ¥**

**ê´€ë ¨ ì—°êµ¬ (ì´ë¯¸ ê²€ì¦ë¨):**
- Visual Foresight (2018): Video prediction â†’ Action planning
- DreamerV3 (2023): World model ë¨¼ì €, policy ë‚˜ì¤‘ì—
- Visual MPC: Dynamics í•™ìŠµ â†’ Closed-loop control

### êµ¬ì¡°

```
[ì—°ì† ì´ë¯¸ì§€] + [Task description]
    â†“ Cross-Attention
    â†“
CLS (universal behavior representation)
    â†“
Robot-specific decoder â†’ action
```

**ì°¨ë³„ì **:
- LAPA: Unsupervised latent action (VQ-VAE)
- ìš°ë¦¬: Task-conditioned visual representation (cross-attention)

---

## RSS 2024 íŠ¸ë Œë“œ ë¶„ì„

**ìš”ì•½**: Cross-embodiment learningê³¼ VLMì´ í•µì‹¬ íŠ¸ë Œë“œ. ìš°ë¦¬ ë…¼ë¬¸ì€ human-to-robot transferë¡œ ì°¨ë³„í™”. Real robot validationê³¼ strong baselines í•„ìˆ˜.

> [!info]- ğŸ“Š ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **ì£¼ìš” ì—°êµ¬ ë°©í–¥**
>
> **1. Vision-Language Models & Foundation Models** (10+ papers)
> - Octo: Open-source generalist robot policy
> - VLMPC: Vision-Language Model Predictive Control
> - RAG-Driver: Multi-modal LLM learning
> - Language-augmented planners
>
> **2. Cross-Embodiment Learning** (ê°•ë ¥í•œ íŠ¸ë Œë“œ)
> - MIRAGE: Cross-embodiment zero-shot transfer
> - "Pushing the Limits of Cross-Embodiment Learning"
> - Universal Manipulation Interface
>
> **3. Representation Learning**
> - 3D Diffusion Policy: Generalizable visuomotor learning
> - Human-oriented representation learning
> - HRP: Human affordances for pre-training
>
> **4. Large-Scale Datasets**
> - DROID: Large-scale manipulation dataset
> - RT-X style multi-embodiment data
>
> **5. Diffusion-Based Policies** (5+ papers)
> - Diffusion for policy learning
> - 3D representations
>
> **ìš°ë¦¬ ë…¼ë¬¸ì˜ ìœ„ì¹˜**
>
> **ê°•ì  (RSS íŠ¸ë Œë“œì™€ ë¶€í•©)**:
> - âœ… Cross-embodiment learning (í•µì‹¬ íŠ¸ë Œë“œ)
> - âœ… Vision-Language fusion (ì¸ê¸° ì£¼ì œ)
> - âœ… Foundation model ì ‘ê·¼ (ì‹œì˜ì ì ˆ)
> - âœ… Generalizable representation (ì£¼ìš” ê´€ì‹¬ì‚¬)
> - âœ… **ì‚¬ëŒ ë°ì´í„° í™œìš©** (LAPAë¡œ ê²€ì¦ëœ ì ‘ê·¼)
>
> **ì°¨ë³„ì  (ê¸°ì¡´ ì—°êµ¬ ëŒ€ë¹„)**:
> - **vs Octo**: Robot-to-robot transfer â†’ ìš°ë¦¬ëŠ” human-to-robot transfer
> - **vs LAPA**: Unsupervised latent action â†’ ìš°ë¦¬ëŠ” task-conditioned representation
> - **vs VC-1**: Task-agnostic visual encoder â†’ ìš°ë¦¬ëŠ” task-aware behavior encoder
>
> **ì•½ì  (RSS ìˆ˜ì¤€ ëŒ€ë¹„ ë¶€ì¡±)**:
> - âš ï¸ **ì‹¤ì œ ë¡œë´‡ ê²€ì¦ ì—†ìŒ**: RSSëŠ” real robot results ì¤‘ì‹œ (â†’ í•´ê²° ê°€ëŠ¥)
> - âš ï¸ **Baseline ë¹„êµ ë¶€ì¡±**: Octo, LAPA ë“±ê³¼ ì§ì ‘ ë¹„êµ í•„ìš” (â†’ ì½”ë“œ ê³µê°œë¨)
> - âš ï¸ **Quantitative results ë¯¸ë¹„**: Success rate, transfer efficiency ë“± (â†’ ì‹¤í—˜ í•„ìš”)
>
> **RSS ì±„íƒì„ ìœ„í•œ í•„ìˆ˜ ìš”ì†Œ**
>
> **Critical (ì—†ìœ¼ë©´ reject)**:
> 1. **ì‹¤ì œ ë¡œë´‡ ì‹¤í—˜**: ìµœì†Œ 1ê°œ embodimentì—ì„œ real-world validation
> 2. **ì •ëŸ‰ì  í‰ê°€**: Success rate, sample efficiency, transfer performance
> 3. **Strong baselines**: Octo, VC-1, R3M ë“±ê³¼ ë¹„êµ
> 4. **Ablation studies**: ê° componentì˜ ê¸°ì—¬ë„ ì…ì¦
>
> **Important (ìˆìœ¼ë©´ ê°•ë ¥)**:
> 1. Multi-embodiment dataset (3+ robots)
> 2. Zero-shot or few-shot transfer ì„±ê³µ ì‚¬ë¡€
> 3. Failure case ë¶„ì„ (Limitations ì„¹ì…˜)
> 4. Code/model release ê³„íš
>
> **Nice-to-have**:
> 1. Human evaluation
> 2. Long-horizon tasks
> 3. Sim-to-real transfer

---

## ë…¼ë¬¸ ìŠ¤í† ë¦¬ (Paper Narrative)

**ìš”ì•½**: ë¬¸ì œ(ë¡œë´‡ë§ˆë‹¤ ë‹¤ë¥¸ action space) â†’ í†µì°°(visual changeê°€ action í‘œí˜„) â†’ LAPA ê²€ì¦(ì‚¬ëŒ ë¹„ë””ì˜¤ > ë¡œë´‡ ë°ì´í„°) â†’ ìš°ë¦¬ ê¸°ì—¬(task-conditioned behavior representation)

> [!note]- ğŸ“– ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **Introduction: The Cross-Embodiment Challenge**
>
> **ë¬¸ì œ**: ë¡œë´‡ë§ˆë‹¤ ë‹¤ë¥¸ action space â†’ í•™ìŠµ ë°ì´í„° ì¬ì‚¬ìš© ë¶ˆê°€
> - 7-DoF arm, end-effector control, mobile manipulator ëª¨ë‘ ë‹¤ë¦„
> - ê° ë¡œë´‡ë§ˆë‹¤ ë³„ë„ ë°ì´í„° ìˆ˜ì§‘ í•„ìš” â†’ ë¹„ìš© ë†’ìŒ
>
> **Key Insight: Visual Behavior Representation**
>
> **í•µì‹¬ ì•„ì´ë””ì–´**: í–‰ë™ì˜ ë³¸ì§ˆì€ visual change
> - "Pick up object": action commandëŠ” ë‹¤ë¥´ì§€ë§Œ, visual outcomeì€ ë™ì¼
> - Temporal image sequenceì— action ì •ë³´ê°€ ë‹´ê²¨ ìˆìŒ
> - â†’ **Action-agnostic representation ê°€ëŠ¥**
>
> **Supporting Evidence: LAPA (ICLR 2025)**
>
> **ì„ í–‰ ì—°êµ¬ ê²€ì¦**:
> - ì‚¬ëŒ ë¹„ë””ì˜¤ â†’ ë¡œë´‡ ì „ì´: 36.8% success
> - ë¡œë´‡ ë°ì´í„° â†’ ë¡œë´‡ ì „ì´: 30.8% success
> - **ì‚¬ëŒ ë°ì´í„°ê°€ ë” ìš°ìˆ˜!**
> - â†’ Visual changeê°€ embodiment-independent action ì •ë³´ë¥¼ ë‹´ëŠ”ë‹¤ëŠ” ì¦ê±°
>
> **Our Contribution**
>
> **ê¸°ì¡´ ì—°êµ¬ ëŒ€ë¹„ ì°¨ë³„ì **:
>
> | Method | Approach | Limitation |
> |--------|----------|------------|
> | Octo | Robot-to-robot transfer | ë¡œë´‡ ë°ì´í„°ì—ë§Œ ì˜ì¡´ |
> | LAPA | Unsupervised latent action | Task ì •ë³´ í™œìš© ì•ˆ í•¨ |
> | VC-1 | Task-agnostic visual encoder | Behavior í‘œí˜„ ì•½í•¨ |
> | **Ours** | **Task-conditioned visual behavior** | - |
>
> **ìš°ë¦¬ ë°©ë²•**:
> - Cross-attentionìœ¼ë¡œ task description + temporal images ìœµí•©
> - CLS token = universal behavior representation
> - Robot-specific decoderë¡œ action ìƒì„±
> - â†’ **Task-aware + Action-agnostic**
>
> **Experimental Design**
>
> **Large-scale pretraining**:
> - EgoDex (829h ì‚¬ëŒ manipulation) + Something-Something V2
> - ê³ í’ˆì§ˆ visual behavior representation í•™ìŠµ
>
> **Robot finetuning**:
> - Bridge V2 (ê¹¨ë—í•œ ë¡œë´‡ ë°ì´í„°)
> - Decoderë§Œ í•™ìŠµ â†’ sample efficient
>
> **Real robot validation**:
> - ìì²´ ë¡œë´‡ì•”ìœ¼ë¡œ 3-5 tasks
> - Human-to-robot transfer ì¦ëª…
>
> **Baselines**:
> - Octo (robot-to-robot baseline)
> - VC-1 (visual representation baseline)
> - LAPA (human video baseline)
>
> **Expected Impact**
>
> **Contributions**:
> 1. Visual changeê°€ actionì„ í‘œí˜„í•¨ì„ ì…ì¦ (LAPA í™•ì¥)
> 2. Task-conditioned behavior representation ì œì•ˆ
> 3. Human videoë¥¼ ë¡œë´‡ í•™ìŠµì— í™œìš©í•˜ëŠ” ì‹¤ìš©ì  ë°©ë²•
> 4. Real robotì—ì„œ human-to-robot transfer ê²€ì¦
>
> **RSS ì í•©ì„±**:
> - Cross-embodiment learning (í•µì‹¬ íŠ¸ë Œë“œ)
> - Human data utilization (ìƒˆë¡œìš´ ë°©í–¥)
> - Foundation model for robotics (ì‹œì˜ì ì ˆ)

---

## Available Resources (ê°€ìš© ìì›)

**ìš”ì•½**: ë¡œë´‡ì•” âœ… | ì‚¬ëŒ ë°ì´í„°(EgoDex 829h, Sthv2 220k) âœ… | ë¡œë´‡ ë°ì´í„°(Bridge V2 60k) âœ… | Baseline(OpenVLA, SCRATCH í•„ìˆ˜ / VC-1, LAPA ì„ íƒ)

> [!note]- ğŸ’¾ ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **í•˜ë“œì›¨ì–´**
> - âœ… **ë¡œë´‡ì•” + ê·¸ë¦¬í¼**: Real robot validation ê°€ëŠ¥
> - ê°„ë‹¨í•œ manipulation task ìˆ˜í–‰ í™˜ê²½ êµ¬ì¶• ê°€ëŠ¥
>
> **ì˜¤í”ˆ ë°ì´í„°ì…‹**
>
> **í•µì‹¬ í†µì°°: ì‚¬ëŒ manipulation ë°ì´í„°ì˜ ìœ íš¨ì„±**
>
> **LAPA (ICLR 2025)** ì—°êµ¬ ê²°ê³¼:
> - ì‚¬ëŒ ë¹„ë””ì˜¤ë¡œ í•™ìŠµ â†’ ë¡œë´‡ ë°ì´í„°ë³´ë‹¤ **ìš°ìˆ˜í•œ ì„±ëŠ¥**
> - LAPA (ì‚¬ëŒ): 36.8% vs OpenVLA (ë¡œë´‡): 30.8% success rate
> - **30-40ë°° íš¨ìœ¨ì ** (272 H100-hrs vs 21,500 A100-hrs)
> - Something-Something V2 (220k ì‚¬ëŒ ë¹„ë””ì˜¤) ì‚¬ìš©
> - â†’ **Visual changeê°€ actionì„ í‘œí˜„í•œë‹¤ëŠ” ìš°ë¦¬ ê°€ì„¤ ê²€ì¦!**

> #### Human Manipulation (Primary Pretraining)
>
> **1. EgoDex (Apple, 2024)** - ìµœê³  í’ˆì§ˆ ì¶”ì²œ
> - **ê·œëª¨**: 829ì‹œê°„, 194 tasks, 338k episodes, 90M frames
> - **í’ˆì§ˆ**: Apple Vision Pro 3D hand tracking (21 joints per hand)
> - **íŠ¹ì§•**: Tabletop manipulation íŠ¹í™”, 1080p 30Hz
> - **ì ‘ê·¼ì„±**: GitHub (`apple/ml-egodex`), CC-by-NC-ND, 2TB
> - **í™œìš©**: ê³ í’ˆì§ˆ behavior representation pretraining
> - **ì¥ì **: ì •í™•í•œ hand pose, ë‹¤ì–‘í•œ task, ê³ í•´ìƒë„
>
> **2. Something-Something V2**
> - **ê·œëª¨**: 220k videos, 174 action categories
> - **ê²€ì¦**: LAPAì—ì„œ OpenVLA ëŠ¥ê°€ ì¦ëª…ë¨
> - **íŠ¹ì§•**: Object interactionì— íŠ¹í™”ëœ ì‚¬ëŒ action
> - **ì ‘ê·¼ì„±**: HuggingFace
> - **í™œìš©**: Large-scale pretraining
>
> #### Robot Manipulation (Finetuning & Evaluation)
>
> **3. Bridge V2** - ê°€ì¥ ê¹¨ë—í•œ ë¡œë´‡ ë°ì´í„°
> - **ê·œëª¨**: 60,096 trajectories, 24 environments
> - **í’ˆì§ˆ**: Controlled environment, skill diversity ìš°ìˆ˜
> - **íŠ¹ì§•**: Open X-Embodiment êµ¬ì„± ìš”ì†Œ ì¤‘ ìµœê³  í’ˆì§ˆ
> - **ì ‘ê·¼ì„±**: GitHub (`rail-berkeley/bridge_data_v2`), Creative Commons
> - **í™œìš©**: Robot finetuning ë° benchmark evaluation
>
> **4. DROID** (ì„ íƒì )
> - **ê·œëª¨**: 76k trajectories, 350 hours, 564 scenes, 86 tasks
> - **íŠ¹ì§•**: Scene diversity ì••ë„ì , but í’ˆì§ˆ ë¶ˆê· ì¼
> - **ì ‘ê·¼ì„±**: TFDS, HuggingFace
> - **í™œìš©**: Generalization í…ŒìŠ¤íŠ¸ìš©
>
> **Open X-Embodiment ì‚¬ìš© ì „ëµ**:
> - âš ï¸ ì „ì²´ 1M trajectoriesëŠ” í’ˆì§ˆ ë¶ˆê· ì¼ (ì•Œë ¤ì§„ ì´ìŠˆ)
> - âœ… Bridge V2, DROID ë“± ê°œë³„ ê³ í’ˆì§ˆ subsetë§Œ ì„ ë³„ ì‚¬ìš©
> - ì–‘ë³´ë‹¤ ì§ˆ ìš°ì„ 

> **Baseline Selection Strategy (ì¤‘ìš”!)**
>
> #### ì„ íƒí•œ Baselines ë° ê·¼ê±°
>
> **1. OpenVLA (í•„ìˆ˜ - Current SOTA)**
> - **ì„ íƒ ì´ìœ **:
>   - 2024ë…„ í˜„ì¬ VLA SOTA ëª¨ë¸ (LAPAê°€ ì´ê²ƒì„ ì´ê¹€)
>   - SOTAë¥¼ ì´ê²¨ì•¼ ë…¼ë¬¸ ì„¤ë“ë ¥ í™•ë³´
>   - êµ¬í˜„ ìš©ì´ì„±: HuggingFace ê¸°ë°˜, pretrained checkpoint ê³µê°œ
>   - ì˜ ì •ë¦¬ëœ ì½”ë“œë² ì´ìŠ¤ì™€ ë¬¸ì„œí™”
> - **ë¹„êµ í¬ì¸íŠ¸**:
>   - Human video pretraining vs Robot action-labeled pretraining
>   - Sample efficiency during finetuning
>   - Cross-embodiment transfer performance
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜†â˜†â˜†â˜† (ë§¤ìš° ì‰¬ì›€)
>   - Checkpoint ë‹¤ìš´ë¡œë“œ â†’ Finetuningë§Œ í•˜ë©´ ë¨
> - **ì‹œê°„ íˆ¬ì**: 1-2ì£¼ (finetuning + evaluation)
>
> **2. SCRATCH (í•„ìˆ˜ - Ablation baseline)**
> - **ì„ íƒ ì´ìœ **:
>   - Pretraining íš¨ê³¼ë¥¼ ì…ì¦í•˜ê¸° ìœ„í•œ í•„ìˆ˜ baseline
>   - êµ¬í˜„ ê³µì§œ: ìš°ë¦¬ backbone ê·¸ëŒ€ë¡œ downstream taskë¡œ finetuning
>   - ëª¨ë“  robot learning ë…¼ë¬¸ì˜ í‘œì¤€ baseline
> - **ë¹„êµ í¬ì¸íŠ¸**:
>   - Pretrainingì˜ ê°€ì¹˜ ì…ì¦
>   - Sample efficiency ì°¨ì´
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜†â˜†â˜†â˜† (ê³µì§œ)
> - **ì‹œê°„ íˆ¬ì**: 0ì£¼ (ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆìŒ)
>
> **3. LAPA-style Baseline (ì„ íƒì  - ë°©ë²•ë¡  ë¹„êµ)**
> - **ì„ íƒ ì´ìœ **:
>   - ê°€ì¥ ìœ ì‚¬í•œ ì ‘ê·¼ë²• (human video pretraining)
>   - ICLR 2025, ìµœì‹  ì—°êµ¬
>   - ìš°ë¦¬ì˜ ì°¨ë³„ì  ê°•ì¡°: VQ-VAE latent action vs Task-conditioned representation
> - **ë¹„êµ í¬ì¸íŠ¸**:
>   - Unsupervised latent action vs Supervised task-conditioned behavior
>   - Reconstruction objective vs Contrastive/supervised objective
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜…â˜†â˜† (ì¤‘ê°„)
>   - VQ-VAE êµ¬í˜„ í•„ìš”í•˜ì§€ë§Œ ë‹¨ìˆœí•œ í¸
>   - ë˜ëŠ” ê°„ì†Œí™”ëœ ë²„ì „ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥
> - **ì‹œê°„ íˆ¬ì**: 2-3ì£¼ (ì—¬ìœ  ìˆì„ ë•Œë§Œ)
> - **ëŒ€ì•ˆ**: LAPA ê²°ê³¼ë¥¼ ì¸ìš©ë§Œ í•˜ê³  ì§ì ‘ ë¹„êµëŠ” ìƒëµ ê°€ëŠ¥
>
> **4. Our Ablations (í•„ìˆ˜ - Component ë¶„ì„)**
> - **ë³€í˜•ë“¤**:
>   - w/o Task conditioning (visual only)
>   - w/o Cross-attention (concat ë°©ì‹)
>   - w/o Temporal modeling (single frame)
>   - Different pretraining objectives (DINO, SimCLR ë“±)
> - **ì„ íƒ ì´ìœ **: ê° componentì˜ ê¸°ì—¬ë„ ì…ì¦
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜†â˜†â˜† (ì‰¬ì›€ - ì´ë¯¸ êµ¬í˜„ëœ ê²ƒì˜ ë³€í˜•)
> - **ì‹œê°„ íˆ¬ì**: 1-2ì£¼
>
> #### ë°°ì œí•œ Baselines ë° ê·¼ê±°
>
> **1. Octo (ë°°ì œ - êµ¬í˜„ ë³µì¡ë„ ëŒ€ë¹„ ê°€ì¹˜ ë‚®ìŒ)**
> - **ë°°ì œ ì´ìœ **:
>   - êµ¬í˜„ ë³µì¡ë„: JAX ê¸°ë°˜, í™˜ê²½ ì„¸íŒ… ê¹Œë‹¤ë¡œì›€
>   - ì„±ëŠ¥: OpenVLAë³´ë‹¤ ë‚®ìŒ (OpenVLAê°€ Octo ê°œì„  ë²„ì „)
>   - Robot-to-robot transferë¼ëŠ” ë‹¤ë¥¸ ë¬¸ì œ ì„¤ì •
>   - ì‹œê°„ ëŒ€ë¹„ ì–»ëŠ” ì¸ì‚¬ì´íŠ¸ ì ìŒ
> - **ëŒ€ì‘ ë…¼ë¦¬** (ë¦¬ë·°ì–´ê°€ ë¬¼ì–´ë³¼ ê²½ìš°):
>   - "OctoëŠ” ë²”ìš©ì„±(generality)ì— ì´ˆì , ìš°ë¦¬ëŠ” ì„±ëŠ¥(performance)ì— ì´ˆì "
>   - "OpenVLAê°€ ë” ìµœì‹ ì´ê³  ì„±ëŠ¥ë„ ë†’ì•„ ë” ì ì ˆí•œ ë¹„êµ ëŒ€ìƒ"
>   - "Octoì˜ modular architecture vs ìš°ë¦¬ì˜ monolithic approachëŠ” ë‹¤ë¥¸ ì„¤ê³„ ì² í•™"
> - **ì‹œê°„ ì ˆì•½**: 3-4ì£¼
>
> **2. RT-2 (ë°°ì œ - ì¬í˜„ ë¶ˆê°€ëŠ¥)**
> - **ë°°ì œ ì´ìœ **:
>   - Google internal, ì½”ë“œ ë¯¸ê³µê°œ
>   - ì¬í˜„ ë¶ˆê°€ëŠ¥
>   - ë°ì´í„°ì…‹ë„ ë¹„ê³µê°œ
> - **ëŒ€ì‘ ë…¼ë¦¬**:
>   - "RT-2ëŠ” ì¬í˜„ ë¶ˆê°€ëŠ¥í•˜ì—¬ ê³µì •í•œ ë¹„êµ ì–´ë ¤ì›€"
>   - "OpenVLAê°€ ê³µê°œëœ ëŒ€ì•ˆìœ¼ë¡œ ë” ì ì ˆ"
>
> **3. VC-1 (ì„ íƒì  - ì‹œê°„ ìˆì„ ë•Œë§Œ)**
> - **ë¶€ë¶„ ì±„íƒ**:
>   - Visual representation quality ë¹„êµì—ëŠ” ìœ ìš©
>   - Linear probe evaluationìœ¼ë¡œ ê°„ë‹¨íˆ ë¹„êµ ê°€ëŠ¥
>   - Full baselineìœ¼ë¡œëŠ” ë¶ˆí•„ìš” (task-agnosticí•˜ë¯€ë¡œ)
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜†â˜†â˜† (ì¤‘ê°„ - pip installë¡œ ê°„ë‹¨)
> - **ì‹œê°„ íˆ¬ì**: 1ì£¼ (linear probeë§Œ)
> - **ê²°ì •**: Linear probe ê²°ê³¼ë§Œ í¬í•¨, full policy ë¹„êµëŠ” ìƒëµ
>
> **4. R3M (ë°°ì œ - VC-1ë¡œ ì¶©ë¶„)**
> - **ë°°ì œ ì´ìœ **:
>   - VC-1ê³¼ ê°™ì€ ì¹´í…Œê³ ë¦¬ (visual representation)
>   - VC-1ì´ ë” ìµœì‹ ì´ê³  ì„±ëŠ¥ ì¢‹ìŒ
>   - ë‘˜ ë‹¤ ë¹„êµí•˜ëŠ” ê±´ ì¤‘ë³µ
> - **ì‹œê°„ ì ˆì•½**: 2ì£¼
>
> **5. Diffusion Policy ê³„ì—´ (ë°°ì œ - ë¬¸ì œ ì„¤ì • ë‹¤ë¦„)**
> - **ë°°ì œ ì´ìœ **:
>   - Action generation ë°©ë²•ë¡ ì— ì´ˆì 
>   - ìš°ë¦¬ëŠ” representation learningì— ì´ˆì 
>   - Decoder ë¶€ë¶„ì€ êµì²´ ê°€ëŠ¥í•˜ë¯€ë¡œ ì§êµì (orthogonal)
> - **ëŒ€ì‘ ë…¼ë¦¬**:
>   - "Diffusion policyëŠ” ìš°ë¦¬ decoderë¡œ ëŒ€ì²´ ê°€ëŠ¥ (complementary)"
>   - "Representation vs Generationì€ ë‹¤ë¥¸ ì°¨ì›ì˜ ë¬¸ì œ"
>
> #### ìµœì¢… Baseline êµ¬ì„± (ìš°ì„ ìˆœìœ„)
>
> **Tier 1 (í•„ìˆ˜ - ì´ê²ƒ ì—†ìœ¼ë©´ ë…¼ë¬¸ ì•ˆ ë¨)**:
> 1. âœ… SCRATCH - Pretraining íš¨ê³¼ ì…ì¦
> 2. âœ… OpenVLA - SOTA ë¹„êµ
>
> **Tier 2 (ê°•ë ¥ ì¶”ì²œ - ìˆìœ¼ë©´ ë…¼ë¬¸ ê°•ë„ ìƒìŠ¹)**:
> 3. âœ… Our Ablations - Component ê¸°ì—¬ë„ ë¶„ì„
> 4. âš ï¸ VC-1 (Linear probe) - Visual representation quality
>
> **Tier 3 (ì„ íƒì  - ì‹œê°„ ì—¬ìœ  ìˆì„ ë•Œ)**:
> 5. âš ï¸ LAPA-style - ë°©ë²•ë¡  ì°¨ë³„í™” ê°•ì¡°
> 6. âš ï¸ VC-1 (Full policy) - ì™„ì „í•œ ë¹„êµ
>
> #### ì‹œê°„ ë°°ë¶„ ê¶Œì¥
>
> ```
> ë‚˜ìœ ì˜ˆ (í”¼í•´ì•¼ í•¨):
> - Baseline êµ¬í˜„: 6ê°œì›” (Octo, RT-2 ì¬í˜„ ì‹œë„, R3M, VC-1 ë“±)
> - ë³¸ì¸ ë°©ë²•: 1ê°œì›”
> - Writing: 1ì£¼
> â†’ ê²°ê³¼: Baselineì— ì§€ì³ ë³¸ì¸ ë°©ë²• ì™„ì„±ë„ ë‚®ìŒ
>
> ì¢‹ì€ ì˜ˆ (ê¶Œì¥):
> - Baseline: 1-2ê°œì›” (OpenVLA finetune + SCRATCH + Ablations)
> - ë³¸ì¸ ë°©ë²•: 3-4ê°œì›” (ì™„ì„±ë„ ë†’ì´ê¸°)
> - ì‹¤í—˜/ë¶„ì„: 1ê°œì›”
> - Writing: 1ê°œì›”
> â†’ ê²°ê³¼: ë³¸ì¸ ë°©ë²• ì™„ì„±ë„ ë†’ê³ , ì„¤ë“ë ¥ ìˆëŠ” ë…¼ë¬¸
> ```
>
> #### ë…¼ë¬¸ ì‘ì„± ì‹œ í‘œí˜„ ë°©ë²•
>
> **Method ì„¹ì…˜ì—ì„œ**:
> ```markdown
> We compare against the following baselines:
> - SCRATCH: Direct finetuning without pretraining
> - OpenVLA [Kim et al., 2024]: State-of-the-art VLA model
> - Ablations: Variants of our method to analyze component contributions
>
> We do not compare with Octo [Team et al., 2024] as it focuses on
> generality across diverse tasks and embodiments, while our work
> prioritizes performance on human-to-robot transfer. OpenVLA provides
> a more recent and stronger baseline for this comparison.
> ```
>
> **Related Work ì„¹ì…˜ì—ì„œ**:
> ```markdown
> While methods like Octo [Team et al., 2024] and RT-2 [Brohan et al., 2023]
> demonstrate impressive generalization, they rely on large-scale robot data.
> Recent work LAPA [Ye et al., 2024] shows promise in learning from human
> videos, but uses unsupervised latent actions without task conditioning.
> Our approach builds on this insight while introducing task-aware behavior
> representations.
> ```
>
> #### ë¦¬ë·°ì–´ ëŒ€ì‘ ì¤€ë¹„
>
> **ì˜ˆìƒ ì§ˆë¬¸ 1**: "Why not compare with Octo?"
> **ë‹µë³€**: "Octo focuses on cross-task generalization with modular architecture, while our work addresses human-to-robot transfer with monolithic design. OpenVLA provides a more direct and recent comparison point for VLA performance. Additionally, Octo's JAX implementation poses practical challenges for fair comparison in our PyTorch-based framework."
>
> **ì˜ˆìƒ ì§ˆë¬¸ 2**: "How does your method compare to RT-2?"
> **ë‹µë³€**: "RT-2's code and data are not publicly available, making direct comparison infeasible. We compare against OpenVLA, which represents the current state-of-the-art among reproducible methods."
>
> **ì˜ˆìƒ ì§ˆë¬¸ 3**: "Why not include more representation learning baselines like R3M?"
> **ë‹µë³€**: "We include VC-1 as the representative visual representation baseline, which is more recent and performs better than R3M. Adding multiple baselines from the same category (task-agnostic visual representations) would not provide additional insights into our core contribution: task-conditioned behavior representations."
>
> ---
>
> **Previously Listed Baseline Models (ì°¸ê³ ìš©)**
>
> **Octo** - RSS 2024 (ë°°ì œ)
> - **ì„±ëŠ¥**: VC-1, RT-1-X ëŒ€ë¹„ í‰ê·  52% í–¥ìƒ
> - **êµ¬ì¡°**: Transformer-based diffusion policy
> - **í•™ìŠµ ë°ì´í„°**: 800k episodes from Open X-Embodiment
> - **ì½”ë“œ**: `octo-models/octo` (JAX)
> - **ë°°ì œ ì´ìœ **: êµ¬í˜„ ë³µì¡, OpenVLAê°€ ë” ë‚˜ìŒ
>
> **VC-1** - (ì„ íƒì )
> - **ì„±ëŠ¥**: Best prior visual representation for embodied AI
> - **í•™ìŠµ ë°ì´í„°**: 4,000+ hours egocentric video + ImageNet
> - **ì½”ë“œ**: `facebookresearch/eai-vc`
> - **ì‚¬ìš©ë²•**: pip install vc_models
> - **í™œìš©**: Linear probe evaluationë§Œ

---

## Gap Analysis & Action Plan

**ìš”ì•½**: í•„ìˆ˜ ìš”ì†Œ(Real robot âœ…, Multi-embodiment data âœ…, Baselines âœ…) í™•ë³´. RSS 2026 13ê°œì›” íƒ€ì„ë¼ì¸ìœ¼ë¡œ ì¶©ë¶„í•œ ì‹¤í—˜ ê°€ëŠ¥.

> [!tip]- ğŸ¯ ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **í˜„ì¬ ìƒíƒœ vs RSS ìš”êµ¬ì‚¬í•­**
>
> | ìš”êµ¬ì‚¬í•­ | í˜„ì¬ ìƒíƒœ | ê°€ìš© ìì› | í•„ìš” ì‘ì—… |
> |---------|---------|---------|---------|
> | Real robot demo | ì¤€ë¹„ë¨ | ë¡œë´‡ì•” + ê·¸ë¦¬í¼ | Task ì„¤ê³„ ë° ì‹¤í—˜ |
> | Multi-embodiment data | âœ… | RT-X, Bridge V2, DROID | ë°ì´í„° ë¡œë”© íŒŒì´í”„ë¼ì¸ |
> | Baseline ë¹„êµ | âœ… | Octo, VC-1 ì½”ë“œ ê³µê°œ | í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± |
> | Ablation study | ì§„í–‰ í•„ìš” | - | Componentë³„ ë¶„ì„ |
> | Quantitative metrics | ì§„í–‰ í•„ìš” | - | Success rate, transfer efficiency |
> | Limitations ì„¹ì…˜ | ì§„í–‰ í•„ìš” | - | Failure cases ë¶„ì„ |
>
> **ê·¹ë³µ ì „ëµ (ìˆ˜ì •)**
>
> **Option 1: RSS 2025 (Aggressive but Feasible, 5ì£¼)**
> ```
> íƒ€ì„ë¼ì¸:
> - Week 1-2: Model êµ¬í˜„ + Open X-Embodiment í•™ìŠµ
> - Week 2-3: Baseline ë¹„êµ (Octo, VC-1)
> - Week 3-4: Real robot task ì„¤ê³„ ë° ì‹¤í—˜
> - Week 4-5: ë…¼ë¬¸ ì‘ì„± + Ablation studies
> â†’ íƒ€ì´íŠ¸í•˜ì§€ë§Œ í•˜ë“œì›¨ì–´ + ë°ì´í„° + ì½”ë“œ í™•ë³´ë¡œ ì‹¤í–‰ ê°€ëŠ¥
> â†’ ìœ„í—˜: ì¤‘ê°„ì— í° ë¬¸ì œ ë°œìƒ ì‹œ ëŒ€ì‘ ì–´ë ¤ì›€
> ```
>
> **Option 2: CoRL 2025 ë˜ëŠ” ICRA 2026 (Recommended)**
> ```
> ì¶©ë¶„í•œ ì‹œê°„ í™•ë³´:
> - 3-4ê°œì›” ì‹¤í—˜ ê¸°ê°„
> - Multiple embodiments ì‹¤í—˜ (ìì²´ ë¡œë´‡ + public data)
> - Extensive ablations
> - ì™„ì„±ë„ ë†’ì€ ë…¼ë¬¸
> â†’ ì±„íƒ í™•ë¥  ë†’ìŒ, ë” ê°•ë ¥í•œ ê²°ê³¼
> ```
>
> **Option 3: RSS 2025 Workshop**
> ```
> ì´ˆê¸° ê²€ì¦:
> - í•µì‹¬ ì•„ì´ë””ì–´ proof-of-concept
> - ì»¤ë®¤ë‹ˆí‹° í”¼ë“œë°±
> - Main conferenceëŠ” ë‹¤ìŒ í•´
> â†’ ì•ˆì „í•œ ì„ íƒ
> ```
>
> **ìµœì†Œ ì‹¤í–‰ ê°€ëŠ¥ ë…¼ë¬¸ (MVP for RSS 2025)**
>
> **5ì£¼ ì•ˆì— ê°€ëŠ¥í•œ ê²ƒ** (í•˜ë“œì›¨ì–´ + ë°ì´í„° í™•ë³´):
> 1. âœ… Open X-Embodimentë¡œ í•™ìŠµ
> 2. âœ… Bridge V2, DROIDë¡œ í‰ê°€
> 3. âœ… Octo, VC-1 baseline ë¹„êµ (pretrained í™œìš©)
> 4. âœ… Real robot validation (1 embodiment, 3-5 tasks)
> 5. âœ… Linear probe evaluation
> 6. âœ… Basic ablation studies
>
> **ì—¬ì „íˆ ì–´ë ¤ìš´ ê²ƒ**:
> 1. âš ï¸ Multiple real robot embodiments (í•˜ë‚˜ë§Œ ê°€ëŠ¥)
> 2. âš ï¸ Large-scale ablations (ì‹œê°„ ë¶€ì¡±)
> 3. âš ï¸ Extensive failure analysis
>
> **ì—…ë°ì´íŠ¸ëœ ê²°ë¡ **:
> - RSS 2025 main conference: **ë„ì „ ê°€ëŠ¥** (í•˜ì§€ë§Œ ë¦¬ìŠ¤í¬ ìˆìŒ)
> - CoRL/ICRA: **ë” ê°•ë ¥í•œ ê²°ê³¼** ê¸°ëŒ€
> - ìµœì¢… ê²°ì •ì€ **Week 2-3 ì‹¤í—˜ ê²°ê³¼ ë³´ê³  íŒë‹¨** ê¶Œì¥

---

## ì‹¤í—˜ ê³„íš (RSS 2026 - ìµœì¢… í™•ì •)

**ìš”ì•½**: Stage 1 (SIMPLER - OpenVLA vs Ours) â†’ Stage 2 (Franka - 4 methods, 10x faster training) â†’ Phase 3 (Analysis & Writing)

> [!example]- ğŸ”¬ ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **ì „ëµ: Two-Stage Evaluation**
>
> **í•µì‹¬ ì›ì¹™**:
> 1. Stage 1 (SIMPLER): Fair comparison with baselines (ê°™ì€ WidowX embodiment)
> 2. Stage 2 (Franka): Cross-embodiment transfer efficiency ê²€ì¦ (ìš°ë¦¬ ì‹œìŠ¤í…œ)

>
> ---
>
> **Stage 1: SIMPLER Benchmark Evaluation**
>
> **íƒ€ì„ë¼ì¸**: 2026ë…„ 1ì›” ~ 2ì›” (ì•½ 6ì£¼)
>
> **ëª©ì **: Standard benchmarkì—ì„œ ê³µì •í•œ ë¹„êµ
>
> **Environment**:
> - SIMPLER simulation (WidowX)
> - BridgeData V2 tasks (4-6ê°œ tasks)
>
> **Baselines (ë³´ìˆ˜ì  ê³„íš)**:
>
> #### **Tier 1: í•„ìˆ˜ (ìµœì†Œ êµ¬ì„±)**
> 1. âœ… **SCRATCH** (From-scratch on SIMPLER tasks)
>    - ëª©ì : Pretraining íš¨ê³¼ ì…ì¦
>    - êµ¬í˜„: Diffusion Policy from scratch
>    - ì‹œê°„: 1ì£¼ (ì´ë¯¸ ì½”ë“œ ì¡´ì¬)
>
> 2. âœ… **OpenVLA** (Pre-trained checkpoint)
>    - ëª©ì : SOTA ë¹„êµ
>    - êµ¬í˜„: Checkpoint ë‹¤ìš´ë¡œë“œë§Œ
>    - ì‹œê°„: 0ì¼ (ê³µì§œ)
>
> **ì´ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„í•œ ì´ìœ **:
> - Pretraining íš¨ê³¼ ì…ì¦ (Scratch vs Ours)
> - SOTA ë¹„êµ (OpenVLA vs Ours)
> - RSS 2024 LAPAë„ ì´ êµ¬ì„±ìœ¼ë¡œ accept
>
> #### **Tier 2: ì¶”ê°€ ì˜µì…˜ (ì—¬ìœ  ìˆìœ¼ë©´)**
> 3. â­ **Diffusion Policy** (Baseline)
>    - ëª©ì : Alternative decoder ë¹„êµ
>    - êµ¬í˜„: ê³µì‹ ì½”ë“œ ì‚¬ìš©
>    - ì‹œê°„: 1ì£¼
>    - **íŒë‹¨ ê¸°ì¤€**: 2ì›” ë§ ì§„í–‰ ìƒí™© ë³´ê³  ê²°ì •
>
> 4. â­ **Octo** (ì„ íƒì )
>    - ëª©ì : ë‹¤ë¥¸ VLA ë¹„êµ
>    - êµ¬í˜„: ê³µì‹ checkpoint
>    - ì‹œê°„: 1ì£¼
>    - **íŒë‹¨ ê¸°ì¤€**: ì‹œê°„ ì—¬ìœ  ìˆì„ ë•Œë§Œ
>
> **êµ¬í˜„**:
> ```bash
> # Repository: SimplerEnv-OpenVLA
> git clone https://github.com/DelinQu/SimplerEnv-OpenVLA
> # OpenVLA checkpoint + evaluation scripts ì œê³µë¨
> ```
>
> **Expected Results**:
> ```
> Method          | Success Rate | Grasp | Moving | Notes
> ----------------|--------------|-------|--------|-------
> SCRATCH         | ~20-30%      | -     | -      | No pretraining
> OpenVLA         | 36.4%        | 50.0% | 67.7%  | Current SOTA
> LAPA (reported) | 57.3%        | 71.9% | 77.1%  | Target to beat
> Ours (target)   | >40%         | -     | -      | Conservative goal
> ```
>
> ---
>
> **Stage 2: Real Franka Cross-Embodiment Transfer**
>
> **ëª©ì **: Embodiment-independenceì™€ transfer efficiency ê²€ì¦
>
> **Setup**:
> - Robot: Franka Panda (7-DoF)
> - Gripper: Robotiq 2F-85
> - Camera: RealSense D405 (wrist-mounted)
> - Tasks: 1-2 representative tasks
>   - Task 1: "Pick cup and place in bowl"
>   - Task 2 (optional): "Stack blocks"
>
> **Data Collection** (Week 3):
> - [ ] Collect 100 demos per task
>   - 80 demos: Training
>   - 20 demos: Validation
> - [ ] Evaluation: 50 rollouts (ìƒˆë¡œìš´ object positions)
> - [ ] ì†Œìš” ì‹œê°„: ~3-4 hours per task
>
> **Methods Compared** (Week 4):
>
> 1. **Zero-shot Transfer** â­ (1ì¼)
>    - OpenVLA checkpoint â†’ Franka ì§ì ‘ ì ìš©
>    - Action space mapping only (6D â†’ 7D)
>    - ì˜ˆìƒ: 5-15% (ê±°ì˜ ì‹¤íŒ¨í•˜ì§€ë§Œ 0ì€ ì•„ë‹˜)
>    - ì˜ë¯¸: "Naive transferëŠ” ì‘ë™ ì•ˆ í•¨"
>
> 2. **From-Scratch** â­â­ (1ì£¼)
>    - Diffusion Policyë¥¼ Franka ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµ
>    - 80 demos, ~25 hrs training
>    - ì˜ˆìƒ: 50-60%
>    - ì˜ë¯¸: "Pretraining ì—†ì´ ì´ ì •ë„"
>
> 3. **Full Fine-tuning (OpenVLA)** â­â­â­ (2ì£¼)
>    - OpenVLA â†’ Franka ì „ì²´ ì¬í•™ìŠµ
>    - 80 demos, ~45 hrs training
>    - ì˜ˆìƒ: 65-75%
>    - ì˜ë¯¸: "SOTA VLAë„ ì „ì²´ ì¬í•™ìŠµ í•„ìš”"
>
> 4. **Ours (Decoder-only)** â­â­â­â­ (3ì¼)
>    - Bridge V2 representation (frozen) â†’ Franka decoderë§Œ í•™ìŠµ
>    - 80 demos, ~5 hrs training
>    - ì˜ˆìƒ: 75-85%
>    - ì˜ë¯¸: "Efficient adaptation + best performance"
>
> **Expected Table 2**:
> ```
> Method                  | Pretraining | Training | Time | Success
> ------------------------|-------------|----------|------|--------
> Zero-shot (OpenVLA)     | Bridge V2   | 0 demos  | 0h   | 8%
> From-scratch (Diffusion)| None        | 80 demos | 25h  | 58%
> Full Finetune (OpenVLA) | Bridge V2   | 80 demos | 45h  | 72%
> Ours (Decoder-only)     | Bridge V2   | 80 demos | 5h   | 82%
>                                                   â†‘10x faster â†‘best
> ```
>
> **í•µì‹¬ Message**:
> - âœ… Best Performance: 82% > 72% > 58% > 8%
> - âœ… 10x Faster: 5hrs vs 45hrs
> - âœ… Same Data: ëª¨ë‘ 80 demos (ê³µì • ë¹„êµ)
> - âœ… Pretraining Value: +24% over from-scratch
>
> ---
>
> **Phase 3: Analysis & Writing (Week 5-6)**
>
> - [ ] Data efficiency curve (10, 20, 40, 80 demos)
> - [ ] Component ablations
> - [ ] Failure case analysis
> - [ ] ë…¼ë¬¸ ì‘ì„±
> - [ ] Limitations ì„¹ì…˜
>
> ---
>
> **ğŸš« DROID ì‚¬ìš© ì•ˆ í•¨ (ì´ìœ )**
>
> **ë¬¸ì œì **:
> 1. Camera mismatch: DROID (Zed) vs Ours (RealSense D405)
> 2. Environment mismatch: 564 unknown scenes vs Our lab
> 3. Task mismatch: 86 diverse tasks vs Our specific tasks
> 4. â†’ ì˜ˆìƒ success rate â‰ˆ 0-5% (ì„¤ëª…í•˜ê¸° ì–´ë ¤ì›€)
>
> **ëŒ€ì•ˆ**:
> - ìš°ë¦¬ í™˜ê²½ì—ì„œ ì§ì ‘ ë°ì´í„° ìˆ˜ì§‘
> - Controlled experiment (ëª¨ë“  methods ê°™ì€ ì¡°ê±´)
> - ì„¤ë“ë ¥ ê·¹ëŒ€í™”

---

## ğŸ“š êµ¬í˜„ ì°¸ê³  ìë£Œ (Code & Resources)

**ìš”ì•½**: SIMPLER(SimplerEnv-OpenVLA) | Franka(Diffusion Policy, OpenVLA finetune) | Data(Bridge V2 60k, EgoDex 829h, Sthv2 220k)

> [!info]- ğŸ’» ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **Stage 1: SIMPLER Evaluation**
>
> **SimplerEnv-OpenVLA**:
> - Repository: https://github.com/DelinQu/SimplerEnv-OpenVLA
> - Official: https://github.com/simpler-env/SimplerEnv
> - Paper: "Evaluating Real-World Robot Manipulation Policies in Simulation" (CoRL 2024)
> - í¬í•¨ ë‚´ìš©:
>   - âœ… OpenVLA evaluation scripts
>   - âœ… RT-1, Octo evaluation scripts
>   - âœ… BridgeData V2 tasks (4-6ê°œ)
>   - âœ… Visual matching setup
>   - âœ… Pre-trained checkpoints
>
> **OpenVLA Checkpoint**:
> - HuggingFace: https://huggingface.co/openvla/openvla-7b
> - Repository: https://github.com/openvla/openvla
> - ì‚¬ìš©ë²•:
> ```python
> from transformers import AutoModel
> model = AutoModel.from_pretrained("openvla/openvla-7b")
> ```
>
> ---
>
> **Stage 2: Franka Implementation**
>
> **Baseline A: From-Scratch (Diffusion Policy)**:
> - Repository: https://github.com/real-stanford/diffusion_policy
> - Paper: "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion" (RSS 2023)
> - íŠ¹ì§•:
>   - âœ… Franka Panda ê²€ì¦ë¨
>   - âœ… Pick-and-place, pushing, pouring tasks
>   - âœ… Configuration files ì œê³µ
>
> **Baseline B: OpenVLA Fine-tuning**:
> - Fine-tuning guide: https://github.com/openvla/openvla/blob/main/docs/TRAINING.md
> - Requirements: 8 A100 GPUs (ë˜ëŠ” gradient accumulation)
> - Action space adapter êµ¬í˜„ í•„ìš” (6D â†’ 7D)
>
> **Franka + 2F-85 Integration**:
> - Official Gripper Example: https://github.com/frankaemika/external_gripper_example
> - Robotiq 2F-85 Driver:
> ```bash
> sudo apt install ros-noetic-robotiq-2f-gripper-control
> ```
>
> **RealSense D405**:
> - SDK: https://github.com/IntelRealSense/librealsense
> - ROS Wrapper:
> ```bash
> sudo apt install ros-noetic-realsense2-camera
> ```
>
> **Teleoperation (Data Collection)**:
> - FrankaTeleop: https://github.com/gjcliff/FrankaTeleop
> - ë˜ëŠ” VR controller ì‚¬ìš©
>
> ---
>
> **Datasets**
>
> **BridgeData V2** (í•„ìˆ˜):
> - Website: https://rail.eecs.berkeley.edu/datasets/bridge_release/data/
> - Repository: https://github.com/rail-berkeley/bridge_data_v2
> - Format: TensorFlow Datasets (RLDS)
> - Size: 60K trajectories, ~200GB
>
> **DROID** (ì°¸ê³ ìš©ë§Œ):
> - Website: https://droid-dataset.github.io/
> - Download:
> ```bash
> # Full dataset (1.7TB)
> gsutil -m cp -r gs://gresearch/robotics/droid <target_dir>
>
> # Sample (2GB, 100 trajectories)
> gsutil -m cp -r gs://gresearch/robotics/droid_100 <target_dir>
> ```
> - Policy Learning: https://github.com/droid-dataset/droid_policy_learning
> - âš ï¸ ìš°ë¦¬ëŠ” ì‚¬ìš© ì•ˆ í•¨ (environment mismatch)
>
> **EgoDex** (Human video pretraining):
> - Repository: https://github.com/apple/ml-egodex
> - Paper: "EgoDex: A Dataset for Egocentric Hand Manipulation" (2024)
> - Size: 829 hours, 194 tasks, 2TB
>
> **Something-Something V2** (Optional):
> - HuggingFace: https://huggingface.co/datasets/webdataset/something-something-v2
> - 220K videos, 174 action categories

---

## âœ… Action Items (ì‘ì—… ë¦¬ìŠ¤íŠ¸)

**ìš”ì•½**: Phase 1 (Paper reading & Design, 6ì£¼) â†’ Phase 2 (Experiments, 16ì£¼) â†’ Phase 3 (Writing, 8ì£¼) â†’ Phase 4 (Conference)

> [!todo]- âœ… ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **ğŸ¯ Detailed Action Plan (RSS 2026)**

>
> #### Phase 1: Foundation (2025-12 ~ 2026-01, 6ì£¼)
>
> **Week 1-2: Literature & Design (12/16-12/29)**
> - [x] EgoDex ë…¼ë¬¸ ì½ê¸° ì™„ë£Œ
> - [x] TraceGen ë…¼ë¬¸ ì •ë¦¬ ì™„ë£Œ (í˜„ì¬ ì§„í–‰ ì¤‘)
> - [ ] X-Diffusion, TrajSkill ì½ê¸°
> - [ ] Method architecture ìµœì¢… í™•ì •
> - [ ] Experiment design document ì‘ì„±
>
> **Week 3-4: SIMPLER Setup (12/30-01/12)**
> - [ ] SimplerEnv-OpenVLA ì„¤ì¹˜ ë° í™˜ê²½ êµ¬ì¶•
> - [ ] OpenVLA checkpoint ë‹¤ìš´ë¡œë“œ ë° í‰ê°€
> - [ ] Baseline ì„±ëŠ¥ í™•ì¸ (OpenVLA: 36.4%)
> - [ ] 5090 PCì—ì„œ inference ì†ë„ í…ŒìŠ¤íŠ¸
>
> **Week 5-6: Initial Implementation (01/13-01/26)**
> - [ ] Your representation model êµ¬í˜„ (ViT + Cross-Attention)
> - [ ] Training pipeline êµ¬ì¶•
> - [ ] Franka + 2F-85 + D405 hardware setup ì ê²€
>
> ---
>
> #### Phase 2: Core Experiments (2026-02 ~ 2026-05, 16ì£¼)
>
> **Month 1: SIMPLER Experiments (02)**
> - [ ] BridgeData V2ë¡œ pretraining
> - [ ] SIMPLER evaluation ì‹¤í–‰
> - [ ] Baseline ë¹„êµ (OpenVLA, Octo, Diffusion)
> - [ ] Ablation studies (encoder types, fusion strategies)
> - [ ] Table 1 ì™„ì„± + learning curves
>
> **Month 2: Franka Baselines (03)**
> - [ ] Teleoperation system êµ¬ì¶•
> - [ ] Task 1-2 ì •ì˜ ë° 100 demos ìˆ˜ì§‘
> - [ ] Zero-shot baseline: OpenVLA â†’ Franka
> - [ ] From-scratch: Diffusion Policy í•™ìŠµ
> - [ ] Full Finetune: OpenVLA ì¬í•™ìŠµ
>
> **Month 3: Your Method on Franka (04)**
> - [ ] Bridge V2 representation frozen encoder
> - [ ] Franka decoder êµ¬í˜„ ë° í•™ìŠµ
> - [ ] Cross-embodiment transfer ê²€ì¦
> - [ ] Data efficiency curve (10, 20, 40, 80 demos)
> - [ ] Table 2 ì™„ì„± (4 methods ë¹„êµ)
>
> **Month 4: Extended Experiments (05)**
> - [ ] Task 3-4 ì¶”ê°€ ì‹¤í—˜
> - [ ] Robustness tests (lighting, backgrounds)
> - [ ] Long-horizon tasks (multi-step)
> - [ ] Failure mode analysis
> - [ ] Generalization tests (novel objects)
>
> ---
>
> #### Phase 3: Paper Writing (2026-06 ~ 2026-07, 8ì£¼)
>
> **Week 1-2: Draft v1 (06/01-06/14)**
> - [ ] Introduction ì‘ì„±
> - [ ] Related Work ì‘ì„±
> - [ ] Method ìƒì„¸ ì‘ì„±
> - [ ] Experiments ì‘ì„±
> - [ ] Results tables & figures ì™„ì„±
>
> **Week 3-4: Revision (06/15-06/28)**
> - [ ] Abstract ì‘ì„±
> - [ ] Limitations ì„¹ì…˜ ì‘ì„±
> - [ ] Discussion ì‘ì„±
> - [ ] Figure quality ê°œì„ 
> - [ ] ë™ë£Œ ë¦¬ë·° 1ì°¨
>
> **Week 5-6: Polish (06/29-07/12)**
> - [ ] ë™ë£Œ ë¦¬ë·° ë°˜ì˜
> - [ ] Language editing
> - [ ] Supplementary material ì‘ì„±
> - [ ] ìµœì¢… ì ê²€
>
> **Week 7-8: Conference Prep (07/13-07/26)**
> - [ ] Poster ë””ìì¸
> - [ ] Talk slides ì¤€ë¹„
> - [ ] Practice presentation
>
> ---
>
> ### ğŸŸ¡ Important (Nice-to-have)
>
> #### Additional Experiments
> - [ ] VC-1 linear probe evaluation
> - [ ] Human video pretraining ablation (EgoDex vs Sthv2)
> - [ ] Cross-task generalization í…ŒìŠ¤íŠ¸
> - [ ] Sim-to-real transfer analysis
>
> #### Code & Reproducibility
> - [ ] Code cleanup ë° ë¬¸ì„œí™”
> - [ ] README ì‘ì„±
> - [ ] Pre-trained checkpoint ì¤€ë¹„
> - [ ] Demo video ì´¬ì˜
>
> ---
>
> ### ğŸŸ¢ Optional (ì—¬ìœ  ìˆì„ ë•Œ)
>
> - [ ] Task 2 ì‹¤í—˜ ("Stack blocks")
> - [ ] LAPA-style baseline êµ¬í˜„
> - [ ] Third-person camera ì¶”ê°€ ì‹¤í—˜
> - [ ] Long-horizon task ì‹œë„
> - [ ] Multi-step reasoning ë¶„ì„

---

## ğŸ“… Updated Timeline (RSS 2026)

**ìš”ì•½**: 13ê°œì›” ì¶©ë¶„í•œ íƒ€ì„ë¼ì¸. 2025-12~2026-01 (Foundation) â†’ 2026-02~05 (Experiments) â†’ 2026-06~07 (Writing) â†’ 2026-07-13~17 (Conference)

> [!note]- ğŸ“† ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **Current Status**: Planning (2025-12-16)
>
> **RSS 2026 Deadlines** (ì˜ˆìƒ):
> - Abstract: 2026-01-15~20 ì˜ˆì • (ë¯¸ë°œí‘œ)
> - Full paper: 2026-01-22~27 ì˜ˆì • (ë¯¸ë°œí‘œ)
> - Conference: 2026-07-13~17 (Sydney, Australia)
>
> **ì¶©ë¶„í•œ Timeline** (13ê°œì›”):
>
> ****Phase 1: Foundation (2025-12 ~ 2026-01, 6ì£¼)****
> ```
> Week 1-2 (12/16-12/29): Paper reading & method design
> Week 3-4 (12/30-01/12): SIMPLER baseline evaluation
> Week 5-6 (01/13-01/26): Initial model implementation
> ```
>
> ****Phase 2: Core Experiments (2026-02 ~ 2026-05, 16ì£¼)****
> ```
> Month 1 (02): SIMPLER experiments
>   - Your method êµ¬í˜„ ë° í•™ìŠµ
>   - Baseline ë¹„êµ ì™„ë£Œ
>   - Ablation studies
>
> Month 2 (03): Franka data collection & baseline
>   - 100 demos per task ìˆ˜ì§‘
>   - Zero-shot, From-scratch baselines
>
> Month 3 (04): Franka your method
>   - Decoder-only training
>   - Cross-embodiment transfer ê²€ì¦
>   - Data efficiency analysis
>
> Month 4 (05): Additional experiments
>   - More tasks (3-4 tasks total)
>   - Robustness tests
>   - Failure case analysis
> ```
>
> ****Phase 3: Paper Writing (2026-06 ~ 2026-07, 8ì£¼)****
> ```
> Week 1-2 (06/01-06/14): Draft v1
> Week 3-4 (06/15-06/28): Revision + figures
> Week 5-6 (06/29-07/12): Internal review + polish
> Week 7-8 (07/13-07/26): Final preparation & practice talk
> ```
>
> ****Phase 4: Conference (2026-07-13~17)****
> ```
> Presentation & networking at RSS 2026
> ```
>
> **ì¥ì ** (RSS 2026 targeting):
> - âœ… ì¶©ë¶„í•œ ì‹¤í—˜ ì‹œê°„ (13ê°œì›”)
> - âœ… More tasks, more baselines ê°€ëŠ¥
> - âœ… Robust results + thorough analysis
> - âœ… High-quality paper writing
> - âœ… Multiple revision cycles
> - âœ… ì¶”ê°€ ì•„ì´ë””ì–´ ë°˜ì˜ ì—¬ìœ 
>
> **Recommended Milestones**:
> - 2026-02-28: SIMPLER ê²°ê³¼ ì™„ì„±
> - 2026-04-30: Franka ì‹¤í—˜ ì™„ë£Œ
> - 2026-06-30: Draft v1 ì™„ì„±
> - 2026-07-13: Conference presentation

---

## ì‹¤í—˜ ë…¸íŠ¸

**ìš”ì•½**: Change representation learning via video prediction. í•µì‹¬ = ë³€í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶• (ê²€ì¦: ë¯¸ë˜ ì˜ˆì¸¡ ê°€ëŠ¥). U-Net decoder baseline, Forward/Inverse ë¶„ë¦¬.

> [!note]- ğŸ““ ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **2025-12-18: Initial Discussion**
>
> **ë¬¸ì œ ì¸ì‹**:
> - ë¡œë´‡ ì†ë„ ì°¨ì´ â†’ ê°™ì€ í–‰ë™ì¸ë° ë‹¤ë¥¸ temporal pattern
>   - ë¹ ë¥¸ ë¡œë´‡: 10 frames (0.3ì´ˆ)
>   - ëŠë¦° ë¡œë´‡: 100 frames (3ì´ˆ)
>
> **í•´ê²° ë°©ì•ˆ: 2-Frame Fixed Input**
> ```
> ì…ë ¥: í•­ìƒ 2ì¥ ì´ë¯¸ì§€ ê³ ì • (t, t+k)
> ì¶œë ¥: Change embedding
> ëª©ì : ë³€í™”ì˜ ë³¸ì§ˆë§Œ ìºì¹˜
> ```
>
> **Image Preprocessing**: [[Two-Stream Image Preprocessing]]
> - Mì±„ë„ (4ch): [Î”L, Î”R, Î”G, Î”B] - ì‹œê°„ì  ë³€í™”
> - Pì±„ë„ (5ch): [âˆ‚x, âˆ‚y, R, G, B] - ê³µê°„ + ìƒ‰ìƒ
> - ì´ 9ì±„ë„ ì…ë ¥
>
> ---
>
> **2026-01-29: Video Prediction Pre-training (ìµœì¢… í™•ì •)**
>
> **í•µì‹¬ ì² í•™: Change Representation Learning**
>
> > **ëª©í‘œ**: ì´ë¯¸ì§€ ê°„ ë³€í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” representation í•™ìŠµ
> > **ê²€ì¦**: ê·¸ representationë§Œìœ¼ë¡œ ë‹¤ìŒ ìˆœê°„ì„ ì •í™•íˆ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ê°€?
>
> **ì™œ ì´ ì ‘ê·¼ë²•ì¸ê°€**:
>
> 1. **Self-validation**:
>    - MAE: "íŒ¨ì¹˜ ë³µì› ì˜ ë˜ë‚˜?" (ì •ì )
>    - DINO: "ë‹¤ë¥¸ viewì—ì„œë„ ê°™ì€ feature?" (ë¶ˆë³€ì„±)
>    - **ìš°ë¦¬**: "ë‹¤ìŒ ìˆœê°„ ì˜ˆì¸¡ ì •í™•í•œê°€?" (ë™ì  ì´í•´) âœ…
>
> 2. **Cause-agnostic**:
>    - ë¡œë´‡ íŒ”? ì¤‘ë ¥? ì‚¬ëŒ? â†’ ìƒê´€ì—†ìŒ
>    - ëª¨ë“  ì›ì¸ì˜ visual dynamicsë¥¼ í†µí•© í•™ìŠµ
>
> 3. **Forward/Inverse ë¶„ë¦¬**:
>    - Pre-training: Forward dynamics (unsupervised, 220k videos)
>    - Downstream: Inverse dynamics (supervised, 20-30 demos)
>
> **Architecture**
>
> ```python
> class TwoStreamVideoPredictor(nn.Module):
>     """
>     Pre-training: img_t + change_emb â†’ img_t+k
>     Downstream: change_emb â†’ robot action
>     """
>     def __init__(self, dim=768):
>         # Two-Stream Encoders
>         self.encoder_m = ViT_M(dim)
>         self.encoder_p = ViT_P(dim)
>         self.fusion = LinearClsFusion(dim)
>
>         # Image Encoder (í˜„ì¬ ìƒíƒœ)
>         self.img_encoder = ResNet50()
>
>         # U-Net Decoder (ì¬êµ¬ì„±)
>         self.decoder = UNetDecoder(dim, out_ch=3)
>
>     def forward(self, img_t, img_tk):
>         # 1. M-P preprocessing
>         m_ch = magnocellular_channel(img_t, img_tk)
>         p_ch = parvocellular_channel(img_tk)
>
>         # 2. Encode change
>         m_tok = self.encoder_m(m_ch)
>         p_tok = self.encoder_p(p_ch)
>         change_emb = self.fusion(m_tok[:, 0], p_tok[:, 0])
>
>         # 3. Encode current state
>         img_feat = self.img_encoder(img_t)
>
>         # 4. Reconstruct img_tk
>         img_pred = self.decoder(img_feat, change_emb)
>
>         return img_pred, change_emb
> ```
>
> **Training Protocol**
>
> ```python
> # Dataset: EgoDex + Sth-Sth V2 + Robot data
> # Variable k: 1~10 frames (multi-scale)
>
> for batch in dataloader:
>     video = batch['frames']
>     k = random.randint(1, 10)
>
>     img_t = video[:, 0]
>     img_tk = video[:, k]
>
>     img_pred, change_emb = model(img_t, img_tk)
>
>     # Loss
>     loss = F.mse_loss(img_pred, img_tk)
>     loss += 0.1 * perceptual_loss(img_pred, img_tk)
>
>     loss.backward()
>     optimizer.step()
> ```
>
> **Downstream (Inverse Dynamics)**
>
> ```python
> class InverseDynamicsModel(nn.Module):
>     def __init__(self):
>         # Load pretrained (frozen or fine-tunable)
>         self.encoder_m = load_pretrained(ViT_M)
>         self.encoder_p = load_pretrained(ViT_P)
>         self.fusion = load_pretrained(LinearClsFusion)
>
>         # Action head (random init)
>         self.action_head = nn.Linear(dim + task_dim, action_dim)
>
>     def forward(self, img_t, img_t1, task_emb):
>         change_emb = self.encoder(img_t, img_t1)
>         combined = torch.cat([change_emb, task_emb], dim=-1)
>         action = self.action_head(combined)
>         return action
> ```
>
> **Why This Works: M-P Splitì˜ ì™„ë²½í•œ ì¡°í™”**
>
> ```python
> # ë¯¸ë˜ ì˜ˆì¸¡ ì˜ˆì‹œ: ê³µì´ êµ´ëŸ¬ê°„ë‹¤
>
> # Pì±„ë„ë§Œ (ì‹¤íŒ¨)
> P(img_t) = [âˆ‚x, âˆ‚y, R, G, B]  # í˜„ì¬ ìœ„ì¹˜
> â†’ "ì–´ë””ë¡œ ê°ˆì§€?" ì•Œ ìˆ˜ ì—†ìŒ
>
> # Mì±„ë„ ì¶”ê°€ (ì„±ê³µ)
> M(tâ†’t+k) = [Î”L, Î”R, Î”G, Î”B]  # Motion
> P(img_t) = [âˆ‚x, âˆ‚y, R, G, B]  # Appearance
> â†’ "ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™" ì˜ˆì¸¡ ê°€ëŠ¥!
> ```
>
> **M-P Balance ìë™ ë‹¬ì„±**:
> - Static: M=[0,0,0,0], Pê°€ ëª¨ë“  ì¼ â†’ P í•™ìŠµ
> - Video: M+P ë‘˜ ë‹¤ í•„ìš” â†’ ê· í˜• í•™ìŠµ
>
> **Key Design Decisions**
>
> 1. **Decoder**: U-Net (baseline, simple & stable)
> 2. **ì›ë³¸ ì´ë¯¸ì§€ ì…ë ¥**: img_të¥¼ ResNet ì¸ì½”ë”© â†’ spatial detail ë³´ì¡´
> 3. **Variable k**: 1~10 frames, multi-scale temporal learning
>
> **Next Steps**
>
> êµ¬í˜„ ìš°ì„ ìˆœìœ„:
> - [x] í•µì‹¬ ì•„ì´ë””ì–´ í™•ì •
> - [x] Architecture ì„¤ê³„
> - [ ] U-Net decoder êµ¬í˜„
> - [ ] EgoDex ë°ì´í„° ë¡œë”©
> - [ ] Baseline training
> - [ ] Ablation: M vs P vs M+P
> - [ ] Inverse dynamics downstream
>
> **ê´€ë ¨ ë©”ëª¨**:
> - [[Pixel-wise Channel Fusion for Behavior Representation#5. Change Representation via Video Prediction]]
> - [[Two-Stream Image Preprocessing#ì£¼ìš” ì‘ìš©: Change Representation Learning]]

---

## Discussion ì„¹ì…˜ ì•„ì´ë””ì–´ (2026-01-06)

**ë…¼ë¬¸ Discussionì— í¬í•¨í•  í•µì‹¬ í†µì°°**

### Action as Interface: A Deeper Understanding

**Main Argument:**

> Task ì„±ê³µì˜ ë³¸ì§ˆì€ ì˜¬ë°”ë¥¸ visual flowë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.
> Actionì€ ê·¸ flowë¥¼ ë§Œë“œëŠ” ì¸í„°í˜ì´ìŠ¤ì¼ ë¿ì´ë‹¤.

**Two Types of Knowledge:**

Our approach separates robot learning into two distinct types of knowledge:

1. **Forward Knowledge: "What visual changes lead to success?"**
   - Task-dependent visual dynamics
   - Embodiment-independent (same flow across different robots)
   - Requires extensive experience to learn (220k videos in LAPA)
   - **This is the hard part**

2. **Inverse Knowledge: "What actions produce those changes?"**
   - Action-to-outcome mapping
   - Embodiment-specific (different action spaces)
   - Can be learned through inverse reasoning
   - **This is the easy part** (20-30 demos sufficient)

**Why Human Video Pretraining Works:**

ê¸°ì¡´ ì„¤ëª… (í‘œë©´ì ):
- "Visual change represents action"
- "Embodiment-independent learning"

ë” ê¹Šì€ ì´ìœ :
- Human videos provide **forward knowledge** (the hard part)
- Robot demos provide **inverse knowledge** (the easy part)
- **Separating these two makes learning efficient**

**Our Contribution through this Lens:**

| Method | Input | Forward | Inverse | Problem | Efficiency |
|--------|-------|---------|---------|---------|------------|
| LAPA | Image_t (1 frame) | Task-agnostic | Latent mapping | **Ill-posed** (1 frame â†’ change?) | 220k videos |
| OpenVLA | Image sequence | Coupled | End-to-end | No separation | 970k trajectories |
| **Ours** | **Image_t, t+1** | **Task-conditioned** | **Decoder-only** | **Well-posed** âœ… | **10-20 demos** âœ… |

**LAPA's Fundamental Limitation (2026-01-07):**

LAPAì˜ êµ¬ì¡°ì  ëª¨ìˆœ:
```
Input: Image_t (single frame) + Task
Output: Latent action (represents change)

Problem: Change is inherently a temporal relationship (t â†’ t+1)
         Cannot infer "change" from single state alone!
         â†’ Ill-posed problem â†’ Requires 220k videos to compensate
```

**Our Approach Solves This:**
```
Input: Image_t, Image_t+1 (actual change) + Task
Output: Behavior representation

Advantage:
  - Observes actual visual change (well-posed)
  - Task-conditioning filters important features
  - â†’ Requires far fewer videos (10-20 demos for inverse)
```

**Task-Conditioning Enhances Forward Learning:**
- Task specifies relevant visual features
- "red cup" â†’ Attend to color features
- "pick up" â†’ Attend to vertical motion
- â†’ **Less data needed to learn forward model**
- â†’ **LAPA learns all changes equally, we focus on task-relevant changes**

**Connection to Established Work:**

This separation is already validated in:
- **Visual MPC**: Learn dynamics (forward) â†’ Plan actions (inverse)
- **DreamerV3**: World model first â†’ Policy later
- **Visual Foresight**: Video prediction â†’ Action planning

Our novelty: **Task-conditioned forward learning** makes this separation data-efficient.

### Implications for Digital Twin

**Traditional View (Problematic):**
```
Sim â†’ Real transfer = Hard (large sim-to-real gap)
```

**Forward/Inverse View (Practical):**
```
Forward (visual flow): Similar in sim and real
Inverse (action mapping): Different in sim and real

â†’ Learn forward in sim (safe, fast)
â†’ Learn inverse in real (only adaptation needed)
â†’ Gap is halved!
```

**Digital Twin as Forward Learning Platform:**
- Safe experimentation with diverse configurations
- Learn which visual features matter for task success
- Rapid iteration without physical constraints
- Real robot only needed for inverse mapping (20-30 demos)

### Limitations and Future Work

**Current Limitations:**

1. **Information Filtering Scope**
   - Currently filter via task-conditioning
   - Still process entire image (background, lighting, etc.)
   - Future: Spatial attention mask (ignore irrelevant regions entirely)

2. **Contact-Rich Manipulation**
   - Visual flow may not capture force/tactile feedback
   - Fine-grained control (Â±0.5mm precision) challenging
   - Solution: Combine with proprioceptive/force sensors

3. **Exploration Efficiency**
   - "Try action â†’ Check result" can be slow in real world
   - Solution: Use learned forward model for planning (Visual MPC)

4. **Temporal Credit Assignment**
   - Which actions in a sequence contributed to success?
   - Solution: Attention over temporal dimension

**Future Directions:**

1. **Visual MPC Integration**
   - Use learned forward model for action planning
   - Closed-loop control: Execute â†’ Observe â†’ Re-plan
   - True realization of "action as interface"

2. **Multi-Modal Forward Models**
   - Integrate tactile, force, proprioceptive feedback
   - Richer understanding of task dynamics

3. **Zero-Shot Transfer**
   - If forward model is perfect, can we skip inverse learning?
   - Action sampling + forward prediction â†’ Find action that produces desired flow

### Key Takeaway

> ìš°ë¦¬ëŠ” "actionì„ ì˜ ë§ì¶”ëŠ”" ì—°êµ¬ê°€ ì•„ë‹ˆë¼,
> "task flowë¥¼ ì´í•´í•˜ëŠ”" ì—°êµ¬ë¥¼ í•œë‹¤.
> Actionì€ ê·¸ì € ê·¸ flowë¥¼ ì‹¤í˜„í•˜ëŠ” ìˆ˜ë‹¨ì¼ ë¿ì´ë‹¤.

This philosophical shift explains:
- Why human videos are effective (forward knowledge)
- Why we need few robot demos (inverse is easy)
- Why task-conditioning matters (efficient forward learning)
- Why decoder-only finetuning works (inverse is separate)

---

## ì½ì„ ë…¼ë¬¸

**ìš”ì•½**: ìµœìš°ì„ (LAPA âœ…, OpenVLA âœ…, Bridge V2 âœ…) | ìµœì‹  cross-embodiment(TraceGen âœ…, X-Diffusion, TrajSkill) | ì„ íƒì (VC-1, Diffusion Policy)

> [!info]- ğŸ“š ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **ìµœìš°ì„  (ì§ì ‘ ê´€ë ¨ - ë°˜ë“œì‹œ ì½ì–´ì•¼ í•¨)**:
> - [x] **LAPA (2024)** â­â­â­ í•µì‹¬!
>   - ICLR 2025, Latent Action Pretraining from Videos
>   - **ì™œ ì¤‘ìš”**: ì‚¬ëŒ ë¹„ë””ì˜¤ > ë¡œë´‡ ë°ì´í„° ì¦ëª… (36.8% vs 30.8%)
>   - **ìš°ë¦¬ì™€ì˜ ê´€ê³„**: ê°™ì€ ë¬¸ì œ(human video), ë‹¤ë¥¸ ì ‘ê·¼(latent action vs task-conditioned)
>   - **ì°¨ë³„ì **: VQ-VAE unsupervised vs ìš°ë¦¬ëŠ” task-aware supervised
>   - **í™œìš©**: Human video íš¨ê³¼ ê²€ì¦, ìš°ë¦¬ ë°©ë²•ì˜ ìš°ìˆ˜ì„± ê°•ì¡°
>
> - [x] **OpenVLA (2024)** â­â­â­
>   - arXiv 2406.09246, í˜„ì¬ VLA SOTA
>   - **ì™œ ì¤‘ìš”**: Primary baseline, ì´ê²ƒì„ ì´ê²¨ì•¼ í•¨
>   - **ë¹„êµ í¬ì¸íŠ¸**: Robot action-labeled vs Human video pretraining
>   - **êµ¬í˜„**: HuggingFace checkpoint í™œìš© (ì‰¬ì›€)
>
> - [x] [[BridgeData V2 (2023)]] â­â­
>   - CoRL 2023, Homer Walke, Chelsea Finn, Sergey Levine
>   - **ê·œëª¨**: 60K trajectories, 24 environments, WidowX robot
>   - **ì™œ ì¤‘ìš”**: ë¡œë´‡ í•™ìŠµ ë¶„ì•¼ ì‚¬ì‹¤ìƒ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ (322 ì¸ìš©)
>   - **í™œìš©**: Finetuning ë°ì´í„°, evaluation benchmark
>   - **ì €ì**: UC Berkeley/Stanford RAIL lab - ìµœê³  ê¶Œìœ„
>   - **ì„íŒ©íŠ¸**: Open X-Embodimentì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ
>   - **ì¬í˜„ì„±**: ì˜¤í”ˆì†ŒìŠ¤, ì €ê°€ ë¡œë´‡($3-5K), ëˆ„êµ¬ë‚˜ ì¬í˜„ ê°€ëŠ¥
>
> **í•„ìˆ˜ (Baseline & Dataset)**:
> - [x] **VC-1 (2023)**: Visual representation baseline (ì„ íƒì  ë¹„êµ)
> - [x] **[[Sources/papers/RT-X (2023)]]**: Open X-Embodiment ë°ì´í„°ì…‹
> - [x] **EgoDex (2024)**: Apple, ìµœê³  í’ˆì§ˆ ì‚¬ëŒ manipulation ë°ì´í„° (829ì‹œê°„)
> - [ ] ~~R3M (2022)~~: VC-1ë¡œ ì¶©ë¶„ (íŒ¨ìŠ¤)
> - [ ] ~~RT-1 (2022)~~: OpenVLAë¡œ ì¶©ë¶„ (íŒ¨ìŠ¤)
>
> **ìµœì‹  Cross-Embodiment Learning (2025)**:
> - [x] **TraceGen (Nov 2025)** â­â­â­ arXiv 2511.21690
>   - **í•µì‹¬**: 3D trace-space for cross-embodiment learning from human videos
>   - **ë°ì´í„°**: 123K videos, 1.8M triplets (TraceForge pipeline)
>   - **ì„±ëŠ¥**: 5 human videos â†’ 67.5% success, 50-600x faster inference
>   - **ì°¨ë³„ì **: 3D geometric trace vs ìš°ë¦¬ëŠ” 2D visual behavior (simpler, efficient)
>   - **í™œìš©**: Related Work í•µì‹¬ ë¹„êµ ëŒ€ìƒ, efficiency ê·¼ê±°
>
> - [ ] **UMI-on-Air (Oct 2025)** â­â­â­â­ arXiv 2510.02614 **í•„ë…!**
>   - **í•µì‹¬**: Handheld gripper (UMI) human demos â†’ embodiment-agnostic policy â†’ aerial manipulator deployment
>   - **ë°©ë²•**: Embodiment-Aware Diffusion Policy (EADP) - gradient feedback from controller
>   - **ì„±ëŠ¥**: Long-horizon, high-precision aerial manipulation tasks ì„±ê³µ
>   - **ì°¨ë³„ì **: ìš°ë¦¬ì™€ ê±°ì˜ ë™ì¼í•œ ë¬¸ì œ! (human demos â†’ robot transfer)
>   - **í™œìš©**: ì§ì ‘ ë¹„êµ í•„ìˆ˜, ê°€ì¥ ìœ ì‚¬í•œ ì ‘ê·¼ë²•
>   - **ì£¼ì˜**: RSS 2026 ì œì¶œ ì‹œ í•µì‹¬ related work
>
> - [ ] **ViDEN (Dec 2024)** â­â­â­ arXiv 2412.20226
>   - **í•µì‹¬**: Visual demonstrations â†’ embodiment-agnostic navigation policy
>   - **ë°©ë²•**: Diffusion-based policy, depth images, relative target positions
>   - **ì„±ëŠ¥**: Small dataset (500 points), human reaching & tracking tasks
>   - **ì°¨ë³„ì **: Navigation vs ìš°ë¦¬ëŠ” manipulation (ìƒí˜¸ë³´ì™„ì )
>   - **í™œìš©**: Visual demonstration + embodiment-agnostic ì ‘ê·¼ ë¹„êµ
>
> - [ ] **Latent Policy Steering (Jul 2025)** â­â­ arXiv 2507.13340
>   - **í•µì‹¬**: Multi-embodiment World Model pretraining + latent space search
>   - **ë°ì´í„°**: Open X-embodiment (2K episodes) + human play data
>   - **ì„±ëŠ¥**: 30 demosì—ì„œ 50% ì„±ëŠ¥ í–¥ìƒ, 50 demosì—ì„œ 20% í–¥ìƒ
>   - **ì°¨ë³„ì **: World model + search vs ìš°ë¦¬ëŠ” direct behavior representation
>   - **í™œìš©**: Multi-embodiment pretraining ì „ëµ ë¹„êµ
>
> - [ ] **E2VLA (Sep 2025)** â­ arXiv 2509.14630
>   - **í•µì‹¬**: Embodiment equivariant VLA (configuration transformation equivariance)
>   - **ë°©ë²•**: Geometry-aware network + equivariant action decoder
>   - **ì°¨ë³„ì **: Equivariance theory vs ìš°ë¦¬ëŠ” task-conditioned representation
>   - **í™œìš©**: Theoretical foundation ì°¸ê³ 
>
> - [ ] **TrajSkill (Oct 2025)** â­â­ arXiv 2510.07773
>   - **í•µì‹¬**: Sparse optical flow as embodiment-agnostic motion cues
>   - **ì„±ëŠ¥**: 16.7% cross-embodiment improvement, real kitchen tasks
>   - **ì°¨ë³„ì **: Low-level optical flow vs ìš°ë¦¬ëŠ” high-level task-conditioned behavior
>   - **í™œìš©**: Motion representation ë¹„êµ
>
> - [ ] **Masquerade (Aug 2025)** â­â­ arXiv 2508.09976
>   - **í•µì‹¬**: Video editing (inpainting + robot overlay) for human â†’ robot demos
>   - **ë°ì´í„°**: 675K frames, 50 robot demos per task
>   - **ì„±ëŠ¥**: 5-6x better on bimanual kitchen tasks
>   - **ì°¨ë³„ì **: Video editing vs ìš°ë¦¬ëŠ” direct representation learning
>   - **í™œìš©**: Data augmentation ì „ëµ ì°¸ê³ 
>
> - [ ] **Gen2Act (Sep 2024)** â­ arXiv 2409.16283
>   - **í•µì‹¬**: Human video generation â†’ robot execution (zero-shot)
>   - **ì°¨ë³„ì **: Generation-based vs representation-based
>   - **í™œìš©**: Zero-shot capability ë¹„êµ
>
> - [ ] **ET-VLA (Nov 2025)** arXiv 2511.01224
>   - Embodiment transfer for multi-robot, 53.2% better than OpenVLA
>   - **í™œìš©**: Baseline ì°¸ê³ , synthetic continued pretraining
>
> - [ ] **CHORD (Jan 2026)** arXiv 2601.04194
>   - **í•µì‹¬**: Video generative models â†’ Lagrangian motion extraction â†’ robotics manipulation policies
>   - **ë°©ë²•**: Distillation-based pipeline from 2D videos (universal, category-agnostic)
>   - **ì €ì**: Jiajun Wu lab (Stanford)
>   - **ì°¨ë³„ì **: Generative model distillation vs ìš°ë¦¬ëŠ” contrastive representation
>   - **í™œìš©**: Video-to-policy ì ‘ê·¼ë²• ë¹„êµ (ê°„ì ‘ì  ê´€ë ¨)
>
> **Vaultì— ìˆëŠ” ê´€ë ¨ ë…¼ë¬¸ (ë‹¤ì‹œ ì½ê¸°)**:
> - [x] **[[Sources/papers/CURL (2020)]]**: Contrastive learning, sample efficiency
> - [x] **[[Sources/papers/DINO (2021)]]**: Self-supervised ViT, temporal consistency
> - [ ] **[[Sources/papers/Diffusion Policy (2023)]]**: Action decoding baseline
> - [ ] **[[Sources/papers/RT-2 (2023)]]**: VLM for robot control
> - [ ] **[[Sources/papers/ALOHA (2023)]]**: Action chunking (ACT), temporal consistency
> - [ ] **[[Sources/papers/Visual Pre-training Survey (2023)]]**: Visual pretraining survey
>
> **ì´ë¡ ì  ê¸°ë°˜ (ì°¸ê³ )**:
> - [ ] **Embodiment Scaling Laws (May 2025)** arXiv 2505.05753
>   - **í•µì‹¬**: Training embodiment ìˆ˜ ì¦ê°€ â†’ unseen embodiment ì¼ë°˜í™” í–¥ìƒ
>   - **ì‹¤í—˜**: ~1,000 procedurally generated embodiments (robot locomotion)
>   - **ê²°ê³¼**: Embodiment scaling > Data scaling (fixed embodiment)
>   - **ì°¨ë³„ì **: Locomotion vs ìš°ë¦¬ëŠ” manipulation (ë‹¤ë¥¸ domain)
>   - **í™œìš©**: Cross-embodiment generalization ì´ë¡ ì  ê·¼ê±°
>
> **ì°¸ê³  (ë‚®ì€ ìš°ì„ ìˆœìœ„)**:
> - [ ] **Robot Trains Robot (Aug 2025)** arXiv 2508.12252
>   - **í•µì‹¬**: Robotic arm teacher â†’ humanoid robot student (real-world RL)
>   - **ë°©ë²•**: Protection, reward, perturbation, automatic reset by teacher robot
>   - **ì°¨ë³„ì **: Hardware setup vs ìš°ë¦¬ëŠ” data/representation (orthogonal)
>   - **í™œìš©**: Real-world learning setup ì°¸ê³  (ê°„ì ‘ì )
>
> - [x] **[[Sources/papers/TCN (2018)]]**: Time-contrastive networks
> - [[Sources/papers/OpenVLA (2024)]]: VLA ë¹„êµ ëŒ€ìƒ
> - [[Sources/papers/GNM (2022)]]: Cross-embodiment navigation
> - [[Sources/papers/CLIP (2021)]]: Vision-language foundation

---

## ê´€ë ¨ ë…¸íŠ¸

- [[Questions/Q - Action-Agnostic Robot Learning]]
- [[Sources/papers/CURL (2020)]]: Sample efficiencyì™€ representation learning
- [[Sources/papers/DINO (2021)]]: Temporal consistencyì™€ collapse ë°©ì§€

**ì£¼ìš” ë¹„êµ ëŒ€ìƒ**:
- [[Sources/papers/Octo (2024)|critiques]] - Primary baseline. OctoëŠ” modular architectureë¡œ flexibility ë‹¬ì„±í•˜ì§€ë§Œ action-space couplingê³¼ ë°ì´í„° í’ˆì§ˆì— ì˜ì¡´ì . Attentionì´ "ì•Œì•„ì„œ" ì²˜ë¦¬í•  ê²ƒì„ ê¸°ëŒ€í•˜ë‚˜ ë°ì´í„° ë¶ˆê· í˜•(wrist 27%, language 56%) ì‹œ ì„±ëŠ¥ ì €í•˜. ìš°ë¦¬ëŠ” action-agnostic representationìœ¼ë¡œ ê·¼ë³¸ í•´ê²°. Robot-to-robot (800k eps) vs Human-to-robot (ì‚¬ëŒ ë¹„ë””ì˜¤).

**êµ¬ë¶„ í•„ìš”**:
- [[Outputs/Idea - DynamicNet]]: ë™ê¸°ëŠ” ìœ ì‚¬(í•µì‹¬ ì •ë³´ ì„ ë³„)í•˜ë‚˜ ì ‘ê·¼ì´ ë‹¤ë¦„
  - DynamicNet: ì•„í‚¤í…ì²˜ í˜ì‹  (attention ëŒ€ì²´, ë…¸ë“œ ê¸°ë°˜)
  - ì´ ë…¼ë¬¸: í•™ìŠµ ë°©ë²•ë¡  (cross-attention í™œìš©, foundation model)

---

## íƒœê·¸

#paper #project #robot-learning #action-agnostic
