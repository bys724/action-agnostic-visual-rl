# ë…¼ë¬¸ - Action-Agnostic Visual Behavior Representation

## ë©”íƒ€ë°ì´í„°

- **ìƒíƒœ**: Planning
- **ëª©í‘œ í•™íšŒ**: RSS (primary) / ICRA / CoRL (backup)
- **ê´€ë ¨ íŠ¹í—ˆ**: [[íŠ¹í—ˆ - ì‹œê³„ì—´ ì‹œê° ê´€ì°° ê¸°ë°˜ ì¡°ê±´ë¶€ í…ìŠ¤íŠ¸ ìƒì„± ì‹œìŠ¤í…œ]]

### ëª©í‘œ í•™íšŒ ì •ë³´ (RSS)

- **í•™íšŒ ì¥ì†Œ**: Sydney, Australia
- **í˜ì´ì§€ ì œí•œ**: ì—†ìŒ (ë‹¨, Limitations ì„¹ì…˜ í•„ìˆ˜)
- **í‰ê°€ ê¸°ì¤€**: Novelty, Technical quality, Significance, Potential impact, Clarity

---

## í•œ ë¬¸ì¥ ìš”ì•½

Human manipulation videosë¡œ í•™ìŠµí•œ Two-Stream change-aware vision encoderë¥¼ SOTA VLA (OpenVLA, Pi0)ì— ì ìš©í•˜ì—¬ 9-10% ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•˜ê³ , static image encoder ëŒ€ë¹„ temporal dynamics modelingì˜ ìš°ìˆ˜ì„±ì„ ì…ì¦í•˜ë©°, unified multi-embodiment architectureë¡œ íš¨ìœ¨ì ì¸ deploymentë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

---

## í•µì‹¬ ì•„ì´ë””ì–´

### ë¬¸ì œ

ë¡œë´‡ë§ˆë‹¤ ë‹¤ë¥¸ action space â†’ ë°ì´í„° ì¬ì‚¬ìš© ë¶ˆê°€

### í•´ê²°ì±…

í–‰ë™ì˜ ë³¸ì§ˆì€ ì‹œê°ì  ë³€í™” â†’ Visual outcomeìœ¼ë¡œ behavior í‘œí˜„

### í•µì‹¬ í†µì°° (LAPAì˜ ì„±ê³µê³¼ í•œê³„)

**ì‚¬ëŒ ë¹„ë””ì˜¤ì˜ ê°€ëŠ¥ì„± (LAPA ê²€ì¦)**
- LAPA (ICLR 2025): ì‚¬ëŒ ë¹„ë””ì˜¤ë¡œ í•™ìŠµ ì‹œ ë¡œë´‡ ë°ì´í„°ë³´ë‹¤ ìš°ìˆ˜
- Visual changeì— action ì •ë³´ê°€ ë‹´ê¹€ (embodiment ë¬´ê´€)
- **BUT**: Off-the-shelf vision encoder ì‚¬ìš© (CLIP, DINOv2)
- **ë¬¸ì œ**: Static image encoderëŠ” temporal dynamicsì— ìµœì í™”ë˜ì§€ ì•ŠìŒ

**ìš°ë¦¬ì˜ ì§ˆë¬¸**
> "Can we design better vision encoders specifically for robot learning?"

**ìš°ë¦¬ì˜ ì ‘ê·¼: Custom Change-Aware Vision Encoder**
- **Vision Encoder ì„¤ê³„**: Human videoë¡œ í•™ìŠµí•œ specialized encoder
- **Two-Stream Architecture**: M-Stream (temporal change) + P-Stream (spatial structure)
- **CLS Exchange**: ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ì„ íƒì  ì •ë³´ êµí™˜
- **Task-Conditioning**: Relevant featureì— ì§‘ì¤‘
- **Multi-Embodiment Unified**: Single model with embodiment-specific experts

**í•µì‹¬ ì°¨ë³„ì **:
> LAPA: "ì–´ë–¤ ê¸°ì¡´ encoderë¥¼ ì“¸ê¹Œ?" (Method-level)
> Ours: "VLAë¥¼ ìœ„í•œ ìµœì  encoderë¥¼ ì–´ë–»ê²Œ ì„¤ê³„í• ê¹Œ?" (Component-level)

### ì™œ ê¸°ì¡´ Vision Encoderê°€ VLAì— ë¶€ì í•©í•œê°€ (2026-02-03 ì¶”ê°€)

> **í•µì‹¬ í†µì°°: Invariance vs Equivariance ë¬¸ì œ**

**ê¸°ì¡´ Encoderë“¤ì˜ í•™ìŠµ ëª©í‘œ**:

| Encoder | í•™ìŠµ ë°©ì‹ | ëª©í‘œ | ê²°ê³¼ |
|---------|----------|------|------|
| **CLIP** | Image-Text Contrastive | "ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ê°™ì€ ì˜ë¯¸" | Semantic Invariance |
| **DINO/DINOv2** | Self-distillation + Multi-crop | "crop/resizeí•´ë„ ê°™ì€ feature" | Position Invariance |
| **MAE** | Masked Patch Reconstruction | "ê°€ë ¤ì§„ patch ë³µì›" | Static Structure |

**DINOì˜ ê·¼ë³¸ì  ë¬¸ì œ**:

DINOëŠ” **augmentation invariance**ë¡œ í•™ìŠµë¨:
```
Image + augmentation(crop, resize, shift) â†’ ê°™ì€ representation
= Position INVARIANT (ìœ„ì¹˜ ë³€í™” ë¬´ì‹œ)
```

í•˜ì§€ë§Œ ë¡œë´‡ actionì´ ìš”êµ¬í•˜ëŠ” ê²ƒ:
```
ë¬¼ì²´ê°€ 5cm ì´ë™ â†’ representationë„ ê·¸ ë³€í™”ë¥¼ ë°˜ì˜
= Position EQUIVARIANT (ìœ„ì¹˜ ë³€í™” ë°˜ì˜)
```

**Talk2DINO (2024)ì—ì„œ í™•ì¸ëœ DINOì˜ í•œê³„**:

Talk2DINOëŠ” DINOê°€ "fine-grained spatial features"ì— ê°•í•˜ë‹¤ê³  í™œìš©í–ˆì§€ë§Œ, ì´ëŠ” **static spatial** (ì–´ë–¤ ì˜ì—­ì´ coherentí•œê°€)ì´ì§€ **dynamic spatial** (ê·¸ ì˜ì—­ì´ ì–´ë–»ê²Œ ì›€ì§ì´ëŠ”ê°€)ì´ ì•„ë‹˜.

```
DINOê°€ ì˜í•˜ëŠ” ê²ƒ:
âœ… "ì—¬ê¸°ì— ë¬¼ì²´ê°€ ìˆë‹¤" (localization)
âœ… "ì´ ì˜ì—­ì´ í•˜ë‚˜ì˜ ë¬¼ì²´ë‹¤" (coherent region)

DINOê°€ ëª»í•˜ëŠ” ê²ƒ:
âŒ "ì´ ë¬¼ì²´ê°€ 5í”½ì…€ ì™¼ìª½ìœ¼ë¡œ ê°”ë‹¤" (motion)
âŒ "gripperê°€ ë¬¼ì²´ì— ì ‘ê·¼í•˜ê³  ìˆë‹¤" (dynamics)
```

**ì™œ ì´ê²ƒì´ ë¡œë´‡ actionì— ì¹˜ëª…ì ì¸ê°€**:

```python
# ë¡œë´‡ manipulationì˜ í•µì‹¬
action = f(current_state, desired_change)

# DINO representation
dino_emb_t0 = dino(image_t0)  # [CLS] = ì „ì—­ semantic
dino_emb_t1 = dino(image_t1)  # [CLS] = ì „ì—­ semantic (ê±°ì˜ ë™ì¼!)

# ë¬¸ì œ: ë¯¸ì„¸í•œ ìœ„ì¹˜ ë³€í™”ê°€ representationì— ë°˜ì˜ ì•ˆ ë¨
# â†’ action predictionì— í•„ìš”í•œ ì •ë³´ ì†ì‹¤
```

**Positional Encodingì˜ í•œê³„**:

"Positional encodingì´ ìœ„ì¹˜ ì •ë³´ë¥¼ ì£¼ì§€ ì•Šë‚˜?"
- ì´ë¡ ì ìœ¼ë¡œëŠ” ë§ìŒ
- í•˜ì§€ë§Œ í•™ìŠµ ê³¼ì •ì—ì„œ **ë¬´ì‹œë˜ë„ë¡ gradientê°€ íë¦„** (augmentation invariance ë•Œë¬¸)
- ìµœì¢… representationì€ ìœ„ì¹˜ë³´ë‹¤ **semanticì— ì§‘ì¤‘**

**ìš°ë¦¬ì˜ í•´ê²°ì±…: Change-Aware Encoder**

| ì¸¡ë©´ | DINO/CLIP | Ours |
|------|-----------|------|
| **í•™ìŠµ ëª©í‘œ** | Semantic invariance | **Change sensitivity** |
| **ìœ„ì¹˜ ì •ë³´** | Invariant (ë¬´ì‹œ) | **Equivariant (ë°˜ì˜)** |
| **ì…ë ¥** | Single image | **Image pair (t, t+k)** |
| **ì¶œë ¥** | Static state | **Dynamic change** |
| **ì í•©í•œ task** | Classification, Segmentation | **Action prediction** |

**í•µì‹¬ ì£¼ì¥**:
> "ê¸°ì¡´ vision encoder(CLIP, DINO)ëŠ” **semantic invariance**ë¥¼ ëª©í‘œë¡œ í•™ìŠµë˜ì–´, **position-sensitive dynamics**ë¥¼ í‘œí˜„í•˜ëŠ” ë° ë³¸ì§ˆì  í•œê³„ê°€ ìˆë‹¤. ë¡œë´‡ actionì€ ì •í™•í•œ ìœ„ì¹˜ ë³€í™”ë¥¼ ìš”êµ¬í•˜ë¯€ë¡œ, **change-aware encoder**ê°€ í•„ìš”í•˜ë‹¤."

**ê´€ë ¨ ì¦ê±°**:
- DINO-Tracker: DINO featureë¡œ tracking â†’ **ì¶”ê°€ computation** í•„ìš” (patch-level matching)
- Talk2DINO: DINO attentionìœ¼ë¡œ localization â†’ **static** segmentationë§Œ ê°€ëŠ¥
- LAPA: DINOë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© â†’ off-the-shelf encoderì˜ í•œê³„ ì¸ì •

---

### ë” ê¹Šì€ ì² í•™: Forward/Inverse ë¶„ë¦¬ (2026-01-06 ì¶”ê°€)

> **"Actionì€ ì¸í„°í˜ì´ìŠ¤, ë³¸ì§ˆì€ Visual Flow"**

**í•µì‹¬ í†µì°°:**

Task ì„±ê³µ = ì˜¬ë°”ë¥¸ visual flow ìƒì„±

ë‘ ê°€ì§€ ì§€ì‹:
1. **Forward Knowledge (ì–´ë ¤ìš´ ë¶€ë¶„)**
   - "ì–´ë–¤ visual ë³€í™”ê°€ ì„±ê³µìœ¼ë¡œ ì´ì–´ì§€ëŠ”ê°€?"
   - Embodiment-independent
   - ì‚¬ëŒ ë¹„ë””ì˜¤ë¡œ í•™ìŠµ ê°€ëŠ¥
   - ë§ì€ ê²½í—˜ í•„ìš” â†’ 220k ë¹„ë””ì˜¤

2. **Inverse Knowledge (ì‰¬ìš´ ë¶€ë¶„)**
   - "ê·¸ ë³€í™”ë¥¼ ë§Œë“¤ë ¤ë©´ ì–´ë–¤ actionì„ ë‚´ì•¼ í•˜ë‚˜?"
   - Embodiment-specific
   - Forwardë¥¼ ì•Œë©´ ì—­ì¶”ë¡  ê°€ëŠ¥
   - ì ì€ ë°ëª¨ë¡œ ì¶©ë¶„ â†’ 20-30 demos

**ì™œ ì‚¬ëŒ ë¹„ë””ì˜¤ê°€ íš¨ê³¼ì ì¸ê°€:**
- ê¸°ì¡´ ì„¤ëª…: "Visual changeê°€ actionì„ í‘œí˜„"
- ë” ê¹Šì€ ì´ìœ : **Forwardë¥¼ ë¨¼ì € í•™ìŠµí•˜ë©´, InverseëŠ” ì‰½ë‹¤**
- ì‚¬ëŒ ë¹„ë””ì˜¤ = Forward knowledge ì œê³µ (ì–´ë ¤ìš´ ë¶€ë¶„)
- ë¡œë´‡ ë°ëª¨ = Inverse knowledge ì œê³µ (ì‰¬ìš´ ë¶€ë¶„)
- â†’ LAPAê°€ 220k ë¹„ë””ì˜¤ë¡œ ì„±ê³µí•œ ì´ìœ 

**Task-conditioningì˜ ì—­í• :**
- Forward í•™ìŠµì„ íš¨ìœ¨í™”
- Taskê°€ ì¤‘ìš”í•œ visual featureë¥¼ ëª…ì‹œ
- "ë¹¨ê°„ ì»µ" â†’ ìƒ‰ìƒ feature ê°•ì¡°
- "ë“¤ì–´ì˜¬ë¦¬ê¸°" â†’ ìˆ˜ì§ motion ê°•ì¡°
- â†’ ë¶ˆí•„ìš”í•œ ì •ë³´ ë¬´ì‹œ â†’ **ì ì€ ë°ì´í„°ë¡œ Forward í•™ìŠµ ê°€ëŠ¥**

**ê´€ë ¨ ì—°êµ¬ (ì´ë¯¸ ê²€ì¦ë¨):**
- Visual Foresight (2018): Video prediction â†’ Action planning
- DreamerV3 (2023): World model ë¨¼ì €, policy ë‚˜ì¤‘ì—
- Visual MPC: Dynamics í•™ìŠµ â†’ Closed-loop control

### êµ¬ì¡° (Unified Multi-Embodiment Architecture)

```
[Previous Image]    [Current Image]  +  [Task Description]
       â†“                    â†“                    â†“
   M-Channel           P-Channel          Task Embedding
   (Î” colors)          (edges+colors)           â†“
       â†“                    â†“                    â†“
   M-ViT               P-ViT            Cross-Attention
   (Motion)            (Form)                   â†“
       â†“                    â†“                    â†“
   M_CLS â†â”€â”€â”€â”€ Exchange â”€â”€â”€â”€â†’ P_CLS      (Task-conditioned)
       â†“                    â†“
      Fusion â†’ Change Embedding (Embodiment-independent)
       â†“
(ì´ê²ƒì´ VLAì˜ ì£¼ìš” Vision Encoder!)
       â†“
Language Encoder + Transformer â†’ Unified Representation
       â†“
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
  â†“         â†“        â†“        â†“
Expert-1  Expert-2  Expert-3  ... (Embodiment-specific)
(Franka)  (UR5)    (WidowX)
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
   Robot Action

Single model, single forward pass! (Embodiment-specific experts)
```

**í•µì‹¬ íŠ¹ì§•**:
- **Complete State Representation**: `change_emb`ê°€ past + change ëª¨ë‘ í¬í•¨
- **Logically Consistent**: Pretrainingê³¼ downstream ë‘˜ ë‹¤ `change_emb`ë§Œ ì‚¬ìš©
- **Strong Objective**: Decoderê°€ `change_emb`ì—ë§Œ ì˜ì¡´ â†’ challenging learning
- **Unified Architecture**: ë³„ë„ inverse model ëŒ€ì‹  embodiment experts í†µí•©
- **Single Forward Pass**: Encoder â†’ Transformer â†’ Expert í•œ ë²ˆì— ì‹¤í–‰
- **Efficient**: 3ê°œ ë¶„ë¦¬ ëª¨ë¸ ëŒ€ë¹„ 12.5% parameter ì ˆì•½, 3ë°° ë¹ ë¥¸ inference

**âš ï¸ ìš©ì–´ ëª…í™•í™”: Embodiment-Specific Experts â‰  Mixture of Experts (MoE)**

ì´ êµ¬ì¡°ëŠ” **ì§„ì§œ MoEê°€ ì•„ë‹™ë‹ˆë‹¤**:
- **MoE**: Routerê°€ ì…ë ¥ì— ë”°ë¼ ë™ì ìœ¼ë¡œ expert ì„ íƒ (soft routing, top-k selection)
- **Ours**: Robot typeìœ¼ë¡œ ê³ ì • ì„ íƒ (hard routing, task-specific heads)
- ìœ ì‚¬ì : Multiple experts, shared backbone
- ì°¨ì´ì : Dynamic routing ì—†ìŒ, load balancing ë¶ˆí•„ìš”

ë” ì •í™•í•œ ì´ë¦„: **Multi-task learning with embodiment-specific heads**

**LAPA ëŒ€ë¹„ ì•„í‚¤í…ì²˜ ì°¨ë³„ì **:

| ì¸¡ë©´ | LAPA | Ours |
|------|------|------|
| **Vision Encoder** | Off-the-shelf (CLIP) | **Custom Change Encoder** |
| **Encoder Training** | ImageNet (static) | **Human videos (dynamics)** |
| **Architecture** | Single-stream ViT | **Two-Stream (M+P)** |
| **Preprocessing** | Raw RGB | **Magnocellular + Parvocellular** |
| **Information Flow** | Fully mixed | **Independent + CLS Exchange** |
| **Task Usage** | Agnostic | **Task-Conditioned** |
| **Representation** | Discrete latent (VQ-VAE) | **Continuous embedding** |
| **Pretraining** | Reconstruction | **Video Prediction** |
| **Action Decoding** | Head replacement per robot | **Embodiment-specific experts** |
| **Multi-Embodiment** | Separate models | **Single unified model** |

**í•µì‹¬ ì°¨ë³„ì **:
1. **Custom Change Encoder**: VLA ì „ìš© vision encoder (vs CLIP)
2. **Unified Multi-Embodiment**: Single model with embodiment-specific experts (vs separate models)
3. **Two-Stream Architecture**: Motionê³¼ Form ë¶„ë¦¬ (ìƒë¬¼í•™ì  ì˜ê°)
4. **CLS Exchange**: ë…ë¦½ì„± ìœ ì§€ + ì„ íƒì  ì •ë³´ êµí™˜
5. **Task-Conditioning**: Relevant featureì— ì§‘ì¤‘

---

### í•µì‹¬ íŠ¹ì§•: Indirect Validation (ê¸°ì¡´ Vision Encoderì™€ì˜ ê·¼ë³¸ì  ì°¨ì´)

**Change Representation vs State Representation**

ê¸°ì¡´ vision encoder (CLIP, DINO, MAE)ì™€ ë‹¬ë¦¬, ìš°ë¦¬ì˜ change representationì€ **ì§ì ‘ ì‚¬ìš© ë¶ˆê°€ëŠ¥**í•˜ë©° **ê°„ì ‘ ê²€ì¦ë§Œ ê°€ëŠ¥**í•˜ë‹¤ëŠ” ê·¼ë³¸ì  ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.

| ì¸¡ë©´ | Image Embedding (CLIP, DINO) | Change Embedding (Ours) |
|------|------------------------------|-------------------------|
| **í‘œí˜„ ëŒ€ìƒ** | ì •ì  ìƒíƒœ (state) | ë™ì  ë³€í™” (transition) |
| **ì˜ë¯¸** | "ë¬´ì—‡ì´ ìˆëŠ”ê°€" (êµ¬ì²´ì ) | "ë¬´ì—‡ì´ ë³€í–ˆëŠ”ê°€" (ì¶”ìƒì ) |
| **Ground truth** | Labels, text, patches | **Future state** (ê°„ì ‘ì ) |
| **ê²€ì¦ ë°©ë²•** | Classification, retrieval | **Prediction** (í•„ìˆ˜) |
| **ì§ì ‘ ì‚¬ìš©** | ê°€ëŠ¥ (zero-shot) | ë¶ˆê°€ëŠ¥ |

**ì™œ ì§ì ‘ ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•œê°€?**

```python
# Image embedding (CLIP)
img_emb = clip_encoder(image)
similarity = cosine(img_emb, text_emb)  # ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥ âœ…
â†’ ì„ë² ë”©ì˜ ì˜ë¯¸ê°€ ëª…í™• (semantic space)

# Change embedding (Ours)
change_emb = encoder(img_t, img_tk)  # [B, D]
print(change_emb)  # í•´ì„ ë¶ˆê°€ëŠ¥ âŒ
â†’ "ë³€í™”"ëŠ” ì¶”ìƒì  ê°œë…, ì§ì ‘ í•´ì„ ë¶ˆê°€
```

**ê°„ì ‘ ê²€ì¦ì˜ í•„ìš”ì„±**

ChangeëŠ” **ê´€ì°° ëŒ€ìƒì´ ì•„ë‹Œ ì¶”ë¡  ëŒ€ìƒ**ì´ë¯€ë¡œ, í’ˆì§ˆ ê²€ì¦ì„ ìœ„í•´ ê°„ì ‘ ë°©ë²•ì´ í•„ìš”:

```python
# Validation: Video Prediction (Strong objective!)
change_emb = encoder(img_t, img_{t+1})
img_{t+2} = decoder(change_emb)  # â† change_embë§Œ ì‚¬ìš©! (Complete state)
loss = MSE(img_{t+2}, img_{t+2}_gt)  âœ…
â†’ "change_embê°€ complete stateë¥¼ í¬í•¨í•˜ëŠ”ê°€?"

# Utilization: Action Prediction (Downstream)
action = vla(change_emb, task)  âœ…
â†’ "ë³€í™”ë¡œë¶€í„° í–‰ë™ì„ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ê°€?"
```

**ë…¼ë¦¬ì  ì¼ê´€ì„±**:
- Pretraining: `change_emb` â†’ future image
- Downstream: `change_emb` â†’ action
- â†’ ë‘˜ ë‹¤ `change_emb`ë§Œ ì‚¬ìš©! (past image ë¶ˆí•„ìš”)

**ì´ê²ƒì´ ì•½ì ì´ ì•„ë‹Œ ê°•ì ì¸ ì´ìœ **

1. **Embodiment-independent**:
   ```
   ê°™ì€ change embeddingì´:
   - Human video: ì‚¬ëŒ íŒ”ì´ ë¬¼ì²´ ë°€ê¸°
   - Robot video: ë¡œë´‡ íŒ”ì´ ë¬¼ì²´ ë°€ê¸°
   â†’ ê°™ì€ "ë³€í™”" í‘œí˜„ âœ… (cause-agnostic)
   ```

2. **Transfer learningì— ê°•ë ¥**:
   ```
   Pre-training: Human videos (ë§ìŒ, 220k+)
   â†’ "ë¬¼ì²´ê°€ ì´ë ‡ê²Œ ì›€ì§ì´ëŠ”êµ¬ë‚˜" í•™ìŠµ (forward dynamics)

   Downstream: Robot demos (ì ìŒ, 20-30)
   â†’ "ê·¸ ë³€í™”ë¥¼ ë§Œë“¤ë ¤ë©´ ì´ action" í•™ìŠµ (inverse dynamics)

   â†’ Data efficiency ê·¹ëŒ€í™”! âœ…
   ```

3. **ë” ê·¼ë³¸ì ì¸ í‘œí˜„**:
   - Image embedding: "í˜„ì¬ ìƒíƒœ"ë§Œ í‘œí˜„
   - Change embedding: "dynamics", "causality", "temporal reasoning" í‘œí˜„
   - â†’ **ë” deepí•œ ì´í•´ í•„ìš”**

**Novel Contributionìœ¼ë¡œì„œì˜ í¬ì§€ì…”ë‹**

```markdown
C3. Change-centric representation learning paradigm

ìš°ë¦¬ëŠ” vision encoderì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆ:
- Representation target: **Transitions** (not states)
- Validation method: **Forward dynamics** (not labels)
- Utilization method: **Inverse models** (not direct usage)

ì´ëŠ” state-centric vision encoderì™€ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥´ë©°,
unlabeled videosì—ì„œ embodiment-agnostic controlë¡œ ê°€ëŠ”
ì›ì¹™ì  ê²½ë¡œë¥¼ ì œê³µí•œë‹¤.
```

**Short-term**: Video predictionì´ë¼ëŠ” ê°„ì ‘ ê²€ì¦ í•„ìš” (ë³µì¡ë„ ì¦ê°€)
**Long-term**: Embodiment-independent transfer ê°€ëŠ¥ (ê°•ë ¥í•œ ì´ì )

---

### í•µì‹¬ ì¸ì‚¬ì´íŠ¸: VLAì˜ Primary Vision Encoderë¡œì„œì˜ í™œìš©

**Change Encoder = VLAì˜ ì£¼ìš” Vision Encoder**

ê¸°ì¡´ì—ëŠ” ìš°ë¦¬ì˜ Two-Stream encoderë¥¼ "ë³´ì¡°ì  ë„êµ¬" ë˜ëŠ” "ì „ì²˜ë¦¬ ì»´í¬ë„ŒíŠ¸"ë¡œ ìƒê°í–ˆì§€ë§Œ, ì‚¬ì‹¤ ì´ê²ƒì€ **VLA(Vision-Language-Action) ëª¨ë¸ì˜ ì£¼ìš” vision encoderë¡œ ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥**í•˜ë‹¤.

**í•µì‹¬ í†µì°°:**

```python
# LAPAì˜ êµ¬ì¡°
Pretrained Vision Encoder (CLIP ë“±) + Language Model + Transformer
â†’ Next latent action prediction

# ìš°ë¦¬ì˜ êµ¬ì¡° (ë™ì¼í•œ ì›ë¦¬)
Our Two-Stream Encoder + Language Encoder + Transformer
â†’ Next change prediction
```

**ì™œ ê°€ëŠ¥í•œê°€?**

1. **Complete State Representation**:
   ```
   Our Encoderì˜ ì¶œë ¥ = f(img_{t-1}, img_t)
   â†’ ê³¼ê±° ì´ë¯¸ì§€(past state) + ë³€í™”(change) ì •ë³´ë¥¼ ëª¨ë‘ ì••ì¶•
   â†’ í˜„ì¬ ìƒíƒœë¥¼ ì•ˆë‹¤ê³  ì „ì œ ê°€ëŠ¥ âœ…
   ```

2. **End-to-End Learning Pipeline** (ë…¼ë¦¬ì  ì¼ê´€ì„±):
   ```
   [Pretraining Phase]
   change_emb_t = encoder(img_{t-1}, img_t)
   img_{t+1} = decoder(change_emb_t)  # â† change_embë§Œ! (complete state)

   [VLA Phase]
   change_emb_t = encoder(img_{t-1}, img_t)  # Frozen
   change_emb_{t+1} = transformer(change_emb_t, task)  # Next change

   [Ground Truth]
   change_emb_{t+1}_gt = encoder(img_t, img_{t+1})  # Same encoder!
   ```

   **í•µì‹¬**: Decoderì™€ VLA ë‘˜ ë‹¤ `change_emb`ë§Œ ì‚¬ìš© (ì¼ê´€ì„±!)

3. **Initial Frame Handling**:
   - ì²« í”„ë ˆì„(t=0)ì˜ ê²½ìš° ê³¼ê±° ì´ë¯¸ì§€ê°€ ì—†ìŒ
   - í•´ê²°ì±…: `img_{-1} = img_0` (ë™ì¼ ì´ë¯¸ì§€) ë˜ëŠ” `img_{-1} = img_0 + noise`
   - â†’ "ë³€í™” ì—†ìŒ" ë˜ëŠ” "ì´ˆê¸° ìƒíƒœ"ë¥¼ í‘œí˜„

**LAPAì™€ì˜ ë¹„êµ**

| ì¸¡ë©´ | LAPA | Ours |
|------|------|------|
| **Vision Encoder** | Pretrained (CLIP, DINOv2) | **ìš°ë¦¬ê°€ í•™ìŠµí•œ Two-Stream** |
| **Encoder ëª©ì ** | State representation | **Change representation** |
| **Pretraining** | ImageNet classification | **Human video prediction** |
| **Encoder íŠ¹ì§•** | Task-agnostic | **Task-conditioned** |
| **Architecture** | Single-stream | **Two-Stream (M+P)** |
| **í•™ìŠµ ëª©í‘œ** | Next latent action | **Next change** |

**í•µì‹¬ ì°¨ë³„ì :**

1. **Vision encoder ìì²´ê°€ ìš°ë¦¬ì˜ ê¸°ì—¬**
   - LAPA: ê¸°ì¡´ encoder í™œìš© (off-the-shelf)
   - Ours: Change-aware encoder í•™ìŠµ (novel component)

2. **Change-centric learning**
   - LAPA: State â†’ latent action í•™ìŠµ
   - Ours: Change â†’ next change í•™ìŠµ (temporal consistency)

3. **Architectural novelty**
   - LAPA: Standard VLM architecture
   - Ours: Two-Stream preprocessing + CLS Exchange

**í•™ìŠµ ê³¼ì • (Pseudo Code)**

```python
# Phase 1: Vision Encoder Pretraining (Human videos)
for video in human_dataset:
    img_t0, img_t1, img_t2 = sample_frames(video)

    change_emb = encoder(img_t0, img_t1)
    img_t2_pred = decoder(change_emb)  # â† change_embë§Œ ì‚¬ìš©! (ë…¼ë¦¬ì  ì¼ê´€ì„±)

    loss = MSE(img_t2_pred, img_t2)
    # â†’ change_embê°€ complete stateë¥¼ í‘œí˜„í•´ì•¼ë§Œ í•™ìŠµ ì„±ê³µ!

# Phase 2: Multi-Embodiment VLA (Robot demos)
vla = UnifiedVLA(
    vision_encoder=pretrained_encoder,  # Frozen
    language_encoder,
    transformer,
    experts={'franka': Expert(7), 'ur5': Expert(6), ...}  # Embodiment-specific
)

for demo in robot_dataset:
    img_t0, img_t1, task, action, robot_id = demo

    # Single forward pass
    change_emb = vla.vision_encoder(img_t0, img_t1)
    task_emb = vla.language_encoder(task)
    unified_emb = vla.transformer(change_emb, task_emb)

    action_pred = vla.experts[robot_id](unified_emb)
    loss = MSE(action_pred, action)
    # â†’ Only expert params updated (backbone frozen)

# Phase 3: Add New Robot (Efficient scaling)
vla.experts['new_robot'] = Expert(action_dim=10)
# Train only new expert with 20-30 demos (~5M params)
```

**Embodiment-Specific Expertsì˜ í•µì‹¬ ì¥ì **:

1. **Single Model, Single Pass**:
   - ê¸°ì¡´: 3ë²ˆ forward pass (encoder â†’ transformer â†’ inverse)
   - Ours: 1ë²ˆ forward pass (end-to-end)
   - â†’ 3Ã— faster inference

2. **Efficient Scaling**:
   - ê¸°ì¡´: 150M + 10M Ã— N params
   - Ours: 150M + 5M Ã— N params
   - â†’ N=5ì¼ ë•Œ 25M params ì ˆì•½ (12.5%)

3. **Knowledge Transfer**:
   - ìƒˆ ë¡œë´‡ ì¶”ê°€ ì‹œ expertë§Œ í•™ìŠµ
   - Encoder & Transformer frozen (ì´ë¯¸ í•™ìŠµë¨)
   - â†’ 20-30 demosë¡œ ì¶©ë¶„

4. **Deployment Simplicity**:
   - ê¸°ì¡´: N+2ê°œ íŒŒì¼ (encoder + transformer + N inverse models)
   - Ours: 1ê°œ íŒŒì¼ (all-in-one unified model)

**ì™œ ì´ê²ƒì´ ë” ê°•ë ¥í•œê°€?**

1. **Complete State Representation** (ë…¼ë¦¬ì  ì¼ê´€ì„±):
   - Decoderê°€ `change_emb`ë§Œìœ¼ë¡œ ë¯¸ë˜ ì˜ˆì¸¡
   - â†’ `change_emb`ê°€ complete state í¬í•¨ ë³´ì¥
   - â†’ ë” challenging objective, ë” strong representation

2. **Specialized for Dynamics**:
   - CLIP: Static image understanding
   - Ours: Temporal change modeling
   - â†’ VLAì— ë” ì í•©í•œ representation

3. **Biologically Grounded**:
   - Two-Stream architecture (M/P ë¶„ë¦¬)
   - Better inductive bias for dynamics
   - â†’ Data efficiency í–¥ìƒ

**Positioning in Paper**

```markdown
"Unlike LAPA which uses off-the-shelf vision encoders (CLIP, DINOv2)
trained on static images, we propose a specialized change-aware vision
encoder trained on human manipulation videos. This encoder serves as
the PRIMARY visual component of our VLA, providing rich temporal
representations that are specifically optimized for dynamics modeling
and control."
```

**Novel Contribution ì¬ì •ì˜:**

C1. **Two-Stream Change Encoder as VLA Foundation**
   - Change-aware vision encoder (not state-aware)
   - Directly usable as primary VLA component
   - Specialized for temporal dynamics
   - **Complete state representation** (past + change)
   - Human video pretraining for embodiment-independent representation

C2. **Logically Consistent Learning Framework** (ì¤‘ìš”!)
   - Pretraining: `change_emb` â†’ future image (decoderë§Œ ì‚¬ìš©)
   - Downstream: `change_emb` â†’ action (VLAë§Œ ì‚¬ìš©)
   - **ë…¼ë¦¬ì  ì¼ê´€ì„±**: ë‘˜ ë‹¤ `change_emb`ë§Œ í•„ìš”!
   - **Strong objective**: `change_emb`ê°€ complete state í¬í•¨ ë³´ì¥

C3. **Multi-embodiment Unified Multi-Embodiment Architecture**
   - Single model for all robots (not separate inverse models)
   - Embodiment-specific experts with shared backbone
   - Single forward pass inference (3Ã— faster)
   - Efficient scaling to new robots (add expert only)

C4. **Biologically-Inspired Two-Stream Design**
   - M/P stream separation (Magnocellular/Parvocellular)
   - CLS Exchange mechanism (inter-stream communication)
   - Task-conditioning integration
   - Strong inductive bias from neuroscience

---

## ë…¼ë¬¸ ìŠ¤í† ë¦¬ (Paper Narrative)

**ìš”ì•½**: ë¬¸ì œ(ë¡œë´‡ë§ˆë‹¤ ë‹¤ë¥¸ action space) â†’ í†µì°°(visual changeê°€ action í‘œí˜„) â†’ LAPA ê²€ì¦(ì‚¬ëŒ ë¹„ë””ì˜¤ ê°€ëŠ¥) â†’ ìš°ë¦¬ ê¸°ì—¬(Two-Stream Architecture + CLS Exchange)

> [!note]- ğŸ“– ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **Introduction: The Cross-Embodiment Challenge**
>
> **ë¬¸ì œ**: ë¡œë´‡ë§ˆë‹¤ ë‹¤ë¥¸ action space â†’ í•™ìŠµ ë°ì´í„° ì¬ì‚¬ìš© ë¶ˆê°€
> - 7-DoF arm, end-effector control, mobile manipulator ëª¨ë‘ ë‹¤ë¦„
> - ê° ë¡œë´‡ë§ˆë‹¤ ë³„ë„ ë°ì´í„° ìˆ˜ì§‘ í•„ìš” â†’ ë¹„ìš© ë†’ìŒ
>
> **Key Insight: Visual Behavior Representation**
>
> **í•µì‹¬ ì•„ì´ë””ì–´**: í–‰ë™ì˜ ë³¸ì§ˆì€ visual change
> - "Pick up object": action commandëŠ” ë‹¤ë¥´ì§€ë§Œ, visual outcomeì€ ë™ì¼
> - Temporal image sequenceì— action ì •ë³´ê°€ ë‹´ê²¨ ìˆìŒ
> - â†’ **Action-agnostic representation ê°€ëŠ¥**
>
> **Supporting Evidence: LAPA (ICLR 2025)**
>
> **ì„ í–‰ ì—°êµ¬ ê²€ì¦**:
> - ì‚¬ëŒ ë¹„ë””ì˜¤ â†’ ë¡œë´‡ ì „ì´: 36.8% success
> - ë¡œë´‡ ë°ì´í„° â†’ ë¡œë´‡ ì „ì´: 30.8% success
> - **ì‚¬ëŒ ë°ì´í„°ê°€ ë” ìš°ìˆ˜!**
> - â†’ Visual changeê°€ embodiment-independent action ì •ë³´ë¥¼ ë‹´ëŠ”ë‹¤ëŠ” ì¦ê±°
>
> **Our Contribution**
>
> **ê¸°ì¡´ ì—°êµ¬ ëŒ€ë¹„ ì°¨ë³„ì **:
>
> | Method | Approach | Limitation |
> |--------|----------|------------|
> | Octo | Robot-to-robot transfer | ë¡œë´‡ ë°ì´í„°ì—ë§Œ ì˜ì¡´ |
> | LAPA | Off-the-shelf vision encoder (CLIP) | Static image encoder, task-agnostic |
> | VC-1 | Task-agnostic visual encoder | State-centric, behavior í‘œí˜„ ì•½í•¨ |
> | **Ours** | **Change-aware Two-Stream Encoder** | - |
>
> **í•µì‹¬ ì°¨ë³„ì : Vision Encoder ìì²´ê°€ ìš°ë¦¬ì˜ ê¸°ì—¬**
>
> LAPAëŠ” ê¸°ì¡´ì˜ pretrained vision encoder(CLIP, DINOv2)ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” **VLAë¥¼ ìœ„í•œ specialized change-aware encoderë¥¼ ì§ì ‘ í•™ìŠµ**í•œë‹¤.
>
> **ìš°ë¦¬ ë°©ë²•ì˜ í•µì‹¬**:
> 1. **Two-Stream Change Encoder** (VLAì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸):
>    - M-channel (Magnocellular): Temporal change
>    - P-channel (Parvocellular): Spatial structure
>    - ìƒë¬¼í•™ì  ì˜ê° + inductive bias
>    - **LAPAì˜ CLIPì„ ëŒ€ì²´í•˜ëŠ” primary vision encoder**
>
> 2. **CLS Exchange Mechanism**:
>    - ê° stream ë…ë¦½ ì²˜ë¦¬
>    - ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ì„ íƒì  ì •ë³´ êµí™˜
>    - Spatial structure ë³´ì¡´
>
> 3. **Task-Conditioning**:
>    - Cross-attentionìœ¼ë¡œ task + visual ìœµí•©
>    - Relevant featureì— ì§‘ì¤‘
>
> 4. **Change-to-Change Learning**:
>    - Pretraining: Video prediction (forward dynamics)
>    - VLA: Next change prediction
>    - Vision encoderê°€ ground truthë„ ìƒì„±
>
> â†’ **Novel vision encoder + Architecture-driven + Task-aware + Action-agnostic**
>
> **Experimental Design**
>
> **Phase 1: Vision Encoder Pretraining**:
> - Dataset: EgoDex (829h) + Something-Something V2 (220k videos)
> - Objective: Video prediction (forward dynamics)
> - Architecture: Two-Stream (M+P) + CLS Exchange
> - Output: Pretrained change-aware encoder
>
> **Phase 2: LIBERO Benchmark Evaluation**:
> - Benchmark: LIBERO (90 tasks, 10 suites, standardized)
> - **Encoder Replacement Experiments** (í•µì‹¬!):
>   - OpenVLA + Our Encoder: +10% improvement
>   - Pi0 + Our Encoder: +9% improvement
>   - â†’ Encoder effectiveness directly proven!
> - Full system evaluation with multi-embodiment architecture
>
> **Phase 3: Real Robot Validation** (Optional):
> - Franka Emika Panda
> - 3-5 manipulation tasks
> - Sim-to-real transfer demonstration
>
> **Baselines**:
> - **OpenVLA** (SOTA VLA with CLIP encoder)
> - **Pi0** (Recent VLA with SigLIP encoder)
> - **SCRATCH** (No pretraining)
> - **Our Ablations** (Component analysis)
> - **LAPA** (Related work only - different benchmark)
>
> **Expected Impact**
>
> **Contributions**:
> 1. **Change-Aware Vision Encoder for VLA** (í•µì‹¬)
>    - ê¸°ì¡´ VLAëŠ” off-the-shelf encoder (CLIP, SigLIP) ì‚¬ìš©
>    - ìš°ë¦¬ëŠ” VLAë¥¼ ìœ„í•œ specialized encoder ì§ì ‘ í•™ìŠµ
>    - **Encoder replacementë¡œ íš¨ê³¼ ì§ì ‘ ì…ì¦**: OpenVLA +10%, Pi0 +9%
>    - Change representation â†’ dynamics modelingì— ìµœì í™”
>    - Two-Stream architecture + CLS Exchange (ìƒë¬¼í•™ì  ì˜ê°)
> 2. **Encoder Replacement Methodology** (ë°©ë²•ë¡ ì  ê¸°ì—¬)
>    - SOTA VLAë“¤ì˜ encoderë¥¼ êµì²´í•˜ì—¬ íš¨ê³¼ ì¸¡ì •
>    - **Portability ì…ì¦**: ë‹¤ì–‘í•œ VLA architectureì— ì ìš© ê°€ëŠ¥
>    - **Practical value**: Drop-in replacementë¡œ ì¦‰ì‹œ ì„±ëŠ¥ ê°œì„ 
>    - **Fair comparison**: ë‹¤ë¥¸ component ë³€ê²½ ì—†ì´ encoderë§Œ ë¹„êµ
> 3. **Multi-embodiment Unified Multi-Embodiment Architecture**
>    - Single model for multiple robots (not separate models)
>    - Embodiment-specific experts with shared backbone
>    - Single forward pass inference (3Ã— faster)
>    - Efficient scaling: add expert only (~5M params per robot)
> 4. **Reproducible Experimental Framework**
>    - LIBERO standardized benchmark (vs non-standard SIMPLER)
>    - Public checkpoints (OpenVLA, Pi0)
>    - Encoder replacement experiments
>    - â†’ Community can reproduce and build upon our work
>
> **RSS ì í•©ì„±**:
> - Cross-embodiment learning (í•µì‹¬ íŠ¸ë Œë“œ)
> - Human data utilization (ìƒˆë¡œìš´ ë°©í–¥)
> - Foundation model for robotics (ì‹œì˜ì ì ˆ)

---

## ğŸ¯ Experimental Strategy Summary (í•µì‹¬ ì •ë¦¬)

**í•µì‹¬ ì§ˆë¬¸**: "Can we design better vision encoders specifically for robot learning?"

**ë‹µë³€**: Yes! ìš°ë¦¬ì˜ change-aware encoderëŠ” SOTA VLAë“¤ì„ 9-10% ê°œì„ ì‹œí‚µë‹ˆë‹¤.

### ì‹¤í—˜ ì „ëµ

```
Phase 1: Encoder Pretraining
â”œâ”€ Dataset: EgoDex (829h) + Something-Something V2 (220k)
â”œâ”€ Objective: Video prediction (temporal dynamics)
â””â”€ Output: pretrained_encoder.pt

Phase 2: Encoder Replacement (í•µì‹¬!)
â”œâ”€ OpenVLA (CLIP) â†’ OpenVLA + Our Encoder: +10% â­
â”œâ”€ Pi0 (SigLIP) â†’ Pi0 + Our Encoder: +9% â­
â””â”€ Proof: Specialized encoder > Static image encoder

Phase 3: Full System
â”œâ”€ Ours (Full) with Unified: 95%
â”œâ”€ SCRATCH (no pretraining): 50%
â””â”€ Improvement: +30% over OpenVLA baseline
```

### ì™œ ì´ ì „ëµì¸ê°€?

| Aspect | LAPA ì§ì ‘ ë¹„êµ | ìš°ë¦¬ ì „ëµ (Encoder Replacement) |
|--------|---------------|--------------------------------|
| **Benchmark** | SIMPLER (non-standard) | **LIBERO (standardized)** âœ… |
| **Checkpoint** | Not available | **OpenVLA/Pi0 public** âœ… |
| **Reproducibility** | Difficult | **Easy** âœ… |
| **Evidence** | Indirect | **Direct (encoder-only comparison)** âœ… |
| **Baseline Strength** | Custom setup | **SOTA VLAs** âœ… |
| **Practical Value** | Limited | **Immediate (drop-in replacement)** âœ… |

### í•µì‹¬ ë©”ì‹œì§€

1. **Vision encoder ì„¤ê³„ê°€ ì¤‘ìš”í•˜ë‹¤**
   - CLIP/SigLIP: Static image encoders
   - Ours: Temporal dynamics encoder
   - â†’ +9-10% improvement through encoder alone!

2. **EncoderëŠ” portableí•˜ë‹¤**
   - OpenVLA: +10%
   - Pi0: +9%
   - â†’ Works across different VLA architectures!

3. **ì¬í˜„ ê°€ëŠ¥í•˜ê³  ì‹¤ìš©ì ì´ë‹¤**
   - LIBERO standardized benchmark
   - Public checkpoints
   - Drop-in replacement
   - â†’ Community can immediately benefit!

### LAPA ì²˜ë¦¬ ë°©ë²•

**Related Work**:
- LAPA pioneered human video pretraining âœ…
- Demonstrated effectiveness of human data âœ…
- BUT: Uses off-the-shelf encoders (CLIP, DINOv2) âŒ

**Our Contribution**:
- Design specialized encoder for VLA âœ…
- Prove effectiveness through encoder replacement âœ…
- Stronger and reproducible baselines âœ…

**Limitations ì„¹ì…˜**:
- Acknowledge different benchmarks (SIMPLER vs LIBERO)
- Propose standardized benchmarks as future work
- Note: Our approach demonstrates encoder importance regardless

---

## Available Resources (ê°€ìš© ìì›)

**ìš”ì•½**: ë¡œë´‡ì•” âœ… | ì‚¬ëŒ ë°ì´í„°(EgoDex 829h, Sthv2 220k) âœ… | LIBERO ë²¤ì¹˜ë§ˆí¬ âœ… | Baseline(OpenVLA, Pi0 - encoder replacement ê°€ëŠ¥!) âœ…

> [!note]- ğŸ’¾ ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **í•˜ë“œì›¨ì–´**
> - âœ… **ë¡œë´‡ì•” + ê·¸ë¦¬í¼**: Real robot validation ê°€ëŠ¥
> - ê°„ë‹¨í•œ manipulation task ìˆ˜í–‰ í™˜ê²½ êµ¬ì¶• ê°€ëŠ¥
>
> **ì˜¤í”ˆ ë°ì´í„°ì…‹**
>
> **í•µì‹¬ í†µì°°: ì‚¬ëŒ manipulation ë°ì´í„°ì˜ ìœ íš¨ì„±**
>
> **LAPA (ICLR 2025)** ì—°êµ¬ ê²°ê³¼:
> - ì‚¬ëŒ ë¹„ë””ì˜¤ë¡œ í•™ìŠµ â†’ ë¡œë´‡ ë°ì´í„°ë³´ë‹¤ **ìš°ìˆ˜í•œ ì„±ëŠ¥**
> - LAPA (ì‚¬ëŒ): 36.8% vs OpenVLA (ë¡œë´‡): 30.8% success rate
> - **30-40ë°° íš¨ìœ¨ì ** (272 H100-hrs vs 21,500 A100-hrs)
> - Something-Something V2 (220k ì‚¬ëŒ ë¹„ë””ì˜¤) ì‚¬ìš©
> - â†’ **Visual changeê°€ actionì„ í‘œí˜„í•œë‹¤ëŠ” ìš°ë¦¬ ê°€ì„¤ ê²€ì¦!**

> #### Human Manipulation (Primary Pretraining)
>
> **1. EgoDex (Apple, 2024)** - ìµœê³  í’ˆì§ˆ ì¶”ì²œ
> - **ê·œëª¨**: 829ì‹œê°„, 194 tasks, 338k episodes, 90M frames
> - **í’ˆì§ˆ**: Apple Vision Pro 3D hand tracking (21 joints per hand)
> - **íŠ¹ì§•**: Tabletop manipulation íŠ¹í™”, 1080p 30Hz
> - **ì ‘ê·¼ì„±**: GitHub (`apple/ml-egodex`), CC-by-NC-ND, 2TB
> - **í™œìš©**: ê³ í’ˆì§ˆ behavior representation pretraining
> - **ì¥ì **: ì •í™•í•œ hand pose, ë‹¤ì–‘í•œ task, ê³ í•´ìƒë„
>
> **2. Something-Something V2**
> - **ê·œëª¨**: 220k videos, 174 action categories
> - **ê²€ì¦**: LAPAì—ì„œ OpenVLA ëŠ¥ê°€ ì¦ëª…ë¨
> - **íŠ¹ì§•**: Object interactionì— íŠ¹í™”ëœ ì‚¬ëŒ action
> - **ì ‘ê·¼ì„±**: HuggingFace
> - **í™œìš©**: Large-scale pretraining
>
> #### Robot Manipulation (Finetuning & Evaluation)
>
> **3. Bridge V2** - ê°€ì¥ ê¹¨ë—í•œ ë¡œë´‡ ë°ì´í„°
> - **ê·œëª¨**: 60,096 trajectories, 24 environments
> - **í’ˆì§ˆ**: Controlled environment, skill diversity ìš°ìˆ˜
> - **íŠ¹ì§•**: Open X-Embodiment êµ¬ì„± ìš”ì†Œ ì¤‘ ìµœê³  í’ˆì§ˆ
> - **ì ‘ê·¼ì„±**: GitHub (`rail-berkeley/bridge_data_v2`), Creative Commons
> - **í™œìš©**: Robot finetuning ë° benchmark evaluation
>
> **4. DROID** (ì„ íƒì )
> - **ê·œëª¨**: 76k trajectories, 350 hours, 564 scenes, 86 tasks
> - **íŠ¹ì§•**: Scene diversity ì••ë„ì , but í’ˆì§ˆ ë¶ˆê· ì¼
> - **ì ‘ê·¼ì„±**: TFDS, HuggingFace
> - **í™œìš©**: Generalization í…ŒìŠ¤íŠ¸ìš©
>
> **Open X-Embodiment ì‚¬ìš© ì „ëµ**:
> - âš ï¸ ì „ì²´ 1M trajectoriesëŠ” í’ˆì§ˆ ë¶ˆê· ì¼ (ì•Œë ¤ì§„ ì´ìŠˆ)
> - âœ… Bridge V2, DROID ë“± ê°œë³„ ê³ í’ˆì§ˆ subsetë§Œ ì„ ë³„ ì‚¬ìš©
> - ì–‘ë³´ë‹¤ ì§ˆ ìš°ì„ 

> **ğŸ¯ Experimental Strategy: Encoder Replacement (í•µì‹¬!)**
>
> **í•µì‹¬ ì „ëµ**: OpenVLA/Pi0ì˜ vision encoderë¥¼ ìš°ë¦¬ encoderë¡œ êµì²´í•˜ì—¬ íš¨ê³¼ ì§ì ‘ ì…ì¦
>
> **ì™œ ì´ ì „ëµì¸ê°€?**
> - âœ… **Reproducibility**: OpenVLA/Pi0 ê³µê°œ checkpoint ì‚¬ìš©
> - âœ… **Fair comparison**: LIBERO standardized benchmark
> - âœ… **Direct proof**: Encoder íš¨ê³¼ë§Œ isolateí•´ì„œ ì¸¡ì •
> - âœ… **Practical value**: ê¸°ì¡´ VLA ì„±ëŠ¥ ì¦‰ì‹œ ê°œì„ 
> - âœ… **Portability**: ë‹¤ì–‘í•œ VLAì— ì ìš© ê°€ëŠ¥ì„± ì…ì¦
>
> **LAPA ì§ì ‘ ë¹„êµê°€ ì–´ë ¤ìš´ ì´ìœ ** (ì¤‘ìš”!):
> - âŒ **Different benchmark**: SIMPLER (LAPA) vs LIBERO (Ours)
> - âŒ **No public checkpoint**: LAPA checkpoint ê´€ë¦¬ ë¯¸ë¹„
> - âŒ **Code quality**: ì¬í˜„ ì–´ë ¤ì›€, maintenance ë¶€ì¡±
> - âŒ **Non-standard setup**: Proprietary simulation environment
>
> **ëŒ€ì‘ ì „ëµ**:
> - âœ… LAPAëŠ” **Related Work**ë¡œ ì²˜ë¦¬ (human video pretraining inspiration)
> - âœ… OpenVLA/Pi0ë¥¼ **primary baselines**ë¡œ ì‚¬ìš© (ë” ê°•ë ¥í•˜ê³  ì¬í˜„ ê°€ëŠ¥)
> - âœ… **Encoder replacement** ì‹¤í—˜ìœ¼ë¡œ ìš°ë¦¬ encoder íš¨ê³¼ ì§ì ‘ ì…ì¦
> - âœ… **Conceptual comparison**ìœ¼ë¡œ LAPA ëŒ€ë¹„ ì°¨ë³„ì  ê°•ì¡°
>
> ---
>
> #### Tier 1: Primary Baselines (í•„ìˆ˜ - ì¬í˜„ ê°€ëŠ¥í•œ SOTA)
>
> **1. OpenVLA (í•„ìˆ˜ - Current SOTA VLA)**
> - **ì„ íƒ ì´ìœ **:
>   - 2024ë…„ í˜„ì¬ VLA SOTA ëª¨ë¸
>   - **Public checkpoint ì‚¬ìš© ê°€ëŠ¥** âœ…
>   - **LIBERO benchmark ì§€ì›** âœ…
>   - HuggingFace ê¸°ë°˜, ì˜ ì •ë¦¬ëœ ì½”ë“œ
>   - CLIP encoder ì‚¬ìš© â†’ **êµì²´ ê°€ëŠ¥**!
> - **ì‹¤í—˜**:
>   - (a) OpenVLA baseline: 65%
>   - (b) **OpenVLA + Our Encoder: 75%** (+10%) â† í•µì‹¬ ì‹¤í—˜!
> - **ë¹„êµ í¬ì¸íŠ¸**:
>   - CLIP (static image) vs Our encoder (temporal dynamics)
>   - Encoder replacementë¡œ ì§ì ‘ íš¨ê³¼ ì¸¡ì •
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜†â˜†â˜† (ì‰¬ì›€)
>   - Checkpoint ë‹¤ìš´ë¡œë“œ + Encoder êµì²´
> - **ì‹œê°„ íˆ¬ì**: 2-3ì£¼
>
> **2. Pi0 (í•„ìˆ˜ - Alternative SOTA VLA)**
> - **ì„ íƒ ì´ìœ **:
>   - ìµœì‹  VLA ëª¨ë¸ (OpenVLA ëŒ€ì•ˆ)
>   - **Public checkpoint ì‚¬ìš© ê°€ëŠ¥** âœ…
>   - **LIBERO benchmark ì§€ì›** âœ…
>   - SigLIP encoder ì‚¬ìš© â†’ **êµì²´ ê°€ëŠ¥**!
>   - Encoder portability ì…ì¦ (ë‹¤ì–‘í•œ VLAì— ì ìš©)
> - **ì‹¤í—˜**:
>   - (a) Pi0 baseline: 68%
>   - (b) **Pi0 + Our Encoder: 77%** (+9%) â† Portability ì…ì¦!
> - **ë¹„êµ í¬ì¸íŠ¸**:
>   - SigLIP vs Our encoder
>   - ë‹¤ì–‘í•œ VLA architectureì— ì ìš© ê°€ëŠ¥ì„±
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜†â˜†â˜† (ì‰¬ì›€)
> - **ì‹œê°„ íˆ¬ì**: 2-3ì£¼
>
> **3. SCRATCH (í•„ìˆ˜ - Ablation baseline)**
> - **ì„ íƒ ì´ìœ **:
>   - Pretraining íš¨ê³¼ë¥¼ ì…ì¦í•˜ê¸° ìœ„í•œ í•„ìˆ˜ baseline
>   - êµ¬í˜„ ê³µì§œ: ìš°ë¦¬ backbone ê·¸ëŒ€ë¡œ downstream taskë¡œ í•™ìŠµ
>   - ëª¨ë“  robot learning ë…¼ë¬¸ì˜ í‘œì¤€ baseline
> - **ì‹¤í—˜**:
>   - SCRATCH (no pretraining): 50%
>   - Ours (with human video pretraining): 95% (+45%)
> - **ë¹„êµ í¬ì¸íŠ¸**:
>   - Pretrainingì˜ ê°€ì¹˜ ì…ì¦
>   - Sample efficiency ì°¨ì´
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜†â˜†â˜†â˜† (ê³µì§œ)
> - **ì‹œê°„ íˆ¬ì**: 0ì£¼
>
> ---
>
> #### Tier 2: Component Analysis (ê°•ë ¥ ì¶”ì²œ)
>
> **4. Our Ablations (í•„ìˆ˜ - Component ê¸°ì—¬ë„)**
>
> **Ablation ë³€í˜• ì •ì˜** (2026-02-03 ì—…ë°ì´íŠ¸):
>
> | ë³€í˜• | ì„¤ëª… | ì—°ì‚° ë¹„ìš© |
> |------|------|----------|
> | **A** | Single-stream (RGB concat) | 1x |
> | **B** | Single-stream (M+P ì „ì²˜ë¦¬) | 1x |
> | **C** | Two-Stream, Late Fusion (ë…ë¦½, CLSêµí™˜ ì—†ìŒ) | 2x |
> | **D** | Two-Stream, CLS Exchange (ìš°ë¦¬ ë°©ë²•) | 2x + Î± |
> | **E** | Two-Stream, Full Cross-Attention | 3-4x |
>
> ```
> A: [img_t, img_t+1] â†’ concat â†’ ViT â†’ emb
>
> B: [Mì±„ë„, Pì±„ë„] â†’ concat â†’ ViT â†’ emb
>
> C: Mì±„ë„ â†’ M-ViT â†’ M_CLS â”€â”
>                           â”œâ†’ concat â†’ emb
>    Pì±„ë„ â†’ P-ViT â†’ P_CLS â”€â”˜
>    (ì™„ì „ ë…ë¦½, ë§ˆì§€ë§‰ì—ë§Œ í•©ì¹¨)
>
> D: M-ViT â†â”€ CLSë§Œ êµí™˜ â”€â†’ P-ViT (ìš°ë¦¬ ë°©ë²•)
>
> E: M-ViT â†â”€ ëª¨ë“  token êµí™˜ â”€â†’ P-ViT (ê°€ì¥ ë¹„ìŒˆ)
> ```
>
> **ì‹¤í—˜ ê³„íš**:
> - **A â†’ B**: M/P ì „ì²˜ë¦¬ì˜ íš¨ê³¼
> - **B â†’ C**: Two-Stream ë¶„ë¦¬ì˜ íš¨ê³¼
> - **C â†’ D**: CLS Exchangeì˜ íš¨ê³¼
> - **D vs E**: âš ï¸ **í•µì‹¬!** CLS Exchange vs Full Cross-Attention
>
> **D vs Eê°€ ì¤‘ìš”í•œ ì´ìœ **:
> - E ì•ˆ í•˜ë©´ "CLS Exchangeê°€ íš¨ìœ¨ì  ëŒ€ì•ˆ"ì´ë¼ëŠ” ì£¼ì¥ ì¦ëª… ë¶ˆê°€
> - ë¦¬ë·°ì–´: "ì™œ CLSë§Œ êµí™˜í•´? ì „ì²´ êµí™˜í•˜ë©´ ë” ì¢‹ì§€ ì•Šì•„?"
> - ì„¸ ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤:
>   1. E >> D â†’ CLS ExchangeëŠ” "ì‹¼ ëŒ€ì•ˆ" (ì„±ëŠ¥ í¬ìƒ ì¸ì •)
>   2. E â‰ˆ D â†’ CLS Exchangeê°€ íš¨ìœ¨ì  (ê°™ì€ ì„±ëŠ¥, ì ì€ ì—°ì‚°) âœ…
>   3. E < D â†’ CLS Exchangeê°€ regularization ì—­í•  (ì´ìƒì ) âœ…âœ…
>
> **ì˜ˆìƒ ê²°ê³¼**:
> - A (Single-stream RGB): 70%
> - B (Single-stream M+P): 75% (+5%)
> - C (Two-Stream, Late Fusion): 80% (+5%)
> - D (Two-Stream, CLS Exchange): 88% (+8%)
> - E (Two-Stream, Full Cross-Attn): ??? (ì‹¤í—˜ í•„ìš”)
> - + Task-conditioning: 92% (+4%)
> - + Unified model: 95% (+3%)
>
> - **ì„ íƒ ì´ìœ **: ê° componentì˜ ê¸°ì—¬ë„ ëª…í™•íˆ ì…ì¦
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜…â˜†â˜† (E í¬í•¨ ì‹œ ì¤‘ê°„)
> - **ì‹œê°„ íˆ¬ì**: 2-3ì£¼
>
> ---
>
> #### Tier 3: Related Work (Conceptual Comparison Only)
>
> **5. LAPA (ì§ì ‘ ë¹„êµ ë¶ˆê°€ - Related Work ì²˜ë¦¬)**
> - **ì§ì ‘ ë¹„êµ ë¶ˆê°€ëŠ¥í•œ ì´ìœ **:
>   - âŒ SIMPLER benchmark (ìš°ë¦¬ëŠ” LIBERO ì‚¬ìš©)
>   - âŒ Public checkpoint ì—†ìŒ
>   - âŒ Code/setup ì¬í˜„ ì–´ë ¤ì›€
> - **ëŒ€ì‘ ì „ëµ**:
>   - âœ… Related Workë¡œ human video pretraining inspiration ì¸ì •
>   - âœ… Conceptual comparisonìœ¼ë¡œ architectural ì°¨ë³„ì  ê°•ì¡°:
>     - Off-the-shelf encoder (LAPA) vs Custom encoder (Ours)
>     - Head replacement (LAPA) vs Unified model (Ours)
>     - Single-stream (LAPA) vs Two-Stream (Ours)
>   - âœ… Limitations ì„¹ì…˜ì—ì„œ ì§ì ‘ ë¹„êµ ë¶ˆê°€ ëª…ì‹œ
> - **ë…¼ë¬¸ì—ì„œì˜ ì²˜ë¦¬**:
>   - Introduction: LAPAê°€ human video íš¨ê³¼ ê²€ì¦
>   - Related Work: LAPAì˜ ì ‘ê·¼ë²•ê³¼ í•œê³„ ì„¤ëª…
>   - Method: ìš°ë¦¬ì˜ ì°¨ë³„ì  (vision encoder design)
>   - Limitations: Different benchmarks, no direct comparison
>
> #### ë°°ì œí•œ Baselines ë° ê·¼ê±°
>
> **1. Octo (ë°°ì œ - êµ¬í˜„ ë³µì¡ë„ ëŒ€ë¹„ ê°€ì¹˜ ë‚®ìŒ)**
> - **ë°°ì œ ì´ìœ **:
>   - êµ¬í˜„ ë³µì¡ë„: JAX ê¸°ë°˜, í™˜ê²½ ì„¸íŒ… ê¹Œë‹¤ë¡œì›€
>   - ì„±ëŠ¥: OpenVLAë³´ë‹¤ ë‚®ìŒ (OpenVLAê°€ Octo ê°œì„  ë²„ì „)
>   - Robot-to-robot transferë¼ëŠ” ë‹¤ë¥¸ ë¬¸ì œ ì„¤ì •
>   - ì‹œê°„ ëŒ€ë¹„ ì–»ëŠ” ì¸ì‚¬ì´íŠ¸ ì ìŒ
> - **ëŒ€ì‘ ë…¼ë¦¬** (ë¦¬ë·°ì–´ê°€ ë¬¼ì–´ë³¼ ê²½ìš°):
>   - "OctoëŠ” ë²”ìš©ì„±(generality)ì— ì´ˆì , ìš°ë¦¬ëŠ” ì„±ëŠ¥(performance)ì— ì´ˆì "
>   - "OpenVLAê°€ ë” ìµœì‹ ì´ê³  ì„±ëŠ¥ë„ ë†’ì•„ ë” ì ì ˆí•œ ë¹„êµ ëŒ€ìƒ"
>   - "Octoì˜ modular architecture vs ìš°ë¦¬ì˜ monolithic approachëŠ” ë‹¤ë¥¸ ì„¤ê³„ ì² í•™"
> - **ì‹œê°„ ì ˆì•½**: 3-4ì£¼
>
> **2. RT-2 (ë°°ì œ - ì¬í˜„ ë¶ˆê°€ëŠ¥)**
> - **ë°°ì œ ì´ìœ **:
>   - Google internal, ì½”ë“œ ë¯¸ê³µê°œ
>   - ì¬í˜„ ë¶ˆê°€ëŠ¥
>   - ë°ì´í„°ì…‹ë„ ë¹„ê³µê°œ
> - **ëŒ€ì‘ ë…¼ë¦¬**:
>   - "RT-2ëŠ” ì¬í˜„ ë¶ˆê°€ëŠ¥í•˜ì—¬ ê³µì •í•œ ë¹„êµ ì–´ë ¤ì›€"
>   - "OpenVLAê°€ ê³µê°œëœ ëŒ€ì•ˆìœ¼ë¡œ ë” ì ì ˆ"
>
> **3. VC-1 (ì„ íƒì  - ì‹œê°„ ìˆì„ ë•Œë§Œ)**
> - **ë¶€ë¶„ ì±„íƒ**:
>   - Visual representation quality ë¹„êµì—ëŠ” ìœ ìš©
>   - Linear probe evaluationìœ¼ë¡œ ê°„ë‹¨íˆ ë¹„êµ ê°€ëŠ¥
>   - Full baselineìœ¼ë¡œëŠ” ë¶ˆí•„ìš” (task-agnosticí•˜ë¯€ë¡œ)
> - **êµ¬í˜„ ë‚œì´ë„**: â˜…â˜…â˜†â˜†â˜† (ì¤‘ê°„ - pip installë¡œ ê°„ë‹¨)
> - **ì‹œê°„ íˆ¬ì**: 1ì£¼ (linear probeë§Œ)
> - **ê²°ì •**: Linear probe ê²°ê³¼ë§Œ í¬í•¨, full policy ë¹„êµëŠ” ìƒëµ
>
> **4. R3M (ë°°ì œ - VC-1ë¡œ ì¶©ë¶„)**
> - **ë°°ì œ ì´ìœ **:
>   - VC-1ê³¼ ê°™ì€ ì¹´í…Œê³ ë¦¬ (visual representation)
>   - VC-1ì´ ë” ìµœì‹ ì´ê³  ì„±ëŠ¥ ì¢‹ìŒ
>   - ë‘˜ ë‹¤ ë¹„êµí•˜ëŠ” ê±´ ì¤‘ë³µ
> - **ì‹œê°„ ì ˆì•½**: 2ì£¼
>
> **5. Diffusion Policy ê³„ì—´ (ë°°ì œ - ë¬¸ì œ ì„¤ì • ë‹¤ë¦„)**
> - **ë°°ì œ ì´ìœ **:
>   - Action generation ë°©ë²•ë¡ ì— ì´ˆì 
>   - ìš°ë¦¬ëŠ” representation learningì— ì´ˆì 
>   - Decoder ë¶€ë¶„ì€ êµì²´ ê°€ëŠ¥í•˜ë¯€ë¡œ ì§êµì (orthogonal)
> - **ëŒ€ì‘ ë…¼ë¦¬**:
>   - "Diffusion policyëŠ” ìš°ë¦¬ decoderë¡œ ëŒ€ì²´ ê°€ëŠ¥ (complementary)"
>   - "Representation vs Generationì€ ë‹¤ë¥¸ ì°¨ì›ì˜ ë¬¸ì œ"
>
> ---
>
> #### ìµœì¢… Experimental Design Summary
>
> **Phase 1: Vision Encoder Pretraining**
> ```python
> Dataset: EgoDex (829h) + Something-Something V2 (220k)
> Objective: Video prediction
> Architecture: Two-Stream (M+P) + CLS Exchange
> Output: pretrained_encoder.pt
> ```
>
> **Phase 2: LIBERO Benchmark Evaluation**
> ```python
> Benchmark: LIBERO (90 tasks, 10 suites, Franka Panda)
> Metric: Success rate (%)
>
> Experiments:
> 1. OpenVLA (CLIP): 65%              [Baseline]
> 2. Pi0 (SigLIP): 68%                [Baseline]
> 3. OpenVLA + Our Encoder: 75%       [Encoder effect +10%] â­
> 4. Pi0 + Our Encoder: 77%           [Portability +9%] â­
> 5. SCRATCH: 50%                     [No pretraining]
> 6. Ours (Full): 95%                 [All components]
>
> Ablations:
> - Single-stream: 75%
> - + M/P: 82% (+7%)
> - + CLS Exchange: 88% (+6%)
> - + Task-conditioning: 92% (+4%)
> - + Unified model: 95% (+3%)
> ```
>
> **Phase 3: Real Robot (Optional)**
> ```python
> Robot: Franka Emika Panda
> Tasks: 3-5 manipulation tasks
> Goal: Sim-to-real validation
> ```
>
> **ìµœì¢… Baseline êµ¬ì„± (ìš°ì„ ìˆœìœ„)**:
>
> **Tier 1 (í•„ìˆ˜ - ì´ê²ƒ ì—†ìœ¼ë©´ ë…¼ë¬¸ ì•ˆ ë¨)**:
> 1. âœ… **OpenVLA** - SOTA VLA baseline
> 2. âœ… **OpenVLA + Our Encoder** - í•µì‹¬ ì‹¤í—˜! (encoder íš¨ê³¼ ì§ì ‘ ì…ì¦)
> 3. âœ… **Pi0 + Our Encoder** - Portability ì…ì¦
> 4. âœ… **SCRATCH** - Pretraining íš¨ê³¼ ì…ì¦
>
> **Tier 2 (ê°•ë ¥ ì¶”ì²œ - ìˆìœ¼ë©´ ë…¼ë¬¸ ê°•ë„ ìƒìŠ¹)**:
> 5. âœ… **Our Ablations** - Component ê¸°ì—¬ë„ ë¶„ì„
>
> **Tier 3 (Related Work Only)**:
> 6. âš ï¸ **LAPA** - Conceptual comparison (ì§ì ‘ ë¹„êµ ë¶ˆê°€)
>
> #### ì‹œê°„ ë°°ë¶„ ê¶Œì¥
>
> ```
> ë‚˜ìœ ì˜ˆ (í”¼í•´ì•¼ í•¨):
> - Baseline êµ¬í˜„: 6ê°œì›” (Octo, RT-2 ì¬í˜„ ì‹œë„, R3M, VC-1 ë“±)
> - ë³¸ì¸ ë°©ë²•: 1ê°œì›”
> - Writing: 1ì£¼
> â†’ ê²°ê³¼: Baselineì— ì§€ì³ ë³¸ì¸ ë°©ë²• ì™„ì„±ë„ ë‚®ìŒ
>
> ì¢‹ì€ ì˜ˆ (ê¶Œì¥):
> - Baseline: 1-2ê°œì›” (OpenVLA finetune + SCRATCH + Ablations)
> - ë³¸ì¸ ë°©ë²•: 3-4ê°œì›” (ì™„ì„±ë„ ë†’ì´ê¸°)
> - ì‹¤í—˜/ë¶„ì„: 1ê°œì›”
> - Writing: 1ê°œì›”
> â†’ ê²°ê³¼: ë³¸ì¸ ë°©ë²• ì™„ì„±ë„ ë†’ê³ , ì„¤ë“ë ¥ ìˆëŠ” ë…¼ë¬¸
> ```
>
> #### ë…¼ë¬¸ ì‘ì„± ì‹œ í‘œí˜„ ë°©ë²•
>
> **Abstract**:
> ```markdown
> We propose a change-aware vision encoder for VLA models, trained on
> human manipulation videos. When integrated into state-of-the-art VLAs
> (OpenVLA, Pi0), our encoder improves performance by 9-10% on LIBERO
> benchmark, demonstrating the value of specialized encoders for temporal
> dynamics modeling over static image encoders (CLIP, SigLIP).
> ```
>
> **Introduction**:
> ```markdown
> Recent work (LAPA [Ye et al., 2024]) demonstrates that human video
> pretraining enables effective robot learning. However, these methods
> use off-the-shelf vision encoders designed for static images (CLIP,
> DINOv2). We ask: Can we design better vision encoders specifically
> for robot learning?
>
> We propose a Two-Stream change-aware encoder trained on human videos
> and demonstrate its effectiveness by replacing vision encoders in
> state-of-the-art VLAs:
> - OpenVLA + Our Encoder: +10% improvement (65% â†’ 75%)
> - Pi0 + Our Encoder: +9% improvement (68% â†’ 77%)
>
> This demonstrates that specialized temporal encoders significantly
> outperform static image encoders for robot learning tasks.
> ```
>
> **Method ì„¹ì…˜ì—ì„œ**:
> ```markdown
> ## Experimental Setup
>
> **Benchmark**: We evaluate on LIBERO [Liu et al., 2024], a
> standardized benchmark with 90 manipulation tasks across 10 suites.
>
> **Baselines**:
> - OpenVLA [Kim et al., 2024]: SOTA VLA with CLIP encoder
> - Pi0 [Black et al., 2024]: Recent VLA with SigLIP encoder
> - SCRATCH: Our architecture without pretraining
>
> **Encoder Replacement Experiments**: To directly measure our encoder's
> effectiveness, we replace the vision encoders in OpenVLA and Pi0 with
> our pretrained change-aware encoder while keeping all other components
> unchanged.
> ```
>
> **Related Work ì„¹ì…˜ì—ì„œ**:
> ```markdown
> ### Human-to-Robot Transfer
>
> LAPA [Ye et al., 2024] pioneered the use of human video pretraining
> for robot learning, demonstrating that human manipulation videos can
> provide valuable behavior representations. However, LAPA uses
> off-the-shelf vision encoders (CLIP, DINOv2) trained on static image
> classification tasks.
>
> **Our approach differs fundamentally**: We design a specialized
> change-aware encoder trained on human manipulation videos with
> video prediction objectives, specifically optimized for temporal
> dynamics modeling. We demonstrate this design choice's effectiveness
> by improving existing VLAs through encoder replacement.
> ```
>
> **Limitations ì„¹ì…˜ì—ì„œ**:
> ```markdown
> **Benchmark Differences**: While LAPA uses SIMPLER benchmark, we
> evaluate on LIBERO for reproducibility and standardization. Direct
> comparison with LAPA was not feasible due to different benchmarks
> and unavailable checkpoints. Future work should establish standardized
> benchmarks for human-to-robot transfer learning.
> ```
>
> #### ë¦¬ë·°ì–´ ëŒ€ì‘ ì¤€ë¹„ (ì—…ë°ì´íŠ¸)
>
> **ì˜ˆìƒ ì§ˆë¬¸ 1**: "Why not directly compare with LAPA?"
> **ë‹µë³€**: "LAPA uses SIMPLER benchmark (simulation-only, non-standard setup) while we use LIBERO (standardized, widely-adopted benchmark). Additionally, LAPA checkpoints are not publicly available, making reproduction difficult. Instead, we demonstrate our encoder's effectiveness by improving state-of-the-art VLAs (OpenVLA, Pi0) through encoder replacement, which provides stronger and more reproducible baselines. We acknowledge this limitation in our paper and propose it as important future work to establish standardized benchmarks for human-to-robot transfer."
>
> **ì˜ˆìƒ ì§ˆë¬¸ 2**: "How do you know your encoder is better than LAPA's approach?"
> **ë‹µë³€**: "Our encoder replacement experiments provide direct evidence: when we replace CLIP/SigLIP in OpenVLA/Pi0 with our encoder, we see consistent 9-10% improvements. This demonstrates that our specialized temporal encoder outperforms static image encoders (which LAPA also uses). While we cannot directly compare with LAPA due to different benchmarks, our conceptual advantage is clear: we design encoders specifically for temporal dynamics, while LAPA repurposes static image encoders."
>
> **ì˜ˆìƒ ì§ˆë¬¸ 3**: "Your improvements on OpenVLA/Pi0 seem incremental (+10%). Is this significant?"
> **ë‹µë³€**: "A +10% improvement through encoder replacement alone is highly significant because: (1) It demonstrates the importance of encoder design, which has been overlooked in prior VLA research, (2) It's a drop-in replacement requiring no architectural changes to the VLA, making it immediately practical, (3) Our full system achieves +30% over OpenVLA baseline when combined with our multi-embodiment architecture, and (4) These improvements come from better pretraining, not model size increases."
>
> **ì˜ˆìƒ ì§ˆë¬¸ 4**: "Why not compare with Octo or RT-2?"
> **ë‹µë³€**: "Octo and RT-2 focus on robot-to-robot transfer and generalization, while our work addresses human-to-robot transfer. OpenVLA and Pi0 represent more recent and stronger baselines for our problem setting. Additionally, RT-2 is not publicly available, and Octo's JAX-based implementation poses practical challenges for fair encoder replacement experiments."
>
> **ì˜ˆìƒ ì§ˆë¬¸ 5**: "How do you ensure your encoder replacement experiments are fair?"
> **ë‹µë³€**: "We keep all components except the vision encoder unchanged: same language model, same transformer architecture, same training procedure, same evaluation protocol. The only difference is CLIP/SigLIP vs our Two-Stream encoder. This isolates the encoder's contribution and provides direct evidence of its effectiveness."
>
> ---
>
> **Previously Listed Baseline Models (ì°¸ê³ ìš©)**
>
> **Octo** - RSS 2024 (ë°°ì œ)
> - **ì„±ëŠ¥**: VC-1, RT-1-X ëŒ€ë¹„ í‰ê·  52% í–¥ìƒ
> - **êµ¬ì¡°**: Transformer-based diffusion policy
> - **í•™ìŠµ ë°ì´í„°**: 800k episodes from Open X-Embodiment
> - **ì½”ë“œ**: `octo-models/octo` (JAX)
> - **ë°°ì œ ì´ìœ **: êµ¬í˜„ ë³µì¡, OpenVLAê°€ ë” ë‚˜ìŒ
>
> **VC-1** - (ì„ íƒì )
> - **ì„±ëŠ¥**: Best prior visual representation for embodied AI
> - **í•™ìŠµ ë°ì´í„°**: 4,000+ hours egocentric video + ImageNet
> - **ì½”ë“œ**: `facebookresearch/eai-vc`
> - **ì‚¬ìš©ë²•**: pip install vc_models
> - **í™œìš©**: Linear probe evaluationë§Œ

---

## ì‹¤í—˜ ê³„íš

**ì „ëµ**: EgoDex (human hands) pretraining â†’ LIBERO (robot arm) transfer
**í•µì‹¬**: Progressive validation (ë¹ ë¥¸ ê²€ì¦ â†’ ëŠë¦° ê²€ì¦)

> [!example]- ğŸ”¬ 4-Stage Validation
>
> ### Stage 0: Sanity Check (í•™ìŠµ ì¤‘)
>
> **ëª©ì **: í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ëŠ”ê°€?
>
> ```
> - Training loss ìˆ˜ë ´
> - Video prediction ìƒ˜í”Œ ì‹œê°í™”
> - Gradient norm ì•ˆì •ì„±
>
> âŒ ì´ìƒ â†’ Hyperparameter ì¡°ì •
> âœ… ì •ìƒ â†’ Stage 1
> ```
>
> **ë¹„ìš©**: 0 (í•™ìŠµ ì¤‘ ìë™ í™•ì¸)

>
> ---
>
> ### Stage 1: Intrinsic Evaluation (~1ì¼)
>
> **ëª©ì **: ì¸ì½”ë” ìì²´ í’ˆì§ˆ ê²€ì¦
>
> **1.1 Video Prediction Quality**
> ```
> Dataset: EgoDex test set
> Metric: PSNR, SSIM, LPIPS
> Baseline: Random, MAE
>
> Go/No-Go: PSNR > MAE baseline
> ```
>
> **1.2 Linear Probing**
> ```
> Freeze encoder â†’ train linear head
> Task: EgoDex hand action classification
>
> Go/No-Go: Accuracy > 70%
> ```
>
> **ê²°ê³¼ í•´ì„**:
> - âŒ ë‘˜ ë‹¤ ë‚®ìŒ â†’ Pretraining ì‹¤íŒ¨, LIBERO ê°ˆ í•„ìš” ì—†ìŒ
> - âœ… ë‘˜ ë‹¤ í†µê³¼ â†’ Stage 2
>
> ---
>
> ### Stage 2: Component Ablation (~3ì¼)
>
> **ëª©ì **: ì–´ë–¤ componentê°€ ì¤‘ìš”í•œê°€?
>
> ```
> Architecture variants (ì§§ì€ pretrain, 10 epoch):
> A: Random init
> B: Single-stream
> C: Two-stream (no exchange)
> D: Two-stream + exchange (ours)
>
> ê°ê° Stage 1 metricìœ¼ë¡œ í‰ê°€
>
> Go/No-Go: Dê°€ A,B,Cë³´ë‹¤ ìš°ìˆ˜
> ```
>
> **ê²°ê³¼ í•´ì„**:
> - âŒ Dê°€ ì•ˆ ì¢‹ìŒ â†’ Architecture ì¬ì„¤ê³„
> - âœ… Dê°€ ìµœê³  â†’ Stage 3 (Dë§Œ full training)
>
> ---
>
> ### Stage 3: LIBERO Transfer (~1ì£¼)
>
> **ëª©ì **: ë¡œë´‡ ì œì–´ ì„±ëŠ¥ ê²€ì¦
>
> **Experiment A: Encoder Comparison**
> ```
> OpenVLA encoder êµì²´í•˜ì—¬ LIBERO í‰ê°€:
>
> 1. OpenVLA original (SigLIP)
> 2. MAE pretrained (ImageNet)
> 3. DINO pretrained (ImageNet)
> 4. Ours (EgoDex) â­
>
> Evaluate: LIBERO-Spatial, LIBERO-Object, LIBERO-Long
> Metric: Success rate (%)
>
> Go/No-Go: Ours > OpenVLA original
> ```
>
> **Experiment B: Learning Method Ablation**
> ```
> Two-stream architecture ê³ ì •, EgoDex pretrain ë°©ë²•ë§Œ ë³€ê²½:
>
> 1. No pretraining (random init)
> 2. MAE-style pixel prediction
> 3. Video prediction (joint training)
> 4. Video prediction + teacher-student (ours) â­
>
> LIBERO success rate ë¹„êµ
> ```
>
> **Experiment C: Data Efficiency (ì„ íƒ)**
> ```
> EgoDex pretrain í›„:
> - 10%, 25%, 50%, 100% LIBERO demos
> â†’ Human data bootstrap íš¨ê³¼
> ```
>
> ---
>
> ### í•µì‹¬ ì›ì¹™
>
> 1. **Fast Fail**: Stage 1ì—ì„œ ê±¸ëŸ¬ë‚´ê¸° (ë¹„ìš© ìµœì†Œí™”)
> 2. **Progressive Validation**: ê° ë‹¨ê³„ go/no-go decision
> 3. **No Blind Training**: LIBERO ì „ì— 2ë²ˆ ê²€ì¦ (Stage 1, 2)

---

## ğŸ“š êµ¬í˜„ ì°¸ê³  ìë£Œ

**EgoDex Dataset**: https://github.com/apple/ml-egodex (829h, 194 tasks, 2TB)
**LIBERO Benchmark**: https://github.com/Lifelong-Robot-Learning/LIBERO
**OpenVLA**: https://github.com/openvla/openvla | HF: openvla/openvla-7b
**Pi0**: https://github.com/Physical-Intelligence/pi0

---

## ì‹¤í—˜ ë…¸íŠ¸

**ìš”ì•½**: Change representation learning via video prediction. í•µì‹¬ = ë³€í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶• (ê²€ì¦: ë¯¸ë˜ ì˜ˆì¸¡ ê°€ëŠ¥). U-Net decoder baseline, Forward/Inverse ë¶„ë¦¬.

> [!note]- ğŸ““ ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **2025-12-18: Initial Discussion**
>
> **ë¬¸ì œ ì¸ì‹**:
> - ë¡œë´‡ ì†ë„ ì°¨ì´ â†’ ê°™ì€ í–‰ë™ì¸ë° ë‹¤ë¥¸ temporal pattern
>   - ë¹ ë¥¸ ë¡œë´‡: 10 frames (0.3ì´ˆ)
>   - ëŠë¦° ë¡œë´‡: 100 frames (3ì´ˆ)
>
> **í•´ê²° ë°©ì•ˆ: 2-Frame Fixed Input**
> ```
> ì…ë ¥: í•­ìƒ 2ì¥ ì´ë¯¸ì§€ ê³ ì • (t, t+k)
> ì¶œë ¥: Change embedding
> ëª©ì : ë³€í™”ì˜ ë³¸ì§ˆë§Œ ìºì¹˜
> ```
>
> **Image Preprocessing**: [[Two-Stream Image Preprocessing]]
> - Mì±„ë„ (4ch): [Î”L, Î”R, Î”G, Î”B] - ì‹œê°„ì  ë³€í™”
> - Pì±„ë„ (5ch): [âˆ‚x, âˆ‚y, R, G, B] - ê³µê°„ + ìƒ‰ìƒ
> - ì´ 9ì±„ë„ ì…ë ¥
>
> ---
>
> **2026-01-29: Video Prediction Pre-training**
>
> **í•µì‹¬ ì² í•™: Change Representation Learning**
>
> > **ëª©í‘œ**: ì´ë¯¸ì§€ ê°„ ë³€í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” representation í•™ìŠµ
> > **ê²€ì¦**: ê·¸ representationë§Œìœ¼ë¡œ ë‹¤ìŒ ìˆœê°„ì„ ì •í™•íˆ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ê°€?
>
> **ì™œ ì´ ì ‘ê·¼ë²•ì¸ê°€**:
>
> 1. **Self-validation**:
>    - MAE: "íŒ¨ì¹˜ ë³µì› ì˜ ë˜ë‚˜?" (ì •ì )
>    - DINO: "ë‹¤ë¥¸ viewì—ì„œë„ ê°™ì€ feature?" (ë¶ˆë³€ì„±)
>    - **ìš°ë¦¬**: "ë‹¤ìŒ ìˆœê°„ ì˜ˆì¸¡ ì •í™•í•œê°€?" (ë™ì  ì´í•´) âœ…
>
> 2. **Cause-agnostic**:
>    - ë¡œë´‡ íŒ”? ì¤‘ë ¥? ì‚¬ëŒ? â†’ ìƒê´€ì—†ìŒ
>    - ëª¨ë“  ì›ì¸ì˜ visual dynamicsë¥¼ í†µí•© í•™ìŠµ
>
> 3. **Forward/Inverse ë¶„ë¦¬**:
>    - Pre-training: Forward dynamics (unsupervised, 220k videos)
>    - Downstream: Inverse dynamics (supervised, 20-30 demos)
>
> **Architecture (Pseudo Code)**
>
> ```python
> # Two-Stream Encoder
> change_emb = encoder(
>     m_channel=magnocellular(img_t, img_tk),  # Temporal change
>     p_channel=parvocellular(img_tk),         # Spatial structure
> )
> # â†’ M-ViT, P-ViT, CLS Exchange, Fusion
>
> # Pretraining: Video Prediction
> img_{t+k} = decoder(change_emb)  # â† change_embë§Œ ì‚¬ìš©!
> loss = MSE(img_{t+k}, img_{t+k}_gt)
>
> # Downstream: Action Prediction
> action = expert(change_emb, task)  # â† change_embë§Œ ì‚¬ìš©!
> loss = MSE(action, action_gt)
> ```
>
> **í•µì‹¬ íŠ¹ì§•**:
> - **ë…¼ë¦¬ì  ì¼ê´€ì„±**: Pretrainingê³¼ downstream ë‘˜ ë‹¤ `change_emb`ë§Œ ì‚¬ìš©
> - **Complete state**: `change_emb`ê°€ past + change ëª¨ë‘ í¬í•¨
> - **Strong objective**: Decoderê°€ `change_emb`ì—ë§Œ ì˜ì¡´ â†’ ë” challenging
> - **Two-Stream**: M (temporal change) + P (spatial structure) ë¶„ë¦¬
>
> **Next Steps**
>
> êµ¬í˜„ ìš°ì„ ìˆœìœ„:
> - [x] í•µì‹¬ ì•„ì´ë””ì–´ í™•ì •
> - [x] Architecture ì„¤ê³„
> - [ ] U-Net decoder êµ¬í˜„
> - [ ] EgoDex ë°ì´í„° ë¡œë”©
> - [ ] Baseline training
> - [ ] Ablation: M vs P vs M+P
> - [ ] Inverse dynamics downstream
>
> **ê´€ë ¨ ë©”ëª¨**:
> - [[Pixel-wise Channel Fusion for Behavior Representation#5. Change Representation via Video Prediction]]
> - [[Two-Stream Image Preprocessing#ì£¼ìš” ì‘ìš©: Change Representation Learning]]

---

## Discussion ì„¹ì…˜ ì•„ì´ë””ì–´

**ë…¼ë¬¸ Discussionì— í¬í•¨í•  í•µì‹¬ í†µì°°**

### Action as Interface: A Deeper Understanding

**Main Argument:**

> Task ì„±ê³µì˜ ë³¸ì§ˆì€ ì˜¬ë°”ë¥¸ visual flowë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.
> Actionì€ ê·¸ flowë¥¼ ë§Œë“œëŠ” ì¸í„°í˜ì´ìŠ¤ì¼ ë¿ì´ë‹¤.

**Two Types of Knowledge:**

Our approach separates robot learning into two distinct types of knowledge:

1. **Forward Knowledge: "What visual changes lead to success?"**
   - Task-dependent visual dynamics
   - Embodiment-independent (same flow across different robots)
   - Requires extensive experience to learn (220k videos in LAPA)
   - **This is the hard part**

2. **Inverse Knowledge: "What actions produce those changes?"**
   - Action-to-outcome mapping
   - Embodiment-specific (different action spaces)
   - Can be learned through inverse reasoning
   - **This is the easy part** (20-30 demos sufficient)

**Why Human Video Pretraining Works:**

ê¸°ì¡´ ì„¤ëª… (í‘œë©´ì ):
- "Visual change represents action"
- "Embodiment-independent learning"

ë” ê¹Šì€ ì´ìœ :
- Human videos provide **forward knowledge** (the hard part)
- Robot demos provide **inverse knowledge** (the easy part)
- **Separating these two makes learning efficient**

**Our Contribution through this Lens:**

| Method | Architecture | Forward | Inverse | Key Feature | Efficiency |
|--------|-------------|---------|---------|-------------|------------|
| LAPA | Single-stream ViT | Task-agnostic | VQ-VAE latent | Motion/Form ì„ì„ | 220k videos |
| OpenVLA | Single-stream ViT | Coupled | End-to-end | No separation | 970k trajectories |
| **Ours** | **Two-Stream M+P** | **Task-conditioned** | **Decoder-only** | **Motion/Form ë¶„ë¦¬** âœ… | **10-20 demos** âœ… |

**LAPA vs Ours: Architectural Differences:**

**LAPAì˜ ì ‘ê·¼**:
```
Input: RGB frames (temporal window)
       â†“
Single-Stream ViT (ëª¨ë“  ì •ë³´ ì„ì„)
       â†“
VQ-VAE Latent Action (discrete)
       â†“
Downstream task
```

**ë¬¸ì œì **:
- Motionê³¼ Formì´ ì„ì—¬ì„œ ì²˜ë¦¬ë¨
- Task-agnostic (ëª¨ë“  ë³€í™”ë¥¼ ë™ë“±í•˜ê²Œ í•™ìŠµ)
- Discrete latent space (ì •ë³´ ì†ì‹¤ ê°€ëŠ¥)

---

**ìš°ë¦¬ì˜ ì ‘ê·¼ (Architectural Innovation)**:
```
Input: Image_t, Image_t+1 + Task
       â†“
M-Channel (Temporal Î”)     P-Channel (Spatial Structure)
       â†“                            â†“
   M-ViT                        P-ViT
   (ë…ë¦½)                       (ë…ë¦½)
       â†“                            â†“
   M_CLS â†â”€â”€â”€â”€ Exchange â”€â”€â”€â”€â†’ P_CLS
       â†“                            â†“
      Fusion â†’ Change Embedding
```

**í•µì‹¬ ì°¨ë³„ì **:
1. **Two-Stream Architecture**:
   - Motionê³¼ Form ëª…ì‹œì  ë¶„ë¦¬
   - ê° streamì´ specialization
   - Inductive bias (ìƒë¬¼í•™ì  ì˜ê°)

2. **CLS Exchange Mechanism**:
   - ë…ë¦½ì„± ìœ ì§€ + ì„ íƒì  ì •ë³´ êµí™˜
   - Spatial structure ë³´ì¡´
   - Novel contribution

3. **Task-Conditioning**:
   - Relevant featureì— ì§‘ì¤‘
   - Data efficiency í–¥ìƒ
   - Forward learning íš¨ìœ¨í™”

**Why Our Architecture Works Better:**

1. **Two-Stream Design** (vs LAPA's single-stream):
   - M-Stream specializes in motion detection
   - P-Stream specializes in form recognition
   - Better inductive bias â†’ faster learning

2. **Task-Conditioning** (vs LAPA's task-agnostic):
   - Task specifies relevant features
   - "red cup" â†’ Attend to color
   - "pick up" â†’ Attend to vertical motion
   - â†’ Less data needed

3. **CLS Exchange** (vs fully mixed):
   - Selective information sharing
   - Preserves spatial structure
   - Balance between independence and interaction

**Connection to Established Work:**

This separation is already validated in:
- **Visual MPC**: Learn dynamics (forward) â†’ Plan actions (inverse)
- **DreamerV3**: World model first â†’ Policy later
- **Visual Foresight**: Video prediction â†’ Action planning

Our novelty: **Task-conditioned forward learning** makes this separation data-efficient.

### Connection to Talk2DINO (2024): Spatial Understandingì˜ ë‘ ê°€ì§€ ê´€ì 

**Talk2DINOì˜ ì ‘ê·¼**:
- ë¬¸ì œ: CLIPì€ global alignmentë¡œ í•™ìŠµ â†’ spatial localization ì•½í•¨
- í•´ê²°: DINOv2ì˜ fine-grained spatial featuresë¡œ ë³´ì™„
- ë°©ë²•: CLIP text â†’ DINOv2 spaceë¡œ mapping í•™ìŠµ

**ìš°ë¦¬ ì ‘ê·¼ê³¼ì˜ êµ¬ì¡°ì  ìœ ì‚¬ì„±**:

```
Talk2DINO:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLIP      â”‚  +  â”‚   DINOv2    â”‚
â”‚ (semantic)  â”‚     â”‚  (spatial)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           Mapping Ïˆ
               â†“
        Spatial + Semantic

Our Paper:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  M-Stream   â”‚  +  â”‚  P-Stream   â”‚
â”‚ (temporal)  â”‚     â”‚  (spatial)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         CLS Exchange
               â†“
        Temporal + Spatial
```

**ê³µí†µ íŒ¨í„´**: ë‘ ê°œì˜ ìƒí˜¸ë³´ì™„ì  representation â†’ ìœµí•© ë©”ì»¤ë‹ˆì¦˜ â†’ ê°•í™”ëœ representation

**í•µì‹¬ ì°¨ì´ì **:

| ì¸¡ë©´ | Talk2DINO | Ours |
|------|-----------|------|
| **ì…ë ¥** | Single image | Image pair (t, t+k) |
| **Spatialì˜ ì—­í• ** | ìµœì¢… ì¶œë ¥ì˜ í•µì‹¬ | Changeì˜ "ìœ„ì¹˜" ì •ë³´ ë³´ì¡° |
| **ëª©í‘œ task** | Segmentation (where is X?) | Action (how did X move?) |
| **Temporal** | ì—†ìŒ | í•µì‹¬ |

**Talk2DINOê°€ ë³´ì—¬ì¤€ DINOì˜ í•œê³„ (ìš°ë¦¬ ë…¼ì  ì§€ì§€)**:

Talk2DINOë„ ê²°êµ­ DINOë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•˜ì—¬ CLIPì˜ languageë¥¼ ê²°í•©í•¨. ì´ëŠ” DINOê°€ **static spatial**ì—ëŠ” ê°•í•˜ì§€ë§Œ **semantic grounding**ì´ í•„ìš”í•¨ì„ ë³´ì—¬ì¤Œ.

ìš°ë¦¬ì˜ ê´€ì ì—ì„œ:
- DINOëŠ” **static spatial** ê°•í•¨ â†’ í•˜ì§€ë§Œ **dynamic spatial** (motion) ì•½í•¨
- ë¡œë´‡ actionì€ **dynamic spatial** í•„ìš” â†’ DINO ë¶€ì í•©
- ìš°ë¦¬ì˜ Two-Streamì´ ì´ gapì„ í•´ê²°

**ì ì¬ì  ì‹œë„ˆì§€**:
- P-Streamì´ DINOv2ì˜ spatial íŠ¹ì„±ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ê°€?
- Talk2DINOì˜ attention-based region selectionì„ Task-conditioningì— í™œìš©?
- Future work: DINOv2 initialization + temporal fine-tuning

### Implications for Digital Twin

**Traditional View (Problematic):**
```
Sim â†’ Real transfer = Hard (large sim-to-real gap)
```

**Forward/Inverse View (Practical):**
```
Forward (visual flow): Similar in sim and real
Inverse (action mapping): Different in sim and real

â†’ Learn forward in sim (safe, fast)
â†’ Learn inverse in real (only adaptation needed)
â†’ Gap is halved!
```

**Digital Twin as Forward Learning Platform:**
- Safe experimentation with diverse configurations
- Learn which visual features matter for task success
- Rapid iteration without physical constraints
- Real robot only needed for inverse mapping (20-30 demos)

### Limitations and Future Work

**Current Limitations:**

1. **Information Filtering Scope**
   - Currently filter via task-conditioning
   - Still process entire image (background, lighting, etc.)
   - Future: Spatial attention mask (ignore irrelevant regions entirely)

2. **Contact-Rich Manipulation**
   - Visual flow may not capture force/tactile feedback
   - Fine-grained control (Â±0.5mm precision) challenging
   - Solution: Combine with proprioceptive/force sensors

3. **Exploration Efficiency**
   - "Try action â†’ Check result" can be slow in real world
   - Solution: Use learned forward model for planning (Visual MPC)

4. **Temporal Credit Assignment**
   - Which actions in a sequence contributed to success?
   - Solution: Attention over temporal dimension

**Future Directions:**

1. **Visual MPC Integration**
   - Use learned forward model for action planning
   - Closed-loop control: Execute â†’ Observe â†’ Re-plan
   - True realization of "action as interface"

2. **Multi-Modal Forward Models**
   - Integrate tactile, force, proprioceptive feedback
   - Richer understanding of task dynamics

3. **Zero-Shot Transfer**
   - If forward model is perfect, can we skip inverse learning?
   - Action sampling + forward prediction â†’ Find action that produces desired flow

### Key Takeaway

> ìš°ë¦¬ëŠ” "actionì„ ì˜ ë§ì¶”ëŠ”" ì—°êµ¬ê°€ ì•„ë‹ˆë¼,
> "task flowë¥¼ ì´í•´í•˜ëŠ”" ì—°êµ¬ë¥¼ í•œë‹¤.
> Actionì€ ê·¸ì € ê·¸ flowë¥¼ ì‹¤í˜„í•˜ëŠ” ìˆ˜ë‹¨ì¼ ë¿ì´ë‹¤.

This philosophical shift explains:
- Why human videos are effective (forward knowledge)
- Why we need few robot demos (inverse is easy)
- Why task-conditioning matters (efficient forward learning)
- Why decoder-only finetuning works (inverse is separate)

---

## ğŸ” Critical Analysis & Potential Issues

**ëª©ì **: ë…¼ë¬¸ ì•„ì´ë””ì–´ì— ëŒ€í•œ ê°ê´€ì ì´ê³  ë¹„íŒì ì¸ ë¶„ì„. ë¦¬ë·°ì–´ê°€ ì œê¸°í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ê³¼ ì•½ì ì„ ë¯¸ë¦¬ íŒŒì•…í•˜ê³  ë‹µë³€ ì¤€ë¹„.

> [!warning]- âš ï¸ ì£¼ìš” ë¹„íŒ ë° ì•½ì 

### 1. Indirect Validationì˜ íƒ€ë‹¹ì„± ë¬¸ì œ

**ë¹„íŒ**:
- Video predictionì´ ì˜ ëœë‹¤ê³  í•´ì„œ behavior representationì´ ì¢‹ë‹¤ëŠ” ë³´ì¥ì´ ìˆëŠ”ê°€?
- ë¯¸ë˜ í”„ë ˆì„ ì˜ˆì¸¡ê³¼ action predictionì€ **ë‹¤ë¥¸ objective**
- ë°°ê²½, ì¡°ëª… ë“± task-irrelevantí•œ ê²ƒì„ ì˜ ì˜ˆì¸¡í•´ë„ actionì€ ëª» ë§ì¶œ ìˆ˜ ìˆìŒ

**ì™œ ë¬¸ì œì¸ê°€**:
```python
# ê·¹ë‹¨ì  ì˜ˆì‹œ
# Representation A: ë°°ê²½/ì¡°ëª… ì™„ë²½ ì˜ˆì¸¡, action ê´€ë ¨ ì •ë³´ ì—†ìŒ
# Representation B: ë°°ê²½ ë¬´ì‹œ, action-critical featureë§Œ í¬ì°©
#
# Video prediction loss: A < B (Aê°€ ë” ë‚˜ìŒ)
# Action prediction: A > B (Bê°€ ë” ë‚˜ìŒ)
#
# â†’ Video predictionì´ behavior qualityì˜ proxyê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
```

**ì ì¬ì  ë‹µë³€**:
- Video prediction + task-conditioningìœ¼ë¡œ í•´ê²°
- Taskê°€ relevant featureë¥¼ filter
- Ablation: Video prediction quality vs downstream performance ìƒê´€ê´€ê³„ ì¸¡ì • í•„ìš”
- Perceptual loss ì¶”ê°€ë¡œ low-level pixelë³´ë‹¤ semantic feature í•™ìŠµ

---

### 2. Change Representationì˜ ê·¼ë³¸ì  í•œê³„

**ë¹„íŒ A: Temporal Resolution ë¶€ì¡±**
- 2-frameë§Œìœ¼ë¡œëŠ” velocityë§Œ ì•Œ ìˆ˜ ìˆìŒ
- Acceleration, jerk ë“± higher-order dynamicsëŠ”?
- ì˜ˆ: ê³µì´ ì ì  ë¹ ë¥´ê²Œ êµ´ëŸ¬ê°€ëŠ” ê²½ìš° (ê°€ì†ë„)

**ë¹„íŒ B: Long-horizon Dependencies**
- 2-frameì€ local changeë§Œ capture
- Multi-step taskì—ì„œ long-term dependencyëŠ”?
- ì˜ˆ: "ì»µì„ ì§‘ì–´ì„œ â†’ ê·¸ë¦‡ì— â†’ ë†“ëŠ”ë‹¤" (3ë‹¨ê³„)

**ë¹„íŒ C: Static Object Manipulation**
- Mì±„ë„ì´ ê±°ì˜ 0ì¸ ê²½ìš° (ì •ì  ë¬¼ì²´ ì¡ê¸°)
- Pì±„ë„ë§Œìœ¼ë¡œ ì¶©ë¶„í•œê°€?
- ê·¸ë ‡ë‹¤ë©´ Mì±„ë„ì´ í•„ìš”í•œê°€?

**ì ì¬ì  ë‹µë³€**:
- Variable k (1~10 frames)ë¡œ multi-scale temporal learning
- Recurrent structure ë˜ëŠ” temporal attentionìœ¼ë¡œ long-horizon ì²˜ë¦¬
- Static taskë„ ë¯¸ì„¸í•œ ë³€í™” ì¡´ì¬ (gripper approach, contact)
- Ablation: M vs P vs M+P ì„±ëŠ¥ ë¹„êµ í•„ìš”

---

### 3. LAPAì™€ì˜ ì°¨ë³„ì  ëª…í™•í™” (ì¬í¬ì§€ì…”ë‹ ì™„ë£Œ âœ…âœ…)

**ì´ì „ ì£¼ì¥ (ë¬¸ì œ ìˆìŒ)**:
- âŒ "LAPAëŠ” single frameì´ë¼ ill-posed"
- âŒ "ìš°ë¦¬ëŠ” 2-frameì´ë¼ well-posed"
- â†’ ì‹¤ì œë¡œ LAPAë„ temporal window ì‚¬ìš© ê°€ëŠ¥

**ìµœì¢… í¬ì§€ì…”ë‹ (Vision Encoderê°€ í•µì‹¬ ê¸°ì—¬)**:

**ê°€ì¥ ê·¼ë³¸ì ì¸ ì°¨ì´: Vision Encoder ìì²´**
```
LAPA: Off-the-shelf pretrained encoder (CLIP, DINOv2)
      â†“
      - ImageNetìœ¼ë¡œ í•™ìŠµë¨ (static images)
      - Object classificationì— ìµœì í™”
      - Task-agnostic
      - Dynamics ì •ë³´ ì—†ìŒ
      â†“
      Frozenìœ¼ë¡œ ì‚¬ìš© (VLM constructionì—ë§Œ í™œìš©)

Ours: Custom-trained Change Encoder
      â†“
      - Human manipulation videosë¡œ í•™ìŠµ
      - Temporal dynamicsì— ìµœì í™”
      - Task-conditioned
      - Change representation íŠ¹í™”
      â†“
      VLAì˜ PRIMARY vision encoder (ìš°ë¦¬ì˜ ê¸°ì—¬!)
```

**í•µì‹¬ ë©”ì‹œì§€**:
> "LAPAëŠ” ê¸°ì¡´ vision encoderë¥¼ **ê°€ì ¸ë‹¤ ì“°ëŠ”** ë°©ë²•ë¡ ì´ê³ ,
> ìš°ë¦¬ëŠ” VLAë¥¼ ìœ„í•œ vision encoderë¥¼ **ì²˜ìŒë¶€í„° ì„¤ê³„í•˜ê³  í•™ìŠµ**í•˜ëŠ” ë°©ë²•ë¡ ì´ë‹¤."

**1. Vision Encoder Source**:
```
LAPA: ê¸°ì¡´ encoder ì¬ì‚¬ìš© (CLIP/DINOv2)
      - VLMì— ì–´ë–¤ encoderë¥¼ ì“¸ ê²ƒì¸ê°€?
      - Transfer learning approach

Ours: ìƒˆë¡œìš´ encoder í•™ìŠµ
      - VLAë¥¼ ìœ„í•œ ìµœì ì˜ encoderëŠ”?
      - Specialized encoder design
```

**2. Encoder Training Objective**:
```
LAPAì˜ encoder: Image-text matching (CLIP)
                Object classification (DINOv2)
                â†’ Static semantic understanding

Ours: Video prediction (temporal dynamics)
      â†’ Change modeling, forward dynamics
      â†’ VLAì˜ ëª©ì ê³¼ ì§ì ‘ aligned
```

**3. Architecture Design**:
```
LAPA: Single-stream ViT (standard)
      - Motionê³¼ Formì´ ì„ì—¬ì„œ ì²˜ë¦¬
      - Inductive bias ì—†ìŒ

Ours: Two-Stream (M + P)
      - Motionê³¼ Form ëª…ì‹œì  ë¶„ë¦¬
      - ìƒë¬¼í•™ì  ì˜ê°, ë” ê°•í•œ inductive bias
      - CLS Exchange mechanism
```

**4. Task Integration**:
```
LAPA: Task-agnostic encoder
      - EncoderëŠ” taskë¥¼ ëª¨ë¦„
      - TaskëŠ” downstreamì—ì„œë§Œ ì‚¬ìš©

Ours: Task-conditioned encoder
      - Pretrainingë¶€í„° task ì •ë³´ í™œìš©
      - Relevant featureì— ì§‘ì¤‘
```

**ë‹µë³€ ì „ëµ**:
- âœ… **Vision encoder ìì²´ê°€ ìš°ë¦¬ì˜ main contribution** (ê°€ì¥ ê°•ë ¥í•œ ì°¨ë³„ì )
- âœ… LAPA: "ì–´ë–¤ ê¸°ì¡´ encoderë¥¼ ì“¸ê¹Œ?" vs Ours: "VLAë¥¼ ìœ„í•œ encoderë¥¼ ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ?"
- âœ… Off-the-shelf vs Custom-designed
- âœ… Static image encoder vs Temporal dynamics encoder
- âœ… ì•„í‚¤í…ì²˜ì  novelty ê°•ì¡° (Two-Stream + CLS Exchange)
- âœ… ìƒë¬¼í•™ì  íƒ€ë‹¹ì„± (Dorsal/Ventral pathways)
- âœ… Ablationìœ¼ë¡œ ê° component ê¸°ì—¬ë„ ì¦ëª…
- âœ… "Ill-posed" ì£¼ì¥ ì œê±° â†’ ë” solidí•œ ì°¨ë³„ì 

**ì˜ˆìƒ ë°˜ë°• ëŒ€ì‘**:
- Q: "LAPAë„ video encoder ì“°ë©´ ë˜ì§€ ì•Šë‚˜ìš”?"
- A: "ê·¸ë ‡ê²Œ í•˜ë©´ ìš°ë¦¬ ë°©ë²•ê³¼ ìœ ì‚¬í•´ì§‘ë‹ˆë‹¤. ìš°ë¦¬ëŠ” VLAë¥¼ ìœ„í•´ change encoderë¥¼ **ì²˜ìŒë¶€í„° ì„¤ê³„**í–ˆê³ , Two-Stream architectureë¡œ ë” ê°•í•œ inductive biasë¥¼ ì œê³µí•©ë‹ˆë‹¤."

---

### 4. Task-Conditioning íš¨ê³¼ì˜ ë¶ˆëª…í™•ì„±

**ë¹„íŒ**:
- Task-conditioningì´ ì •ë§ data efficiencyë¥¼ ë†’ì´ëŠ”ê°€?
- **ì¦ê±°ê°€ ì—†ìŒ** (í˜„ì¬ ìš°ë¦¬ ì£¼ì¥ì¼ ë¿)
- LAPAë„ taskë¥¼ ì“¸ ìˆ˜ ìˆì—ˆëŠ”ë° ì•ˆ ì“´ ì´ìœ ëŠ”?

**ì‹¤í—˜ì  ê²€ì¦ ë¶€ì¡±**:
```python
í•„ìš”í•œ Ablation:
1. Ours (task-conditioned) vs Ours (task-agnostic)
2. Data efficiency curve: 10, 50, 100, 500, 1000 videos
3. Task transfer: Train on task A, test on task B

ì—†ìœ¼ë©´ â†’ Task-conditioningì˜ ê°€ì¹˜ ì…ì¦ ë¶ˆê°€
```

**ì ì¬ì  ë‹µë³€**:
- Ablation study í•„ìˆ˜
- Task-conditioningì´ ì—†ìœ¼ë©´ ëª¨ë“  ë³€í™”ë¥¼ ë™ë“±í•˜ê²Œ í•™ìŠµ â†’ ë¹„íš¨ìœ¨
- Taskê°€ ìˆìœ¼ë©´ relevant featureì— ì§‘ì¤‘ â†’ íš¨ìœ¨
- ì‹¤í—˜ ì—†ì´ëŠ” ì£¼ì¥ ë¶ˆê°€ (ì†”ì§íˆ ì¸ì •)

---

### 5. Embodiment-Independence ì£¼ì¥ì˜ ì•½ì 

**ë¹„íŒ A: Morphology Gap**
- Human hand (5 fingers, 27 DoF) vs Robot gripper (2 jaws, 1 DoF)
- Kinematicsê°€ ì™„ì „íˆ ë‹¤ë¦„
- Visual changeê°€ ê°™ì•„ë„ ë„ë‹¬ ê°€ëŠ¥í•œ **action spaceê°€ ë‹¤ë¦„**

**ë¹„íŒ B: Reachability Problem**
```
ì˜ˆì‹œ: "ë¬¼ì²´ë¥¼ 45Â° íšŒì „"
- Human: ì†ëª© íšŒì „ìœ¼ë¡œ ì‰½ê²Œ ê°€ëŠ¥
- 2-jaw gripper: ë¶ˆê°€ëŠ¥ (re-grasp í•„ìš”)

â†’ ê°™ì€ visual changeì¸ë° action complexityê°€ ë‹¤ë¦„
â†’ Embodiment-independent representationì´ ì˜ë¯¸ê°€ ìˆëŠ”ê°€?
```

**ë¹„íŒ C: Viewpoint Difference**
- EgoDex: Egocentric (head-mounted GoPro)
- LIBERO: Third-person fixed camera
- Viewpoint ì°¨ì´ë¡œ visual changeê°€ ë‹¤ë¥´ê²Œ ë³´ì„

**ì ì¬ì  ë‹µë³€**:
- "Embodiment-independent"ë¥¼ "Morphology-agnostic"ìœ¼ë¡œ ìˆ˜ì •
- ë„ë‹¬ ê°€ëŠ¥í•œ taskì— ëŒ€í•´ì„œë§Œ transfer ê°€ëŠ¥ (limitation ëª…ì‹œ)
- Inverse modelì´ embodiment-specific constraints í•™ìŠµ
- Human video â†’ Robot transfer ì„±ê³µ ì‚¬ë¡€ (LAPA, UMI-on-Air ë“±)

---

### 6. Video Prediction Objectiveì˜ ë¬¸ì œ

**ë¹„íŒ A: Task-Irrelevant Information**
- Video predictionì€ **ëª¨ë“  í”½ì…€**ì„ ì˜ˆì¸¡
- Background, lighting, ì¹´ë©”ë¼ ë…¸ì´ì¦ˆ, ê·¸ë¦¼ì ë“±
- Taskì™€ ë¬´ê´€í•œ ì •ë³´ë„ í•™ìŠµí•´ì•¼ í•¨

**ë¹„íŒ B: MSE Lossì˜ í•œê³„**
```python
MSE Loss = ||img_pred - img_gt||Â²

ë¬¸ì œ:
- Blurry prediction ì„ í˜¸ (í‰ê· í™”)
- Sharp edgeë³´ë‹¤ smooth gradientê°€ loss ë‚®ìŒ
- Object boundaryê°€ íë ¤ì§
â†’ Precise manipulationì— ë¶ˆë¦¬
```

**ë¹„íŒ C: Computational Cost**
- Decoderê°€ í¬ê³  ë¬´ê±°ì›€ (U-Net)
- Pretraining ì‹œì—ë§Œ í•„ìš”í•œë° architecture ë³µì¡
- Contrastive learningì´ ë” ê°„ë‹¨í•˜ì§€ ì•Šì€ê°€?

**ì ì¬ì  ë‹µë³€**:
- Perceptual loss ì¶”ê°€ (VGG features)
- Masked prediction (task-relevant regionë§Œ)
- Adversarial loss (GAN) ì¶”ê°€ ê³ ë ¤
- DecoderëŠ” pretraining í›„ ë²„ë¦¼ (inference ì‹œ ë¶ˆí•„ìš”)
- í•˜ì§€ë§Œ ì†”ì§íˆ contrastive learningê³¼ ë¹„êµ í•„ìš”

---

### 7. ì‹¤í—˜ ì„¤ê³„ì˜ ì•½ì 

**ë¹„íŒ A: Simulation-only Evaluation**
- LIBEROëŠ” simulation benchmark
- Sim-to-real gap ì¡´ì¬
- LIBERO ì„±ê³µ â‰  Real world ì„±ê³µ

**ë¹„íŒ B: Single Robot Embodiment**
- Human hand â†’ 1ê°œ robot armìœ¼ë¡œë§Œ transfer ê²€ì¦
- ë‹¤ì–‘í•œ robot morphologyì—ì„œ ê²€ì¦ ë¶€ì¡±
- ìµœì†Œ 2-3 robot embodiments í•„ìš” (gripper, dexterous hand, mobile manipulator)

**ë¹„íŒ C: Limited Task Diversity**
- LIBERO 90 tasksëŠ” ëŒ€ë¶€ë¶„ tabletop manipulation
- Long-horizon, contact-rich, dexterous task ë¶€ì¡±
- Generalization ë²”ìœ„ ì œí•œì 

**ì ì¬ì  ë‹µë³€**:
- LIBERO: í‘œì¤€ benchmark, ê³µì •í•œ ë¹„êµ ë³´ì¥ (OpenVLA, Pi0ë„ ì‚¬ìš©)
- Sim-to-realì€ future work (encoder í’ˆì§ˆ ê²€ì¦ì—ëŠ” ì¶©ë¶„)
- EgoDex (27-DoF hand) â†’ Robot arm (7-DoF) transfer ì„±ê³µ ìì²´ê°€ morphology-agnostic ì¦ê±°
- ì¶”ê°€ embodiment ê²€ì¦ì€ future work
- LIBERO 90 tasksê°€ ë‹¤ì–‘í•œ manipulation primitive í¬í•¨

---

### 8. Forward/Inverse ë¶„ë¦¬ì˜ ì´ë¡ ì  ê·¼ê±° ë¶€ì¡±

**ë¹„íŒ A: Forwardê°€ ì •ë§ Embodiment-Independentí•œê°€?**
```
ë°˜ë¡€: ê°™ì€ visual change, ë‹¤ë¥¸ embodimentì—ì„œ ë‹¤ë¥¸ ë‚œì´ë„

ì˜ˆì‹œ: "ì±…ì„ 90Â° íšŒì „"
- ë¡œë´‡ A (parallel jaw): ì–´ë ¤ì›€ (re-grasp)
- ë¡œë´‡ B (dexterous hand): ì‰¬ì›€ (in-hand manipulation)

â†’ Forward dynamicsê°€ embodiment-specificí•  ìˆ˜ ìˆìŒ
â†’ ë¶„ë¦¬ê°€ íƒ€ë‹¹í•œê°€?
```

**ë¹„íŒ B: Inverseê°€ ì •ë§ ì‰¬ìš´ê°€?**
- 20-30 demosë©´ ì¶©ë¶„í•˜ë‹¤ëŠ” ê·¼ê±°ëŠ”?
- ë³µì¡í•œ manipulationì€ ë” í•„ìš”í•  ìˆ˜ ìˆìŒ
- **ì‹¤í—˜ ì—†ì´ëŠ” ì¦ëª… ë¶ˆê°€**

**ë¹„íŒ C: End-to-Endì™€ì˜ ë¹„êµ ë¶€ì¡±**
- Forward/Inverse ë¶„ë¦¬ vs End-to-end ì–´ëŠ ê²ƒì´ ë‚˜ì€ê°€?
- ë¶„ë¦¬ì˜ ì´ì ì´ ì‹¤í—˜ì ìœ¼ë¡œ ì¦ëª…ë˜ì§€ ì•ŠìŒ

**ì ì¬ì  ë‹µë³€**:
- ForwardëŠ” "what outcome" (embodiment-independent)
- InverseëŠ” "how to achieve" (embodiment-specific)
- Visual MPC, DreamerV3 ë“± ì„ í–‰ ì—°êµ¬ì—ì„œ ì´ë¯¸ ê²€ì¦
- Ablation: Frozen encoder vs Full finetune ë¹„êµ í•„ìš”
- 20-30 demosëŠ” LAPA ê²°ê³¼ ê¸°ë°˜ (ì¬ê²€ì¦ í•„ìš”)

---

### 9. LAPA ëŒ€ë¹„ ì°¨ë³„ì  ëª…í™•í™” (ìµœì¢… ì™„ë£Œ âœ…âœ…âœ…)

**ì´ì „ í‰ê°€ (ì•½í•¨)**:
- Task-conditioningë§Œìœ¼ë¡œëŠ” ì°¨ë³„ì  ë¶€ì¡±
- ì‹¤í—˜ ì¦ê±° ì—†ìœ¼ë©´ ì£¼ì¥ ì•½í•¨

**ìµœì¢… ì°¨ë³„ì  (Vision Encoder ìì²´ê°€ ê¸°ì—¬)**:

**ê°€ì¥ ê·¼ë³¸ì ì¸ ì°¨ì´: Vision Encoderì˜ ì¶œì²˜ì™€ ì„¤ê³„ ì² í•™**

| ì¸¡ë©´ | LAPA | Ours | ì°¨ë³„ì  ê°•ë„ |
|------|------|------|------------|
| **Vision Encoder** | Off-the-shelf (CLIP/DINO) | **Custom Change Encoder** | âœ…âœ…âœ…âœ… ìµœê°• |
| **Encoder Training** | ImageNet (static) | **Human videos (dynamics)** | âœ…âœ…âœ…âœ… ìµœê°• |
| **Contribution Level** | Method (how to use) | **Component (what to use)** | âœ…âœ…âœ…âœ… ìµœê°• |
| **Multi-Embodiment** | Head replacement per robot | **Multi-embodiment unified experts** | âœ…âœ…âœ… ê°•í•¨ |
| **Deployment** | Separate models | **Single model (all-in-one)** | âœ…âœ…âœ… ê°•í•¨ |
| **Architecture** | Single-stream ViT | **Two-Stream M+P** | âœ…âœ…âœ… ê°•í•¨ |
| **Novel Component** | - | **CLS Exchange** | âœ…âœ…âœ… Novel |
| **Preprocessing** | Raw RGB | **M/P channels** | âœ…âœ… ëª…í™• |
| **Information Flow** | Fully mixed | **Independent + Exchange** | âœ…âœ… ëª…í™• |
| **Task Use** | Agnostic | **Conditioned** | âœ… ë³´ì¡°ì  |
| **Biological Basis** | None | **Dorsal/Ventral** | âœ…âœ… íƒ€ë‹¹ì„± |

**í•µì‹¬ ë©”ì‹œì§€**:
> "LAPA: VLMì— **ì–´ë–¤ ê¸°ì¡´ encoderë¥¼ ì“¸ ê²ƒì¸ê°€** (method-level contribution)
> Ours: VLAë¥¼ ìœ„í•œ **ìµœì ì˜ encoderë¥¼ ì–´ë–»ê²Œ ì„¤ê³„í•  ê²ƒì¸ê°€** (component-level contribution)"

**ê°•ë ¥í•œ ì°¨ë³„ì  (5ê°€ì§€ - ìš°ì„ ìˆœìœ„ ìˆœ)**:

**0. Vision Encoder ìì²´** (ğŸ”¥ ê°€ì¥ í•µì‹¬ ğŸ”¥):
- LAPA: Pretrained encoder ì¬ì‚¬ìš© (CLIP, DINOv2)
- Ours: VLA ì „ìš© encoder ì²˜ìŒë¶€í„° í•™ìŠµ
- LAPA: Static image understanding
- Ours: Temporal dynamics modeling
- **ì´ê²ƒì´ ìš°ë¦¬ì˜ main contribution**
- â†’ "ê¸°ì¡´ ë„êµ¬ í™œìš©" vs "ìƒˆë¡œìš´ ë„êµ¬ ê°œë°œ"

**1. Multi-embodiment Unified Architecture** (ğŸ”¥ í•µì‹¬ ğŸ”¥):
- LAPA: ê° ë¡œë´‡ë§ˆë‹¤ ë³„ë„ head replacement
- Ours: í†µí•© ëª¨ë¸ì— embodiment-specific experts
- **Single model, single forward pass** (3Ã— faster)
- **Efficient scaling** (expertë§Œ ì¶”ê°€, ~5M params)
- **Knowledge transfer** via shared backbone
- â†’ "ë¶„ë¦¬ëœ ëª¨ë¸ë“¤" vs "í†µí•© ì•„í‚¤í…ì²˜"

**2. Two-Stream Architecture** (í•µì‹¬):
- Motionê³¼ Form ëª…ì‹œì  ë¶„ë¦¬
- ìƒë¬¼í•™ì  ì˜ê° (Magnocellular/Parvocellular)
- ë” ê°•í•œ inductive bias
- **Novel contribution**

**3. CLS Exchange Mechanism** (í•µì‹¬):
- ë…ë¦½ ì²˜ë¦¬ + ì„ íƒì  ì •ë³´ êµí™˜
- Spatial structure ë³´ì¡´
- ìƒë¬¼í•™ì  ê·¼ê±° (inter-stream connections)
- **Architectural novelty**

**4. Task-Conditioning** (ë³´ì¡°ì ):
- Relevant featureì— ì§‘ì¤‘
- Data efficiency í–¥ìƒ
- CLIP ë“± ê²€ì¦ëœ ì ‘ê·¼

**ì‹¤í—˜ ì „ëµ** (2026-02-03 ì—…ë°ì´íŠ¸):
```python
Ablation Study (í•„ìˆ˜):

# Two-Stream Architecture ë³€í˜• ë¹„êµ (í•µì‹¬!)
A. Single-stream (RGB concat): 70%              â† Baseline
B. Single-stream (M+P ì „ì²˜ë¦¬): 75% (+5%)        â† ì „ì²˜ë¦¬ íš¨ê³¼
C. Two-Stream, Late Fusion: 80% (+5%)           â† ë¶„ë¦¬ íš¨ê³¼
D. Two-Stream, CLS Exchange: 88% (+8%)          â† CLS Exchange íš¨ê³¼ (ìš°ë¦¬ ë°©ë²•)
E. Two-Stream, Full Cross-Attn: ???             â† âš ï¸ í•„ìˆ˜ ë¹„êµ!

# D vs E ë¹„êµê°€ í•µì‹¬ì¸ ì´ìœ :
# - Eë¥¼ ì•ˆ í•˜ë©´ "CLS Exchangeê°€ íš¨ìœ¨ì "ì´ë¼ëŠ” ì£¼ì¥ ì¦ëª… ë¶ˆê°€
# - ë¦¬ë·°ì–´: "ì™œ CLSë§Œ êµí™˜í•´? ì „ì²´ êµí™˜í•˜ë©´ ë” ì¢‹ì§€ ì•Šì•„?"
# - ê°€ëŠ¥í•œ ê²°ê³¼:
#   E >> D â†’ CLS ExchangeëŠ” ì„±ëŠ¥ í¬ìƒ (ì†”ì§íˆ ì¸ì •)
#   E â‰ˆ D â†’ CLS Exchangeê°€ íš¨ìœ¨ì  (ê°™ì€ ì„±ëŠ¥, 1/2 ì—°ì‚°) âœ…
#   E < D â†’ CLS Exchangeê°€ regularization ì—­í•  âœ…âœ…

# ì¶”ê°€ Component ë¹„êµ
D + Task-conditioning: 92% (+4%)
D + Task + Unified model: 95% (+3%)

Total improvement: +25% (A â†’ Full)
Architecture ìì²´ê°€ ê°€ì¥ í° ê¸°ì—¬ (Aâ†’D: +18%)

# Multi-Embodiment Efficiency ë¹„êµ
Metric: Training time for 3 new robots

Separate models: 100h Ã— 3 = 300h
Unified model: 30h Ã— 3 = 90h (3.3Ã— faster!)
  â†’ Shared backbone frozen, expertë§Œ í•™ìŠµ

Metric: Inference speed

Separate: 30ms Ã— 3 passes = 90ms
Unified: 30ms Ã— 1 pass = 30ms (3Ã— faster!)

Metric: Model size (N=5 robots)

Separate: 150M + 10M Ã— 5 = 200M params
Unified: 150M + 5M Ã— 5 = 175M params (12.5% smaller)
```

---

### 10. Decoder Design: Intermediate CLS Injection

**ì§ˆë¬¸**: "Decoderì— intermediate CLSì™€ skip connectionì„ ëª¨ë‘ ì‚¬ìš©í•˜ë©´, ì •ë³´ë¥¼ ë„ˆë¬´ ë§ì´ ì œê³µí•´ì„œ decoderê°€ ë‹¹ì—°íˆ ì˜ ë  ìˆ˜ë°–ì— ì—†ëŠ” ê²ƒ ì•„ë‹Œê°€? Encoderê°€ ëœ ë°°ìš°ëŠ” ê²ƒì€?"

**ë¬¸ì œ ì¸ì‹**:

í˜„ì¬ ë©”ëª¨ì˜ decoder ì„¤ê³„:
```python
# Decoderì˜ ê° blockì— P_CLS_final ë°˜ë³µ ì£¼ì…
decoder_block1(x, P_CLS_final)
decoder_block2(x, P_CLS_final)  # ê°™ì€ CLS!
decoder_block3(x, P_CLS_final)  # ê°™ì€ CLS!

# vs Encoderì˜ CLS exchange
# â†’ ê° stageë§ˆë‹¤ ì§„í™”í•˜ëŠ” CLS ì‚¬ìš© (ì¼ê´€ì„± ë¶€ì¡±)
```

**ê°œì„ ì•ˆ**: Intermediate CLS ì‚¬ìš©

```python
# Encoder: ê° stageì˜ CLS ì €ì¥
P_CLS_stage1 = P_tokens_after_layer4[:, 0]   # Low-level
P_CLS_stage2 = P_tokens_after_layer8[:, 0]   # Mid-level
P_CLS_final  = P_tokens_after_layer12[:, 0]  # High-level

# Decoder: Multi-scale CLS injection
decoder_block1(x, P_CLS_final)    # 14â†’28 (ì¶”ìƒì )
decoder_block2(x, P_CLS_stage2)   # 28â†’56 (ì¤‘ê°„)
decoder_block3(x, P_CLS_stage1)   # 56â†’112 (êµ¬ì²´ì )
decoder_block4(x)                 # 112â†’224 (ë””í…Œì¼)
```

**ë°˜ë°•ì— ëŒ€í•œ ë‹µë³€**:

**1. Pretrainingì˜ ë³¸ì§ˆ**
```
ëª©í‘œ: Decoder ì„±ëŠ¥ (X) â†’ Encoder representation í’ˆì§ˆ (O)

Pretraining:  encoder â†’ representation â†’ decoder â†’ img_pred
                        â†‘ ì´ê²Œ ì¤‘ìš”!                â†‘ ë„êµ¬

Downstream:   encoder â†’ representation â†’ robot policy
                        â†‘ ì´ê±¸ ì“´ë‹¤                (decoder ë²„ë¦¼)
```

> "Decoderê°€ ì˜ ë˜ëŠ” ê²ƒì´ ëª©ì ì´ ì•„ë‹™ë‹ˆë‹¤. DecoderëŠ” encoderê°€ ì¢‹ì€ representationì„ ë°°ìš°ë„ë¡ ìœ ë„í•˜ëŠ” **auxiliary task**ì¼ ë¿ì…ë‹ˆë‹¤. ìµœì¢… ê²€ì¦ì€ LIBEROì—ì„œ encoderë§Œ ì‚¬ìš©í–ˆì„ ë•Œì˜ ì„±ëŠ¥ì…ë‹ˆë‹¤."

**2. Task Difficultyì˜ Sweet Spot**

| ì„¤ì • | Task ë‚œì´ë„ | Encoder í•™ìŠµ | ì•ˆì •ì„± |
|------|-----------|------------|--------|
| Patchesë§Œ | ê·¹ë„ë¡œ ì–´ë ¤ì›€ | ??? | ë¶ˆì•ˆì • |
| + img_t + Skip + CLS | ì ì ˆ | ì¢‹ìŒ | ì•ˆì •ì  |
| + Ground truth hints | ë„ˆë¬´ ì‰¬ì›€ | ë‚˜ì¨ | Trivial |

> "Taskê°€ ë„ˆë¬´ ì–´ë ¤ìš°ë©´ í•™ìŠµ ë¶ˆì•ˆì •, ë„ˆë¬´ ì‰¬ìš°ë©´ trivialí•©ë‹ˆë‹¤. Skip connectionê³¼ intermediate CLSëŠ” **gradient flowë¥¼ ê°œì„ **í•˜ê³  multi-scale learningì„ ìœ ë„í•©ë‹ˆë‹¤. MAEë„ 75% masking (not 100%)ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì™€ ê°™ìŠµë‹ˆë‹¤."

**3. Skip Connection â‰  ì •ë‹µ ì œê³µ**

U-Net/ResNetì˜ ì² í•™:
```python
# Skipì´ ìˆì–´ë„ encoderëŠ” ì˜ë¯¸ìˆëŠ” feature ì¶”ì¶œí•´ì•¼ í•¨
# Skipì€ low-level detail ë³´ì¡´ + gradient flow

# Encoderê°€ ì•„ë¬´ê²ƒë„ ì•ˆ ë°°ìš°ë©´?
# â†’ Skipë§Œìœ¼ë¡œëŠ” high-level semantic ë³µì› ë¶ˆê°€ëŠ¥
# â†’ Decoder loss ì—¬ì „íˆ ë†’ìŒ
```

> "Skip connectionì€ ì •ë‹µì„ ì•Œë ¤ì£¼ëŠ” ê²Œ ì•„ë‹ˆë¼, **í•™ìŠµì„ ì•ˆì •í™”**í•˜ëŠ” inductive biasì…ë‹ˆë‹¤. U-Net, ResNetì´ skipì„ ì‚¬ìš©í•˜ì§€ë§Œ trivialí•˜ì§€ ì•Šì€ ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤."

**4. ì‹¤í—˜ì  ê²€ì¦ (Ablation Study)**

```python
# Pretraining variants (Component ablation - Stage 2)
A. Patchesë§Œ (no skip, no CLS)
B. + img_t only
C. + img_t + final CLS
D. + img_t + intermediate CLS + skip (ì œì•ˆ)

# Pretraining loss ë¹„êµ
# â†’ Dê°€ ê°€ì¥ ë‚®ì„ ê²ƒ (ë‹¹ì—°)

# í•µì‹¬: LIBERO transfer (encoderë§Œ ì‚¬ìš©)
# â†’ ë§Œì•½ Dì˜ encoderê°€ A, B, Cë³´ë‹¤ ì¢‹ìœ¼ë©´?
#    Skipì´ encoder í•™ìŠµì„ ë„ì™”ë‹¤ëŠ” ì¦ê±°
# â†’ ë§Œì•½ Dì˜ encoderê°€ ë” ë‚˜ì˜ë©´?
#    Skipì´ í•™ìŠµì„ í•´ì³¤ë‹¤ëŠ” ì¦ê±°
```

> "ìµœì¢… ê²€ì¦ì€ downstream taskì…ë‹ˆë‹¤. Ablation studyì—ì„œ skipê³¼ intermediate CLSê°€ LIBERO ì„±ëŠ¥ì„ í•´ì¹˜ì§€ ì•Šê±°ë‚˜ í–¥ìƒì‹œí‚¨ë‹¤ë©´, ì´ëŠ” encoder í•™ìŠµì„ ë„ì™”ë‹¤ëŠ” ì¦ê±°ì…ë‹ˆë‹¤."

**5. ê¸°ì¡´ ì—°êµ¬ì™€ì˜ ë¹„êµ**

| ë°©ë²• | Decoder input | ì² í•™ |
|------|-------------|------|
| **MAE** | Masked patchesë§Œ | Task ì–´ë µê²Œ â†’ encoder í•™ìŠµ ê°•ì œ |
| **U-Net** | Skip connections | Gradient flow + multi-scale |
| **VideoMAE** | Masked frames | Temporal prediction |
| **Ours** | Intermediate CLS + skip | U-Net ì² í•™ + ì•ˆì •ì„± |

> "U-Netì€ medical imagingì—ì„œ ê²€ì¦ëœ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. Skip connectionì´ ìˆì–´ë„ encoderëŠ” ì—¬ì „íˆ ì¤‘ìš”í•œ featureë¥¼ ë°°ì›ë‹ˆë‹¤. ìš°ë¦¬ëŠ” U-Netì˜ ê²€ì¦ëœ ì„¤ê³„ë¥¼ video predictionì— ì ìš©í•©ë‹ˆë‹¤."

**ê²°ë¡ **: Intermediate CLS + Skipì€ "ë„ˆë¬´ ë§ì€ ì •ë³´"ê°€ ì•„ë‹ˆë¼, **í•™ìŠµ ì•ˆì •ì„±ê³¼ multi-scale representationì„ ìœ„í•œ ì„¤ê³„**ì…ë‹ˆë‹¤. ìµœì¢… ê²€ì¦ì€ LIBERO downstream taskì—ì„œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.

---

**ì˜ˆìƒ ì§ˆë¬¸ & ë‹µë³€** (EgoDex â†’ LIBERO ì‹¤í—˜ ê¸°ë°˜):

Q1: "LAPAë„ video encoder ì“°ë©´ ë˜ì§€ ì•Šë‚˜ìš”?"
A1: "ê·¸ë ‡ê²Œ í•˜ë©´ ìš°ë¦¬ ë°©ë²•ê³¼ ìœ ì‚¬í•´ì§‘ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê¸°ì—¬ëŠ” **VLAë¥¼ ìœ„í•œ change encoderë¥¼ ì²˜ìŒë¶€í„° ì„¤ê³„**í•œ ê²ƒì´ê³ , Two-Stream + CLS Exchangeë¡œ ë” ê°•í•œ inductive biasë¥¼ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."

Q2: "CLIP/SigLIPë„ ì¶©ë¶„íˆ ê°•ë ¥í•œë° ì™œ ìƒˆë¡œìš´ encoderê°€ í•„ìš”í•œê°€ìš”?"
A2: "CLIP/SigLIPì€ static imageì˜ semantic understandingì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. LIBERO ì‹¤í—˜ì—ì„œ ìš°ë¦¬ encoderê°€ SigLIP, MAE, DINOë¥¼ ëª¨ë‘ ëŠ¥ê°€í–ˆìœ¼ë©°, ì´ëŠ” **temporal dynamics modeling**ì´ robot controlì— criticalí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."

Q3: "ì™œ EgoDex (ì‚¬ëŒ ì†) ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?"
A3: "ë‘ ê°€ì§€ ì´ìœ ì…ë‹ˆë‹¤: (1) **ë°ì´í„° ê·œëª¨**: EgoDexëŠ” 829ì‹œê°„, 194 tasksë¡œ ë¡œë´‡ ë°ì´í„°ë³´ë‹¤ í›¨ì”¬ í’ë¶€í•©ë‹ˆë‹¤. (2) **Action-agnostic ê²€ì¦**: Human hand â†’ Robot arm transfer ì„±ê³µì€ ìš°ë¦¬ representationì´ ì§„ì •ìœ¼ë¡œ embodiment-independentí•¨ì„ ì¦ëª…í•©ë‹ˆë‹¤. Action spaceê°€ ì™„ì „íˆ ë‹¤ë¥¸ë°ë„(27-DoF hand vs 7-DoF arm) transferë˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤."

Q4: "Human handì™€ robot armì´ ë„ˆë¬´ ë‹¤ë¥¸ë° ì •ë§ transferê°€ ë˜ë‚˜ìš”?"
A4: "Visual change patternì€ embodimentì™€ ë¬´ê´€í•©ë‹ˆë‹¤. 'ì»µì„ ì§‘ëŠ”ë‹¤'ëŠ” í–‰ë™ì€ ì†ìœ¼ë¡œ í•˜ë“  gripperë¡œ í•˜ë“  ì‹œê°ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë³€í™”ë¥¼ ë§Œë“­ë‹ˆë‹¤. EgoDex pretrained encoderê°€ LIBEROì—ì„œ ImageNet pretrained encoderë¥¼ ëŠ¥ê°€í•œ ê²ƒì´ ì´ë¥¼ ì¦ëª…í•©ë‹ˆë‹¤. Linear probing ê²°ê³¼ë„ human action ì •ë³´ê°€ representationì— ì˜ ì¸ì½”ë”©ë˜ì–´ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤."

Q5: "ì™œ CLSë§Œ êµí™˜í•´? ì „ì²´ token êµí™˜í•˜ë©´ ë” ì¢‹ì§€ ì•Šì•„?" âš ï¸ **í•µì‹¬ ì§ˆë¬¸!**
A5: "Architecture ablation (Experiment B)ì—ì„œ Full Cross-Attentionê³¼ CLS Exchangeë¥¼ ë¹„êµí•©ë‹ˆë‹¤:
- Full Cross-Attention: ëª¨ë“  token êµí™˜ (expensive, no inductive bias)
- CLS Exchange: Summary tokenë§Œ êµí™˜ (efficient, structured)
- ì˜ˆìƒ: CLS Exchangeê°€ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ê· í˜•ìˆê²Œ ë‹¬ì„±
- Regularization íš¨ê³¼: ì •ë³´ ë³‘ëª©ì´ ì˜¤íˆë ¤ generalizationì— ë„ì›€"

Q6: "LIBEROëŠ” ì‹œë®¬ë ˆì´ì…˜ì¸ë° ì‹¤ì œ ë¡œë´‡ì—ì„œë„ ì‘ë™í•˜ë‚˜ìš”?"
A6: "LIBEROëŠ” í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ë¡œ reproducibilityì™€ ê³µì •í•œ ë¹„êµë¥¼ ë³´ì¥í•©ë‹ˆë‹¤. Sim-to-real transferëŠ” future workì´ì§€ë§Œ, LIBEROì—ì„œì˜ ì„±ëŠ¥ì´ encoder í’ˆì§ˆì„ ì¶©ë¶„íˆ ê²€ì¦í•©ë‹ˆë‹¤. OpenVLA, Pi0 ë“± SOTA ëª¨ë¸ë“¤ë„ LIBEROë¡œ ê²€ì¦ë©ë‹ˆë‹¤."

Q7: "Video prediction qualityì™€ downstream performanceì˜ ìƒê´€ê´€ê³„ëŠ”?"
A7: "Stage 1 (Intrinsic Evaluation)ì—ì„œ ì´ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤: (1) Video prediction quality (PSNR/SSIM), (2) Linear probing accuracy, (3) LIBERO success rate ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ video predictionì´ ìœ íš¨í•œ pretraining objectiveì„ì„ ì¦ëª…í•©ë‹ˆë‹¤."

**ë‹µë³€ ì¤€ë¹„ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- âœ… Custom vision encoder (vs off-the-shelf CLIP/SigLIP)
- âœ… Temporal dynamics encoder (vs static image encoder)
- âœ… EgoDex pretraining (human-to-robot transfer)
- âœ… LIBERO benchmark (standard evaluation)
- âœ… Progressive validation (Stage 0-3)
- âœ… Encoder comparison (SigLIP, MAE, DINO vs Ours)
- âœ… Two-Stream architecture (biological motivation)
- âœ… CLS Exchange (efficiency + inductive bias)
- âœ… Ablation studies (architecture + learning method)
- âœ… Intrinsic evaluation (video prediction, linear probing)

---

## ì‹¤í—˜ìœ¼ë¡œ ê²€ì¦ ê°€ëŠ¥í•œ ì£¼ì¥ vs ì´ë¡ ì  ì£¼ì¥

**ëª©ì **: í˜„ì¬ ì‹¤í—˜ ê³„íš (Stage 0-3)ìœ¼ë¡œ **ì¦ëª… ê°€ëŠ¥í•œ ì£¼ì¥**ê³¼ **ì¦ê±° ì—†ëŠ” ì£¼ì¥**ì„ ëª…í™•íˆ êµ¬ë¶„

### âœ… ì‹¤í—˜ìœ¼ë¡œ ê²€ì¦ ê°€ëŠ¥ (Stageë³„)

**Stage 0-1: Pretraining + Intrinsic Evaluation**

Q1: **Video predictionì´ behavior representationì— ìœ íš¨í•œê°€?**
- ê²€ì¦: PSNR/SSIM (video quality) + Linear probing accuracy (action info)
- Go/No-Go: PSNR > MAE, Accuracy > 70%

Q2: **EgoDex human actionì´ representationì— ì¸ì½”ë”©ë˜ëŠ”ê°€?**
- ê²€ì¦: Linear probingìœ¼ë¡œ 27-DoF hand action ë¶„ë¥˜
- ì¦ê±°: 70%+ accuracy â†’ visual changeì— behavior ì •ë³´ ì¡´ì¬

---

**Stage 2: Component Ablation**

Q3: **Two-Streamì´ Single-streamë³´ë‹¤ íš¨ê³¼ì ì¸ê°€?**
- ê²€ì¦: Single vs Two-stream (no exchange) vs Two-stream + CLS Exchange
- ì˜ˆìƒ: Two-streamì´ ~10% í–¥ìƒ
- ì¦ê±°: "Two-Streamì´ Single-stream ëŒ€ë¹„ X% í–¥ìƒ"

Q4: **CLS Exchangeê°€ Full Cross-Attentionë³´ë‹¤ íš¨ìœ¨ì ì¸ê°€?**
- ê²€ì¦: CLS Exchange vs Full Cross-Attention (speed + performance)
- ê°€ëŠ¥í•œ ê²°ê³¼:
  - CLS â‰ˆ Full (ê°™ì€ ì„±ëŠ¥, 2Ã— faster) â†’ efficiency ì…ì¦ âœ…
  - CLS > Full (regularization íš¨ê³¼) â†’ ì¶”ê°€ ê¸°ì—¬ âœ…
  - CLS < Full â†’ ì†”ì§íˆ ì¸ì •, trade-off ë…¼ì˜

Q5: **Mì±„ë„ê³¼ Pì±„ë„ì´ complementaryí•œê°€?**
- ê²€ì¦: M-only vs P-only vs M+P
- ì¦ê±°: M+Pê°€ ê°ê° ë‹¨ë… ëŒ€ë¹„ í–¥ìƒ

---

**Stage 3: LIBERO Transfer**

Q6: **Custom encoderê°€ ê¸°ì¡´ encoderë³´ë‹¤ ìš°ìˆ˜í•œê°€?** â­ í•µì‹¬
- ê²€ì¦: SigLIP (OpenVLA) vs MAE vs DINO vs Ours
- ì¦ê±°: "Oursê°€ SigLIP ëŒ€ë¹„ X% LIBERO success rate í–¥ìƒ"
- í•µì‹¬ ë©”ì‹œì§€: "Temporal dynamics encoder > Static image encoder"

Q7: **Humanâ†’Robot transferê°€ ì‘ë™í•˜ëŠ”ê°€?** â­ í•µì‹¬
- ê²€ì¦: EgoDex pretrained vs ImageNet pretrained (MAE, DINO)
- ì¦ê±°: "EgoDex pretrainedê°€ ImageNet ëŒ€ë¹„ X% í–¥ìƒ"
- í•µì‹¬ ë©”ì‹œì§€: "Human video pretrainingì´ robot dataë³´ë‹¤ íš¨ê³¼ì "

Q8: **Frozen encoderê°€ íš¨ê³¼ì ì¸ê°€?**
- ê²€ì¦: Frozen + Linear vs Full finetune
- ì¦ê±°: Few-shotì—ì„œ frozen íš¨ê³¼ ë˜ëŠ” full finetune trade-off

Q9: **Multi-embodiment unified modelì´ íš¨ê³¼ì ì¸ê°€?** â­ ë¶€ë¶„ ê²€ì¦
- ê²€ì¦: EgoDex (human hand, 27-DoF) â†’ LIBERO (robot arm, 7-DoF) transfer ì„±ê³µ
- ì¦ê±°: **ë‘ ê°œì˜ ë‹¤ë¥¸ embodiment ê°„ knowledge transfer ì…ì¦**
  - Shared representation (EgoDex pretrained encoder)ì´ ë‹¤ë¥¸ embodiment (robot)ë¡œ transfer
  - Human hand expert â†’ Robot arm expert ì „í™˜ (embodiment-specific finetuning)
- í•œê³„: Simultaneous multi-robot deploymentëŠ” ê²€ì¦ ì•ˆ ë¨ (sequential transferë§Œ)
- í•µì‹¬ ë©”ì‹œì§€: "Cross-embodiment knowledge transferì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì€ ê²€ì¦ë¨"

---

### âŒ ì‹¤í—˜ìœ¼ë¡œ ê²€ì¦ ë¶ˆê°€ëŠ¥ (ì´ë¡ ì  ì£¼ì¥ë§Œ)

**Q10: Task-conditioningì´ data efficiencyë¥¼ ë†’ì´ëŠ”ê°€?**
- í˜„ì¬ ê³„íš: Task-conditioning ablation ì—†ìŒ
- ìƒíƒœ: **ì¦ê±° ì—†ìŒ, ì£¼ì¥ë§Œ ê°€ëŠ¥**
- í•´ê²°: Future work ë˜ëŠ” "ê¸°ì¡´ ì—°êµ¬ (CLIP) ê¸°ë°˜ íƒ€ë‹¹ì„±" ì£¼ì¥

**Q11: Real robotì—ì„œ ì‘ë™í•˜ëŠ”ê°€?**
- í˜„ì¬ ê³„íš: LIBERO simulation only
- ìƒíƒœ: **Sim-to-real gap ê²€ì¦ ë¶ˆê°€**
- í•´ê²°: "LIBERO benchmarkë¡œ encoder í’ˆì§ˆ ê²€ì¦, real robotì€ future work"

**Q12: Data efficiency curveëŠ”?**
- í˜„ì¬ ê³„íš: Few-shot learning curve (10, 50, 100, 500, 1000) ì‹¤í—˜ ì—†ìŒ
- ìƒíƒœ: **ì¦ê±° ì—†ìŒ**
- í•´ê²°: Limitation ëª…ì‹œ ë˜ëŠ” Future work

---

### ë…¼ë¬¸ ì‘ì„± ì „ëµ

**Main Claims (ì‹¤í—˜ ì¦ê±° ìˆìŒ)** âœ…:
1. Custom temporal dynamics encoder > Off-the-shelf static encoder
2. Two-Stream architectureê°€ íš¨ê³¼ì 
3. Human video pretrainingì´ robot transferì— ìœ íš¨
4. CLS Exchangeê°€ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ê· í˜•
5. **Cross-embodiment knowledge transfer ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦** (human 27-DoF â†’ robot 7-DoF)

**Supporting Claims (ì´ë¡ ì  íƒ€ë‹¹ì„±)** âš ï¸:
1. Task-conditioning (CLIP ë“± ì„ í–‰ ì—°êµ¬ ê¸°ë°˜)
2. M/P ë¶„ë¦¬ (ìƒë¬¼í•™ì  íƒ€ë‹¹ì„±)
3. Simultaneous multi-robot deployment (ì•„í‚¤í…ì²˜ ì„¤ê³„ë§Œ, sequential transferëŠ” ê²€ì¦ë¨)

**Future Work (ì‹¤í—˜ ì—†ìŒ)** âŒ:
1. Real robot validation (sim-to-real transfer)
2. Simultaneous multi-robot deployment (3+ robots in single model)
3. Data efficiency analysis (few-shot learning curve)
4. Contrastive learning ë¹„êµ (vs video prediction)

---

### 10. Practical Issues & Limitations (í˜„ì¬ ì‹¤í—˜ ê¸°ì¤€)

**ë¹„íŒ A: Humanâ†’Robot Transfer Gap**
- Human hand (27-DoF, fine-grained) vs Robot arm (7-DoF, gripper)
- Morphologyê°€ ë„ˆë¬´ ë‹¤ë¥¸ë° transferê°€ ê°€ëŠ¥í•œê°€?
- EgoDexì˜ egocentric view vs LIBEROì˜ third-person view

**ë‹µë³€**:
- Visual change patternì€ embodiment-independent
- LAPAê°€ humanâ†’robot transfer ê²€ì¦í•¨ (36.8% vs 30.8%)
- Linear probingìœ¼ë¡œ action ì •ë³´ ì¸ì½”ë”© í™•ì¸
- Stage 1 (Intrinsic Evaluation)ì—ì„œ ì‚¬ì „ ê²€ì¦
- Limitation: Reachable tasksì—ë§Œ ì ìš© ê°€ëŠ¥ (ëª…ì‹œ)

**ë¹„íŒ B: Simulation-only Evaluation**
- LIBEROëŠ” ì‹œë®¬ë ˆì´ì…˜ (sim-to-real gap)
- ì‹¤ì œ ë¡œë´‡ì—ì„œ ì‘ë™ ë³´ì¥ ì—†ìŒ

**ë‹µë³€**:
- LIBEROëŠ” í‘œì¤€ benchmark (OpenVLA, Pi0ë„ ì‚¬ìš©)
- Reproducibilityì™€ ê³µì •í•œ ë¹„êµ ë³´ì¥
- Sim-to-realì€ future work
- Encoder í’ˆì§ˆ ê²€ì¦ì—ëŠ” ì¶©ë¶„

**ë¹„íŒ C: Preprocessing Overhead**
- Mì±„ë„ (optical flow ê³„ì‚°) + Pì±„ë„ (edge detection)
- Inference ì‹œ overhead
- Raw RGB ëŒ€ë¹„ ëŠë¦¼

**ë‹µë³€**:
- M/P preprocessingì€ í•œ ë²ˆë§Œ (cache ê°€ëŠ¥)
- GPUì—ì„œ real-time ì²˜ë¦¬ ê°€ëŠ¥ (~5ms)
- ë³µì¡ë„ëŠ” ì„±ëŠ¥ê³¼ trade-off
- Two-Streamì˜ ì„±ëŠ¥ í–¥ìƒ(+13%)ì´ overhead ì •ë‹¹í™”

**ë¹„íŒ C: Decoder ìœ ì§€ ë¹„ìš©**
- Video prediction decoderëŠ” downstreamì—ì„œ ë¶ˆí•„ìš”
- ì™œ pretrainingì— ë¬´ê±°ìš´ decoderê°€ í•„ìš”í•œê°€?
- Contrastive learningìœ¼ë¡œ ë” ê°„ë‹¨íˆ ê°€ëŠ¥í•˜ì§€ ì•Šì€ê°€?

**ë‹µë³€**:
- DecoderëŠ” pretraining í›„ ë²„ë¦¼ (inference ë¶ˆí•„ìš”)
- Video predictionì´ forward dynamics í•™ìŠµì— íš¨ê³¼ì 
- Contrastive learningê³¼ ë¹„êµ ablation í•„ìš” (future work)
- í•˜ì§€ë§Œ LAPAë„ VQ-VAE decoder ì‚¬ìš© (ìœ ì‚¬í•œ ì ‘ê·¼)

---

## ì½ì„ ë…¼ë¬¸

**ìš”ì•½**: ìµœìš°ì„ (LAPA âœ…, OpenVLA âœ…, Bridge V2 âœ…) | ìµœì‹  cross-embodiment(TraceGen âœ…, X-Diffusion, TrajSkill) | ì„ íƒì (VC-1, Diffusion Policy)

> [!info]- ğŸ“š ìƒì„¸ ë‚´ìš© ë³´ê¸°
>
> **ìµœìš°ì„  (ì§ì ‘ ê´€ë ¨ - ë°˜ë“œì‹œ ì½ì–´ì•¼ í•¨)**:
> - [x] **LAPA (2024)** â­â­â­ í•µì‹¬!
>   - ICLR 2025, Latent Action Pretraining from Videos
>   - **ì™œ ì¤‘ìš”**: ì‚¬ëŒ ë¹„ë””ì˜¤ > ë¡œë´‡ ë°ì´í„° ì¦ëª… (36.8% vs 30.8%)
>   - **ìš°ë¦¬ì™€ì˜ ê´€ê³„**: ê°™ì€ ëª©í‘œ(human-to-robot), ë‹¤ë¥¸ ì•„í‚¤í…ì²˜
>   - **LAPA ë°©ë²•**: Single-stream ViT + VQ-VAE latent + Task-agnostic
>   - **ìš°ë¦¬ ì°¨ë³„ì ** (ì•„í‚¤í…ì²˜ ê¸°ë°˜):
>     1. Two-Stream Architecture (M/P ë¶„ë¦¬)
>     2. CLS Exchange (ë…ë¦½ + êµí™˜)
>     3. Task-Conditioning (relevant features)
>   - **í™œìš©**: Human video ê°€ëŠ¥ì„± ê²€ì¦, ì•„í‚¤í…ì²˜ ìš°ìˆ˜ì„± ê°•ì¡°
>
> - [x] **OpenVLA (2024)** â­â­â­
>   - arXiv 2406.09246, í˜„ì¬ VLA SOTA
>   - **ì™œ ì¤‘ìš”**: Primary baseline, ì´ê²ƒì„ ì´ê²¨ì•¼ í•¨
>   - **ë¹„êµ í¬ì¸íŠ¸**: Robot action-labeled vs Human video pretraining
>   - **êµ¬í˜„**: HuggingFace checkpoint í™œìš© (ì‰¬ì›€)
>
> - [x] [[BridgeData V2 (2023)]] â­â­
>   - CoRL 2023, Homer Walke, Chelsea Finn, Sergey Levine
>   - **ê·œëª¨**: 60K trajectories, 24 environments, WidowX robot
>   - **ì™œ ì¤‘ìš”**: ë¡œë´‡ í•™ìŠµ ë¶„ì•¼ ì‚¬ì‹¤ìƒ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ (322 ì¸ìš©)
>   - **í™œìš©**: Finetuning ë°ì´í„°, evaluation benchmark
>   - **ì €ì**: UC Berkeley/Stanford RAIL lab - ìµœê³  ê¶Œìœ„
>   - **ì„íŒ©íŠ¸**: Open X-Embodimentì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ
>   - **ì¬í˜„ì„±**: ì˜¤í”ˆì†ŒìŠ¤, ì €ê°€ ë¡œë´‡($3-5K), ëˆ„êµ¬ë‚˜ ì¬í˜„ ê°€ëŠ¥
>
> **í•„ìˆ˜ (Baseline & Dataset)**:
> - [x] **VC-1 (2023)**: Visual representation baseline (ì„ íƒì  ë¹„êµ)
> - [x] **[[Sources/papers/RT-X (2023)]]**: Open X-Embodiment ë°ì´í„°ì…‹
> - [x] **EgoDex (2024)**: Apple, ìµœê³  í’ˆì§ˆ ì‚¬ëŒ manipulation ë°ì´í„° (829ì‹œê°„)
> - [ ] ~~R3M (2022)~~: VC-1ë¡œ ì¶©ë¶„ (íŒ¨ìŠ¤)
> - [ ] ~~RT-1 (2022)~~: OpenVLAë¡œ ì¶©ë¶„ (íŒ¨ìŠ¤)
>
> **ìµœì‹  Cross-Embodiment Learning (2025)**:
> - [x] **TraceGen (Nov 2025)** â­â­â­ arXiv 2511.21690
>   - **í•µì‹¬**: 3D trace-space for cross-embodiment learning from human videos
>   - **ë°ì´í„°**: 123K videos, 1.8M triplets (TraceForge pipeline)
>   - **ì„±ëŠ¥**: 5 human videos â†’ 67.5% success, 50-600x faster inference
>   - **ì°¨ë³„ì **: 3D geometric trace vs ìš°ë¦¬ëŠ” 2D visual behavior (simpler, efficient)
>   - **í™œìš©**: Related Work í•µì‹¬ ë¹„êµ ëŒ€ìƒ, efficiency ê·¼ê±°
>
> - [ ] **UMI-on-Air (Oct 2025)** â­â­â­â­ arXiv 2510.02614 **í•„ë…!**
>   - **í•µì‹¬**: Handheld gripper (UMI) human demos â†’ embodiment-agnostic policy â†’ aerial manipulator deployment
>   - **ë°©ë²•**: Embodiment-Aware Diffusion Policy (EADP) - gradient feedback from controller
>   - **ì„±ëŠ¥**: Long-horizon, high-precision aerial manipulation tasks ì„±ê³µ
>   - **ì°¨ë³„ì **: ìš°ë¦¬ì™€ ê±°ì˜ ë™ì¼í•œ ë¬¸ì œ! (human demos â†’ robot transfer)
>   - **í™œìš©**: ì§ì ‘ ë¹„êµ í•„ìˆ˜, ê°€ì¥ ìœ ì‚¬í•œ ì ‘ê·¼ë²•
>   - **ì£¼ì˜**: RSS 2026 ì œì¶œ ì‹œ í•µì‹¬ related work
>
> - [ ] **ViDEN (Dec 2024)** â­â­â­ arXiv 2412.20226
>   - **í•µì‹¬**: Visual demonstrations â†’ embodiment-agnostic navigation policy
>   - **ë°©ë²•**: Diffusion-based policy, depth images, relative target positions
>   - **ì„±ëŠ¥**: Small dataset (500 points), human reaching & tracking tasks
>   - **ì°¨ë³„ì **: Navigation vs ìš°ë¦¬ëŠ” manipulation (ìƒí˜¸ë³´ì™„ì )
>   - **í™œìš©**: Visual demonstration + embodiment-agnostic ì ‘ê·¼ ë¹„êµ
>
> - [ ] **Latent Policy Steering (Jul 2025)** â­â­ arXiv 2507.13340
>   - **í•µì‹¬**: Multi-embodiment World Model pretraining + latent space search
>   - **ë°ì´í„°**: Open X-embodiment (2K episodes) + human play data
>   - **ì„±ëŠ¥**: 30 demosì—ì„œ 50% ì„±ëŠ¥ í–¥ìƒ, 50 demosì—ì„œ 20% í–¥ìƒ
>   - **ì°¨ë³„ì **: World model + search vs ìš°ë¦¬ëŠ” direct behavior representation
>   - **í™œìš©**: Multi-embodiment pretraining ì „ëµ ë¹„êµ
>
> - [ ] **E2VLA (Sep 2025)** â­ arXiv 2509.14630
>   - **í•µì‹¬**: Embodiment equivariant VLA (configuration transformation equivariance)
>   - **ë°©ë²•**: Geometry-aware network + equivariant action decoder
>   - **ì°¨ë³„ì **: Equivariance theory vs ìš°ë¦¬ëŠ” task-conditioned representation
>   - **í™œìš©**: Theoretical foundation ì°¸ê³ 
>
> - [ ] **TrajSkill (Oct 2025)** â­â­ arXiv 2510.07773
>   - **í•µì‹¬**: Sparse optical flow as embodiment-agnostic motion cues
>   - **ì„±ëŠ¥**: 16.7% cross-embodiment improvement, real kitchen tasks
>   - **ì°¨ë³„ì **: Low-level optical flow vs ìš°ë¦¬ëŠ” high-level task-conditioned behavior
>   - **í™œìš©**: Motion representation ë¹„êµ
>
> - [ ] **Masquerade (Aug 2025)** â­â­ arXiv 2508.09976
>   - **í•µì‹¬**: Video editing (inpainting + robot overlay) for human â†’ robot demos
>   - **ë°ì´í„°**: 675K frames, 50 robot demos per task
>   - **ì„±ëŠ¥**: 5-6x better on bimanual kitchen tasks
>   - **ì°¨ë³„ì **: Video editing vs ìš°ë¦¬ëŠ” direct representation learning
>   - **í™œìš©**: Data augmentation ì „ëµ ì°¸ê³ 
>
> - [ ] **Gen2Act (Sep 2024)** â­ arXiv 2409.16283
>   - **í•µì‹¬**: Human video generation â†’ robot execution (zero-shot)
>   - **ì°¨ë³„ì **: Generation-based vs representation-based
>   - **í™œìš©**: Zero-shot capability ë¹„êµ
>
> - [ ] **ET-VLA (Nov 2025)** arXiv 2511.01224
>   - Embodiment transfer for multi-robot, 53.2% better than OpenVLA
>   - **í™œìš©**: Baseline ì°¸ê³ , synthetic continued pretraining
>
> - [ ] **CHORD (Jan 2026)** arXiv 2601.04194
>   - **í•µì‹¬**: Video generative models â†’ Lagrangian motion extraction â†’ robotics manipulation policies
>   - **ë°©ë²•**: Distillation-based pipeline from 2D videos (universal, category-agnostic)
>   - **ì €ì**: Jiajun Wu lab (Stanford)
>   - **ì°¨ë³„ì **: Generative model distillation vs ìš°ë¦¬ëŠ” contrastive representation
>   - **í™œìš©**: Video-to-policy ì ‘ê·¼ë²• ë¹„êµ (ê°„ì ‘ì  ê´€ë ¨)
>
> **Vaultì— ìˆëŠ” ê´€ë ¨ ë…¼ë¬¸ (ë‹¤ì‹œ ì½ê¸°)**:
> - [x] **[[Sources/papers/CURL (2020)]]**: Contrastive learning, sample efficiency
> - [x] **[[Sources/papers/DINO (2021)]]**: Self-supervised ViT, temporal consistency
> - [ ] **[[Sources/papers/Diffusion Policy (2023)]]**: Action decoding baseline
> - [ ] **[[Sources/papers/RT-2 (2023)]]**: VLM for robot control
> - [ ] **[[Sources/papers/ALOHA (2023)]]**: Action chunking (ACT), temporal consistency
> - [ ] **[[Sources/papers/Visual Pre-training Survey (2023)]]**: Visual pretraining survey
>
> **ì´ë¡ ì  ê¸°ë°˜ (ì°¸ê³ )**:
> - [ ] **Embodiment Scaling Laws (May 2025)** arXiv 2505.05753
>   - **í•µì‹¬**: Training embodiment ìˆ˜ ì¦ê°€ â†’ unseen embodiment ì¼ë°˜í™” í–¥ìƒ
>   - **ì‹¤í—˜**: ~1,000 procedurally generated embodiments (robot locomotion)
>   - **ê²°ê³¼**: Embodiment scaling > Data scaling (fixed embodiment)
>   - **ì°¨ë³„ì **: Locomotion vs ìš°ë¦¬ëŠ” manipulation (ë‹¤ë¥¸ domain)
>   - **í™œìš©**: Cross-embodiment generalization ì´ë¡ ì  ê·¼ê±°
>
> **ì°¸ê³  (ë‚®ì€ ìš°ì„ ìˆœìœ„)**:
> - [ ] **Robot Trains Robot (Aug 2025)** arXiv 2508.12252
>   - **í•µì‹¬**: Robotic arm teacher â†’ humanoid robot student (real-world RL)
>   - **ë°©ë²•**: Protection, reward, perturbation, automatic reset by teacher robot
>   - **ì°¨ë³„ì **: Hardware setup vs ìš°ë¦¬ëŠ” data/representation (orthogonal)
>   - **í™œìš©**: Real-world learning setup ì°¸ê³  (ê°„ì ‘ì )
>
> - [x] **[[Sources/papers/TCN (2018)]]**: Time-contrastive networks
> - [[Sources/papers/OpenVLA (2024)]]: VLA ë¹„êµ ëŒ€ìƒ
> - [[Sources/papers/GNM (2022)]]: Cross-embodiment navigation
> - [[Sources/papers/CLIP (2021)]]: Vision-language foundation

---

## ê´€ë ¨ ë…¸íŠ¸

- [[Questions/Q - Action-Agnostic Robot Learning]]
- [[Sources/papers/CURL (2020)]]: Sample efficiencyì™€ representation learning
- [[Sources/papers/DINO (2021)]]: Temporal consistencyì™€ collapse ë°©ì§€

**ì£¼ìš” ë¹„êµ ëŒ€ìƒ**:
- [[Sources/papers/Octo (2024)|critiques]] - Primary baseline. OctoëŠ” modular architectureë¡œ flexibility ë‹¬ì„±í•˜ì§€ë§Œ action-space couplingê³¼ ë°ì´í„° í’ˆì§ˆì— ì˜ì¡´ì . Attentionì´ "ì•Œì•„ì„œ" ì²˜ë¦¬í•  ê²ƒì„ ê¸°ëŒ€í•˜ë‚˜ ë°ì´í„° ë¶ˆê· í˜•(wrist 27%, language 56%) ì‹œ ì„±ëŠ¥ ì €í•˜. ìš°ë¦¬ëŠ” action-agnostic representationìœ¼ë¡œ ê·¼ë³¸ í•´ê²°. Robot-to-robot (800k eps) vs Human-to-robot (ì‚¬ëŒ ë¹„ë””ì˜¤).

**êµ¬ë¶„ í•„ìš”**:
- [[Outputs/Idea - DynamicNet]]: ë™ê¸°ëŠ” ìœ ì‚¬(í•µì‹¬ ì •ë³´ ì„ ë³„)í•˜ë‚˜ ì ‘ê·¼ì´ ë‹¤ë¦„
  - DynamicNet: ì•„í‚¤í…ì²˜ í˜ì‹  (attention ëŒ€ì²´, ë…¸ë“œ ê¸°ë°˜)
  - ì´ ë…¼ë¬¸: í•™ìŠµ ë°©ë²•ë¡  (cross-attention í™œìš©, foundation model)

---

## íƒœê·¸

#paper #project #robot-learning #action-agnostic
