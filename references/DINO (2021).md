---
title: Emerging Properties in Self-Supervised Vision Transformers
authors: Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, Armand Joulin
year: 2021
citekey: caronEmergingPropertiesSelfSupervised2021a
type: literature
tags:
  - ğŸ“špaper
  - Vision-Transformer
  - Self-Supervised-Learning
  - Self-Distillation
  - Representation-Learning
  - Momentum-Encoder
  - Knowledge-Distillation
---

# Emerging Properties in Self-Supervised Vision Transformers

> [!info] Metadata
> **Authors**: Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, Armand Joulin
> **Year**: 2021
> **Publication**: 
> **DOI**: 10.48550/arXiv.2104.14294
> **URL**: https://arxiv.org/abs/2104.14294
> **Zotero**: [Open]([Caron ë“± - 2021 - Emerging Properties in Self-Supervised Vision Transformers.pdf](zotero://select/library/items/VZLJKLAZ))

---

## ğŸ–¼ï¸ Key Figures


![[Sources/paper imgs/caronEmergingPropertiesSelfSupervised2021a/img-undefined-x47-y397.png]]

![[Sources/paper imgs/caronEmergingPropertiesSelfSupervised2021a/img-2-x301-y429.png]]

![[Sources/paper imgs/caronEmergingPropertiesSelfSupervised2021a/img-6-x46-y543.png]]

![[Sources/paper imgs/caronEmergingPropertiesSelfSupervised2021a/img-7-x43-y182.png]]


---

## ğŸ“‹ Summary

## ğŸ“Œ Reading Context

**ì™œ ì´ ë…¼ë¬¸ì„ ì½ì—ˆëŠ”ê°€?** ë¡œë´‡ foundation model ì—°êµ¬ì—ì„œ vision encoderì˜ representation learningì´ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ì°¨ì§€í•œë‹¤. íŠ¹íˆ VLA ëª¨ë¸ë“¤ì´ ì‚¬ìš©í•˜ëŠ” vision backboneì˜ í•™ìŠµ ë°©ì‹ì„ ì´í•´í•˜ê³ , self-supervised learningì´ ì–´ë–»ê²Œ ì˜ë¯¸ ìˆëŠ” visual featureë¥¼ ë§Œë“¤ì–´ë‚´ëŠ”ì§€ íŒŒì•…í•˜ê¸° ìœ„í•´ ì½ì—ˆë‹¤.

**ê¸°ëŒ€í–ˆë˜ ê²ƒ** Self-supervised learningì´ ViTì—ì„œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€, ì™œ supervised ë°©ì‹ë³´ë‹¤ ë” í’ë¶€í•œ representationì„ ë§Œë“¤ì–´ë‚´ëŠ”ì§€ì— ëŒ€í•œ ì´í•´.

---

## ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´ (Core Ideas)

**í•œ ë¬¸ì¥ ìš”ì•½** Momentum encoder ê¸°ë°˜ì˜ self-distillationìœ¼ë¡œ ViTë¥¼ í•™ìŠµì‹œí‚¤ë©´, ë¼ë²¨ ì—†ì´ë„ segmentation ì •ë³´ê°€ ìì—°ìŠ¤ëŸ½ê²Œ emergeí•˜ê³  k-NNë§Œìœ¼ë¡œë„ ë›°ì–´ë‚œ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤.

**ì£¼ìš” ê¸°ì—¬ (Main Contributions)**

- Self-supervised ViTì—ì„œ segmentation ì •ë³´ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜íƒ€ë‚¨ (supervised ViTë‚˜ ConvNetì—ì„œëŠ” ì•ˆ ë‚˜íƒ€ë‚¨)
- Finetuning ì—†ì´ k-NNë§Œìœ¼ë¡œ ImageNet 78.3% top-1 ë‹¬ì„±
- ê¸°ì¡´ self-supervised ë°©ë²•ë“¤(BYOL, MoCo, SwAV) ëŒ€ë¹„ ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œ ë™ë“± ì´ìƒì˜ ì„±ëŠ¥
- Negative sample, predictor, advanced normalization ì—†ì´ centering + sharpeningë§Œìœ¼ë¡œ collapse ë°©ì§€

**í•µì‹¬ ì ‘ê·¼ë²• (Key Approach)** Knowledge distillationì„ ë¼ë²¨ ì—†ì´ ìˆ˜í–‰í•˜ëŠ” self-distillation ë°©ì‹. Teacherì™€ Studentê°€ ì™„ì „íˆ ë™ì¼í•œ ViT êµ¬ì¡°ë¥¼ ê°€ì§€ë©°, TeacherëŠ” Studentì˜ EMA(Exponential Moving Average)ë¡œ ì—…ë°ì´íŠ¸ëœë‹¤. ê°™ì€ ì´ë¯¸ì§€ì˜ ì„œë¡œ ë‹¤ë¥¸ augmentationì— ëŒ€í•´ Studentê°€ Teacherì˜ ì¶œë ¥ ë¶„í¬ë¥¼ cross-entropyë¡œ ë”°ë¼ê°€ë„ë¡ í•™ìŠµí•œë‹¤. Contrastive learningê³¼ ë‹¬ë¦¬ negative sample ë¹„êµ ì—†ì´, ì˜¤ì§ Teacherë¥¼ ì¶”ì¢…í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

---

## ğŸ”¬ ë°©ë²•ë¡  ìƒì„¸ (Methodology Details)

### í•™ìŠµ êµ¬ì¡°

ì´ë¯¸ì§€ x
 Â  Â â”‚
 Â  Â â”œâ”€â”€â”€ global view (224Â²) â”€â”€â†’ Teacher (ViT) â”€â”€â†’ [CLS] ì¶œë ¥
 Â  Â â”‚ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
 Â  Â â”‚ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â centering + sharpening
 Â  Â â”‚ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
 Â  Â â”‚ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Cross-Entropy Loss
 Â  Â â”‚ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†‘
 Â  Â â””â”€â”€â”€ multi-crop views â”€â”€â”€â”€â†’ Student (ViT) â”€â”€â†’ [CLS] ì¶œë ¥

### Teacher ì—…ë°ì´íŠ¸ (Momentum Encoder)

theta_teacher = Î» * theta_teacher + (1 - Î») * theta_student
# Î» = 0.996 ~ 1.0 (cosine schedule)

TeacherëŠ” gradientë¥¼ ë°›ì§€ ì•Šê³ , Student ê°€ì¤‘ì¹˜ì˜ ëŠë¦¬ê²Œ ì›€ì§ì´ëŠ” í‰ê· ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ([[EMA (Exponential Moving Average)]]). ì´ë¡œ ì¸í•´ Teacherê°€ í•­ìƒ Studentë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì•ˆì •ì ì¸ í•™ìŠµ targetì„ ì œê³µ.

**ìì„¸í•œ ë‚´ìš©**: [[Momentum Encoder and Self-Distillation]]

### Collapse ë°©ì§€

|ë°©ë²•|ì—­í• |
|---|---|
|Centering|Teacher ì¶œë ¥ì—ì„œ mean ë¹¼ê¸° â†’ í•œ ì°¨ì›ì´ dominateí•˜ëŠ” ê²ƒ ë°©ì§€|
|Sharpening|ë‚®ì€ temperature softmax â†’ uniform ë¶„í¬ë¡œ ìˆ˜ë ´ ë°©ì§€|

ì´ ë‘ ê°€ì§€ê°€ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ê· í˜•ì„ ë§ì¶¤. ë‹¤ë¥¸ ë°©ë²•ë“¤ì´ ì‚¬ìš©í•˜ëŠ” predictor, Sinkhorn-Knopp, contrastive loss ë“±ì´ ë¶ˆí•„ìš”.

### ViT êµ¬ì¡° ë° íŒ¨ì¹˜ í‘œê¸°ë²•

|í‘œê¸°|ì˜ë¯¸|224Ã—224 ì´ë¯¸ì§€ ê¸°ì¤€|
|---|---|---|
|ViT/16|íŒ¨ì¹˜ í¬ê¸° 16Ã—16 í”½ì…€|14Ã—14 = 196ê°œ íŒ¨ì¹˜|
|ViT/8|íŒ¨ì¹˜ í¬ê¸° 8Ã—8 í”½ì…€|28Ã—28 = 784ê°œ íŒ¨ì¹˜|

íŒ¨ì¹˜ê°€ ì‘ì„ìˆ˜ë¡ í•´ìƒë„ê°€ ë†’ì•„ì§€ê³  segmentation ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ì§€ë§Œ, ê³„ì‚°ëŸ‰ ì¦ê°€.

---

## ğŸ“Š ê²°ê³¼ ë° ë¶„ì„ (Results & Analysis)

**ì£¼ìš” ì‹¤í—˜ ê²°ê³¼**

|Method|Architecture|Linear|k-NN|
|---|---|---|---|
|DINO|ViT-S/16|77.0%|74.5%|
|DINO|ViT-B/8|80.1%|77.4%|
|BYOL|ViT-S|71.4%|66.6%|
|MoCo-v2|ViT-S|72.7%|64.4%|
|SwAV|ViT-S|73.5%|66.3%|

**Segmentation ì •ëŸ‰ í‰ê°€ (PASCAL VOC12 Jaccard Similarity)**

|Model|Jaccard|
|---|---|
|Random|23.7%|
|Supervised ViT-S/8|21.8%|
|DINO ViT-S/8|44.7%|

**í•œê³„ì  (Limitations)**

- Attention map í•´ìƒë„ê°€ 14Ã—14 ë˜ëŠ” 28Ã—28ë¡œ coarseí•¨
- "Segmentation"ë³´ë‹¤ëŠ” "object localization"ì— ê°€ê¹Œì›€
- ì •ë°€í•œ í…Œë‘ë¦¬ë¼ê³  í•˜ê¸°ì—” í•´ìƒë„ í•œê³„ ì¡´ì¬

---

## ğŸ’¬ Discussion Highlights

**í† ë¡  ì¤‘ ë‚˜ì˜¨ ì§ˆë¬¸ë“¤**

- ViTê°€ convolutionì„ ì‚¬ìš©í•˜ëŠ”ê°€? â†’ íŒ¨ì¹˜ embeddingì— Conv2dë¡œ êµ¬í˜„í•˜ê¸°ë„ í•˜ì§€ë§Œ ê°œë…ì ìœ¼ë¡œëŠ” linear projection
- 8Ã—8 patchesì˜ ì˜ë¯¸? â†’ íŒ¨ì¹˜ ê°œìˆ˜ê°€ ì•„ë‹ˆë¼ íŒ¨ì¹˜ í¬ê¸°ê°€ 8Ã—8 í”½ì…€
- Segmentation ì •ë³´ê°€ ì •ë§ ê·¸ í•´ìƒë„ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ”ê°€? â†’ ì •ëŸ‰ í‰ê°€ë¡œ í™•ì¸í•˜ì§€ë§Œ, "coarse localization"ì´ ë” ì •í™•í•œ í‘œí˜„
- Momentum encoderê°€ ë­”ê°€? â†’ Student ê°€ì¤‘ì¹˜ì˜ EMAë¡œ Teacherë¥¼ ë§Œë“œëŠ” ë°©ì‹
- Distillationì´ ë­”ê°€? â†’ í° ëª¨ë¸ì˜ soft labelì„ ì‘ì€ ëª¨ë¸ì´ ë”°ë¼í•˜ëŠ” í•™ìŠµ ë°©ë²•, DINOëŠ” ì´ë¥¼ ë¼ë²¨ ì—†ì´ ìê¸° ìì‹ ì—ê²Œ ì ìš©

**ë¹„êµ ë…¼ì˜í•œ ë…¼ë¬¸ë“¤**

- CLIP â†’ SigLIP: Contrastiveì—ì„œ ë” ë‹¨ìˆœí•œ ë°©ì‹ìœ¼ë¡œì˜ ë³€í™”ì™€ ìœ ì‚¬í•œ ë§¥ë½
- BYOL: DINOê°€ ì˜ê°ì„ ë°›ì•˜ì§€ë§Œ, predictor ì—†ì´ ë” ë‹¨ìˆœí™”
- MoCo, SimCLR: Negative sample í•„ìš” vs DINOëŠ” ë¶ˆí•„ìš”

**ê¹Šì´ ìˆê²Œ ë‹¤ë£¬ ì£¼ì œ**

- Contrastive learning vs Self-distillationì˜ ì°¨ì´
- Collapse ë°©ì§€ ë©”ì»¤ë‹ˆì¦˜ (centering + sharpening)
- [CLS] token attention weight ì‹œê°í™” ë°©ë²•

---

## ğŸ’­ Personal Evaluation & Insights

**ë…¼ë¬¸ì— ëŒ€í•œ í‰ê°€**

- **Strengths**: ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ë§¤ìš° ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œ SOTA ë‹¬ì„±. Negative sample, predictor ë“± ë³µì¡í•œ ìš”ì†Œ ì œê±°.
- **Weaknesses**: Segmentation "emergence"ì— ëŒ€í•œ ì£¼ì¥ì´ ë‹¤ì†Œ ê³¼ì¥ëœ ë©´ ìˆìŒ. ì‹¤ì œë¡œëŠ” coarse localization.
- **Novelty**: Self-distillationì„ ë¼ë²¨ ì—†ì´ ì ìš©í•œ ê²ƒ, centering+sharpeningë§Œìœ¼ë¡œ collapse ë°©ì§€í•œ ê²ƒì´ í•µì‹¬ ê¸°ì—¬.
- **Impact**: Self-supervised ViT í•™ìŠµì˜ de facto standardê°€ ë¨. í›„ì† ì—°êµ¬(DINOv2 ë“±)ì˜ ê¸°ë°˜.

**ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê²°ì ** ë¡œë´‡ ë¹„ì „ì—ì„œ self-supervised pretrained encoderë¥¼ ì‚¬ìš©í•  ë•Œ, DINOë¡œ í•™ìŠµëœ ViTê°€ ì™œ ì¢‹ì€ representationì„ ì œê³µí•˜ëŠ”ì§€ ì´í•´í•˜ëŠ” ë° ë„ì›€. íŠ¹íˆ ë¬¼ì²´ ì˜ì—­ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ì‹í•˜ëŠ” íŠ¹ì„±ì€ manipulation taskì—ì„œ ìœ ìš©í•  ìˆ˜ ìˆìŒ.

**ìƒˆë¡œìš´ í†µì°° ë° ì•„ì´ë””ì–´**

- Supervised learningì€ image-level label í•˜ë‚˜ë¡œ í’ë¶€í•œ ì‹œê° ì •ë³´ë¥¼ ì¶•ì†Œì‹œí‚¨ë‹¤ëŠ” ê´€ì ì´ ì¸ìƒì 
- Self-supervisedê°€ ë” í’ë¶€í•œ representationì„ ë§Œë“œëŠ” ì´ìœ ì— ëŒ€í•œ ì´í•´

**ì¸ìƒ ê¹Šì—ˆë˜ ì ** ë³µì¡í•œ ê¸°ë²•ë“¤ì„ í•˜ë‚˜ì”© ì œê±°í•´ë„ ì„±ëŠ¥ì´ ìœ ì§€ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ ablationì´ ì„¤ë“ë ¥ ìˆìŒ. "ì´ê²ƒë§Œ ìˆìœ¼ë©´ ëœë‹¤"ë¥¼ ëª…í™•íˆ ì¦ëª….

---

## ğŸ”— Related Papers & References

**ê´€ë ¨ ë…¼ë¬¸ (Typed Links)**

- ã€ŒBYOLã€ - Momentum encoder ê¸°ë°˜ self-supervised learningì˜ ê¸°ë°˜, DINOê°€ ì§ì ‘ ì˜ê°ì„ ë°›ìŒ
- ã€ŒMoCoã€ - Contrastive loss ì‚¬ìš© vs DINOëŠ” cross-entropyë§Œ ì‚¬ìš©, negative sample í•„ìš” ì—¬ë¶€ê°€ í•µì‹¬ ì°¨ì´
- ã€ŒSwAVã€ - Sinkhorn-Knopp normalization ì‚¬ìš© vs DINOëŠ” centering+sharpeningë§Œìœ¼ë¡œ ì¶©ë¶„
- [[Sources/papers/CURL (2020)]] - Momentum encoder êµ¬ì¡°ë¥¼ ê³µìœ í•˜ì§€ë§Œ, CURLì€ contrastive learning, DINOëŠ” self-distillation ì‚¬ìš©

**ì¸ìš© ê´€ê³„ ë©”ëª¨** CURLê³¼ DINOëŠ” ì§ì ‘ ì¸ìš© ê´€ê³„ëŠ” ì—†ì§€ë§Œ (RL vs Vision ë„ë©”ì¸), momentum encoder ê¸°ë°˜ í•™ìŠµì´ë¼ëŠ” ê³µí†µì  ì¡´ì¬

---

## â“ Questions & Future Exploration

**ë¯¸í•´ê²° ì§ˆë¬¸**

- DINOv2ì—ì„œ ì–´ë–¤ ì ì´ ê°œì„ ë˜ì—ˆëŠ”ê°€?
- ë¡œë´‡ manipulationì—ì„œ DINO featureê°€ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ ìœ ìš©í•œê°€?

**í–¥í›„ ì½ì„ ë…¼ë¬¸**

- DINOv2: DINOì˜ í›„ì† ì—°êµ¬
- MAE: ë‹¤ë¥¸ ë°©ì‹ì˜ self-supervised ViT í•™ìŠµ
- VC-1, MVP: ë¡œë´‡ ë„ë©”ì¸ì—ì„œì˜ visual representation learning

**ì‹œë„í•´ë³¼ ê²ƒ**

- DINO pretrained ViTì˜ attention map ì‹œê°í™” ì§ì ‘ í•´ë³´ê¸°
- ë¡œë´‡ manipulation ë°ì´í„°ì—ì„œ DINO feature í’ˆì§ˆ í‰ê°€




### Note (2025-12-09)
Comment: 21 pages




---

## ğŸ“ Contents

### ğŸ“Œ All Highlights

> first, self-supervised ViT features contain explicit information about the semantic segmentation of an image ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=LBF52T3V)) ^highlight-2

> Second, these features are also excellent k-NN classifiers ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=YD7R32NS)) ^highlight-3

> We implement our findings into a simple self-supervised method, called DINO ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=9CGANSWU)) ^highlight-4

> Transformers [70] have recently emerged as an alternative to convolutional neural networks (convnets) for visual recognition [19, 69, 83] ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=H2CJ2SHT)) ^highlight-5

> Vision Transformers (ViT) [19] ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=HVI7STPL)) ^highlight-6

> they are computationally more demanding ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=PAII2FF4)) ^highlight-7

> require more training data ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=WNAIHI2X)) ^highlight-8

> their features do not exhibit unique properties ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=DIS5RWUI)) ^highlight-9

> Our motivation is that one of the main ingredients for the success of Transformers in NLP was the use of self-supervised pretraining ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=GIYY2GQ3)) ^highlight-10

> Similarly, in images, imagelevel supervision often reduces the rich visual information contained in an image to a single concept selected from a predefined set of a few thousand categories of objects [60] ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=3D5KMAUT)) ^highlight-11

> Self-supervised ViT features explicitly contain the scene layout and, in particular, object boundaries ([p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=TAZXQXY6)) ^highlight-13

> Self-supervised ViT features perform particularly well with a basic nearest neighbors classifier (k-NN) without any finetuning ([p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=YNFYTMDQ)) ^highlight-14

> momentum encoder ([p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=DA6MJGCV)) ^highlight-16

> multi-crop augmentation ([p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=JPMXGN2S)) ^highlight-17

> instance classification [12, 20, 33, 73] ([p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=ZUU88IC4)) ^highlight-19

> Wu et al. [73] propose to use a noise contrastive estimator (NCE) [32] to compare instances instead of classifying them ([p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=X3TXAFTL)) ^highlight-20

> A caveat of this approach is that it requires comparing features from a large number of images simultaneously ([p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=GH9S9VED)) ^highlight-21

> Methods like BYOL work even without a momentum encoder, at the cost of a drop of performance [16, 30] ([p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=24DRLUA8)) ^highlight-23

> self-distillation ([p.3](zotero://open-pdf/library/items/VZLJKLAZ?page=3&annotation=3269RIGG)) ^highlight-25

> However, these works rely on a pre-trained fixed teacher while our teacher is dynamically built during training ([p.3](zotero://open-pdf/library/items/VZLJKLAZ?page=3&annotation=W96T4WIL)) ^highlight-29

> with Ï„s > 0 a temperature parameter that controls the ([p.3](zotero://open-pdf/library/items/VZLJKLAZ?page=3&annotation=GSC7GBDN)) ^highlight-30

> sharpness of the output distribution ([p.3](zotero://open-pdf/library/items/VZLJKLAZ?page=3&annotation=U6P3ZCXM)) ^highlight-31



### âœï¸ Notes (Yellow)

- ë¼ë²¨ì„ ê°€ì§€ê³  í•™ìŠµí•˜ë©´ ì˜¤íˆë ¤ ì •ë³´ì–‘ì´ ì¤„ì–´ë“¦ ^note-12
  - Source: [p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=ABSI2DCH)

- segmentation ì •ë³´ëŠ” self-supervised í•™ìŠµì´ë¼ë©´, ë°©ì‹ê³¼ ê´€ê³„ì—†ì´ ëŒ€ì²´ì ìœ¼ë¡œ ì˜ í•™ìŠµë¨
í•˜ì§€ë§Œ k-NN ì„±ëŠ¥ê¹Œì§€ ë‚´ê¸° ìœ„í•´ì„œëŠ” Momentum encoder, Multi-crop ì´ ì¶”ê°€ë¡œ ìˆì–´ì•¼ í•¨ ^note-15
  - Source: [p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=6EQMBSI3)

- DINOëŠ” BYOLì˜ ì•„ì´ë””ì–´ë¥¼ ê°€ì ¸ì˜¤ë˜, ë” ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ì–´ì„œ ë¼ë²¨ ì—†ëŠ” self-distillation ^note-22
  - Source: [p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=S68WNCPM)

- ê¸°ì¡´ distillation : teacherì˜ ë…¸í•˜ìš° ì „ìˆ˜
self-distillation : ê³¼ê±°ì˜ ìì‹ ì—ê²Œ ë°°ì›€ ^note-24
  - Source: [p.3](zotero://open-pdf/library/items/VZLJKLAZ?page=3&annotation=DAF2P99R)

- ë¼ë²¨ ì „íŒŒ ë° Distillation ^note-26
  - Source: [p.3](zotero://open-pdf/library/items/VZLJKLAZ?page=3&annotation=HLVGMEUA)

- ë¼ë²¨ ì „íŒŒ : ë¼ë²¨ ìˆëŠ” ë°ì´í„° -> ì—†ëŠ” ë°ì´í„°ë¡œ í™•ì¥, Soft label ì‚¬ìš©í•˜ë©´ ë³¸ì§ˆì ìœ¼ë¡œ ê°™ìŒ
Knowledge Distillation : ëª¨ë¸ì••ì¶•, í°ëª¨ë¸->ì‘ì€ ëª¨ë¸ë¡œ ì§€ì‹ ì „ë‹¬ ^note-27
  - Source: [p.3](zotero://open-pdf/library/items/VZLJKLAZ?page=3&annotation=4MN7SQBT)

- ê¸°ì¡´ ì—°êµ¬ :  ì‚¬ì „í•™ìŠµëœ ê³ ì • ëª¨ë¸
DINO : í•™ìŠµ ì¤‘ ë™ì  ìƒì„±(EMA) ^note-28
  - Source: [p.3](zotero://open-pdf/library/items/VZLJKLAZ?page=3&annotation=JLYTZ4WW)



---

## ğŸ’¡ Ideas & Insights (Green)



---

## âš ï¸ Critical Notes & Limitations (Red)

> they are computationally more demanding ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=PAII2FF4))


> require more training data ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=WNAIHI2X))


> their features do not exhibit unique properties ([p.](zotero://open-pdf/library/items/VZLJKLAZ?page=&annotation=DIS5RWUI))


> A caveat of this approach is that it requires comparing features from a large number of images simultaneously ([p.2](zotero://open-pdf/library/items/VZLJKLAZ?page=2&annotation=GH9S9VED))




---

## ğŸ”— Connections

### ìë™ ì—°ê²°
```dataview
LIST
WHERE contains(file.outlinks, this.file.link)
```