# Development Status & Next Steps

**Date**: 2026-02-09
**Last Updated**: After Experimental Strategy Discussion

---

## ğŸ“Š Current Implementation Status

### âœ… Completed

**Model Architecture** (`src/models/two_stream.py`):
- âœ… Two-Stream Preprocessing (M: 4ch, P: 5ch)
- âœ… InterleavedTwoStreamViT (CLS Exchange, 3 stages)
- âœ… PixelwiseFusion
- âš ï¸ VideoDecoder (í˜„ì¬: patchesë§Œ ì‚¬ìš©, **ì—…ë°ì´íŠ¸ í•„ìš”**)

**Training**:
- âœ… BridgeDataset implementation
- âœ… EgoDexDataset implementation
- âœ… Multi-gap sampling (1-10 frames)
- âœ… Training pipeline with DataParallel

**Checkpoints**:
- Latest: `data/checkpoints/two_stream/20260202_085022/`
- Progress: 46/350 epochs (13%)
- Dataset: Bridge V2 (1.24M train, 324K eval)
- Best eval loss: 0.0000379

---

## ğŸ¯ Current Task: Baseline Implementation & Code Completion

### Goal

**H100 Phase (~3 days)**: ëª¨ë“  ì‹¤í—˜ ì½”ë“œ ì™„ì„± + ë¹ ë¥¸ ê²€ì¦

**í•„ìš”í•œ êµ¬í˜„**:
1. VideoMAE baseline (masked reconstruction)
2. Single-stream baseline (future prediction, P-stream only)
3. Two-stream (future prediction, ours) - ê¸°ì¡´ ì½”ë“œ ì—…ë°ì´íŠ¸
4. OpenVLA encoder êµì²´ ì½”ë“œ
5. Bridge V2 ì§§ì€ í•™ìŠµ + LIBERO quick test

---

## ğŸ› ï¸ H100 Phase: Required Implementation

### Task 1: Baseline Implementation

**1.1 VideoMAE Baseline**
```python
# GitHub: MCG-NJU/VideoMAE
# Architecture: Video ViT with masked reconstruction
# Task: Masked patch prediction (90% masking)
# Output: Reconstruction loss
```

**1.2 Single-Stream Baseline**
```python
# P-stream only (spatial structure)
# Reuse Two-Stream encoder code with flag
# Task: Future frame prediction
# Output: img_t+k from img_tk only
```

**1.3 Two-Stream (Update)**
```python
# M-stream + P-stream
# Update decoder: intermediate CLS + skip connection
# Task: Future frame prediction
# Output: img_t+k from img_t + img_tk
```

### Task 2: Short Pretraining (Bridge V2, 5-10 epochs)

```bash
python train.py --method videomae --epochs 10
python train.py --method single_stream --epochs 10
python train.py --method two_stream --epochs 10
```

### Task 3: Quick LIBERO Test

**3.1 OpenVLA Encoder Replacement**
```python
# openvla/openvla-7b ì½”ë“œ ìˆ˜ì •
# Vision encoderë§Œ êµì²´ (SigLIP â†’ ours)
# Language model, action head ê³ ì •
```

**3.2 Quick Fine-tuning**
```bash
# Bridge V2 subset (10-20% demos)
python finetune_openvla.py --encoder videomae
python finetune_openvla.py --encoder single_stream
python finetune_openvla.py --encoder two_stream
```

**3.3 Early Signal Check**
- Loss convergence speed
- Eval loss comparison
- Sanity check: Two-stream > Single-stream?

---

## ğŸ“š Key Design Rationale

### Q: "ì •ë³´ë¥¼ ë„ˆë¬´ ë§ì´ ì£¼ë©´ ë‹¹ì—°íˆ ì˜ ë˜ëŠ” ê²ƒ ì•„ë‹Œê°€?"

**A**:
1. **Pretraining ëª©ì **: Decoder ì„±ëŠ¥ (X) â†’ Encoder representation í’ˆì§ˆ (O)
2. **Task difficulty**: ë„ˆë¬´ ì–´ë ¤ì›€ (ë¶ˆì•ˆì •) vs ì ì ˆí•¨ (ì•ˆì •) vs ë„ˆë¬´ ì‰¬ì›€ (trivial)
3. **Skip â‰  ì •ë‹µ**: U-Net/ResNetì²˜ëŸ¼ gradient flow ê°œì„ ìš©
4. **ìµœì¢… ê²€ì¦**: LIBERO downstream taskì—ì„œ encoderë§Œ ì‚¬ìš©í–ˆì„ ë•Œ

**Reference**: ë©”ì¸ ë©”ëª¨ Section 10 "Decoder Design: Intermediate CLS Injection"

---

## ğŸ§ª Quick Test Commands

```bash
# Sanity check
python train.py --method videomae --test --batch-size 8
python train.py --method single_stream --test --batch-size 8
python train.py --method two_stream --test --batch-size 8

# Short training (5-10 epochs)
python train.py --method videomae --epochs 10 --data bridge_v2
python train.py --method single_stream --epochs 10 --data bridge_v2
python train.py --method two_stream --epochs 10 --data bridge_v2

# Quick LIBERO test
python finetune_openvla.py --encoder videomae --demos 0.2
```

---

## ğŸ“‹ AWS Phase (After H100)

**ì™„ì„±ëœ ì½”ë“œ ì‹¤í–‰ë§Œ**:

1. **EgoDex Full Pretraining**
   - 3ê°œ encoder (VideoMAE, Single-stream, Two-stream)
   - Full dataset (829h, 194 tasks)

2. **LIBERO Full Evaluation**
   - OpenVLA + ê° encoder
   - Success rate ì¸¡ì • (Spatial, Object, Long)

3. **Component Ablation (ë¬¸ì œ ë°œìƒ ì‹œë§Œ)**
   - Intermediate CLS vs Final CLS
   - Skip connection ìœ ë¬´
   - Distillation íš¨ê³¼

---

## ğŸ”— Key References

- **Main memo**: `references/ë…¼ë¬¸ - Action-Agnostic Visual Behavior Representation.md`
  - Section 10: Decoder Design Q&A
  - Section "ì‹¤í—˜ ê³„íš": Two-Phase Strategy

- **GitHub**:
  - VideoMAE: `MCG-NJU/VideoMAE`
  - OpenVLA: `openvla/openvla`

---

## âš ï¸ Important Notes

1. **H100 = Code Completion**: ëª¨ë“  ì½”ë“œ ì™„ì„± í›„ AWSë¡œ
2. **3-way Comparison**: VideoMAE vs Single-stream vs Two-stream
3. **Early Signal**: H100ì—ì„œ Quick LIBERO Testë¡œ sanity check
4. **Final Goal**: LIBERO downstream task, not pretraining loss
